[
  {
    "objectID": "grading.html",
    "href": "grading.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "I will grade the report and code according to the following criteria:\n\n20% Code readability & code documentation\n20% Reproducibility & modularity\n20% Sanity Checks & sanity visualizations & their discussion*\n20% Result visualization & Results documentation & interpretation\n20% Scope of Project\n\nGenerally, you see that documentation and explanations are important. I do not (only) want to see that you analyzed the dataset, but I want to see what your motivation for choosing parameters is (parameteres like reference, filter settings, data removal criteria, algorithmic choices etc.).\nRegarding readability, I think direct-documentation (code-comments), style and modularity are key.\nRegarding reproducibility, it is important that you control and document the software versions, that your analysis scripts can be easily reproduced on another computer. If you are using mne-BIDS-pipeline you should already have a good start.\nSanity checks are small asserts & plots here and there, maybe even a unittest or two - small things that make sure your code is doing what it is supposed to be doing.\nSentences starting with (‚ÄúThis seems correct, because xy‚Äù, ‚Äúthis is strange because‚Ä¶‚Äù, etc.) are expected. Such sentences give me an idea how much you understood the material.",
    "crumbs": [
      "Projects",
      "üõ†Ô∏è Grading"
    ]
  },
  {
    "objectID": "grading.html#grading",
    "href": "grading.html#grading",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "I will grade the report and code according to the following criteria:\n\n20% Code readability & code documentation\n20% Reproducibility & modularity\n20% Sanity Checks & sanity visualizations & their discussion*\n20% Result visualization & Results documentation & interpretation\n20% Scope of Project\n\nGenerally, you see that documentation and explanations are important. I do not (only) want to see that you analyzed the dataset, but I want to see what your motivation for choosing parameters is (parameteres like reference, filter settings, data removal criteria, algorithmic choices etc.).\nRegarding readability, I think direct-documentation (code-comments), style and modularity are key.\nRegarding reproducibility, it is important that you control and document the software versions, that your analysis scripts can be easily reproduced on another computer. If you are using mne-BIDS-pipeline you should already have a good start.\nSanity checks are small asserts & plots here and there, maybe even a unittest or two - small things that make sure your code is doing what it is supposed to be doing.\nSentences starting with (‚ÄúThis seems correct, because xy‚Äù, ‚Äúthis is strange because‚Ä¶‚Äù, etc.) are expected. Such sentences give me an idea how much you understood the material.",
    "crumbs": [
      "Projects",
      "üõ†Ô∏è Grading"
    ]
  },
  {
    "objectID": "exercises/ex10_decoding.html",
    "href": "exercises/ex10_decoding.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 10]",
    "section": "",
    "text": "Information on the dataset at hand can be found here - a total of 109 subjects exist.\n\n\n\nSubjects imagine opening left & right hand or hand & feet. We want to decode which they do, in case they are doing that for real, or when they are just imagining it (brain reading ü§Ø!)\n\nWe will use Common Spatial Patterns to get a good feature space and LinearDiscriminatAnalysis as a simple decoder. Of course you can always change things up and see how it changes.\nWe will use cross-validation to preclude overfitting\nWe will apply the weights over all timepoints to get a score across time\n\n\n\n\nLoad the data using\nepochs = ccs_eeg_utils.get_classification_dataset()\nSubjects imagine opening left & right hand or hand & feet (you could change the type of get_classification_dataset() to get e.g.¬†trials for left/right decoding, have a look at the function). Our training data should exclude the evoked response, because it will not be sustained throughout the trial. We will make a copy of the epochs and crop it between 1 and 2 seconds (epochs_train.crop(tmin=1.,tmax=2.))\nT: To get a first look at the training data, 1) average the epochs_train over time and plot the channels C3 and C4 as x/y axis in a scatter plot. Color the datapoints according to the labels. Data you can get using .get_data(picks=[\"C3\",\"C4\"]), labels via .events\nQ: Would this be easy or difficult to separate with a linear classifier?\n\n\n\nWe can define a CommonSpatialPatterns object using csp = mne.decoding.CSP(n_components=2)\nTo fit the CSP we have to give it the data and the labels. For now we just want to look at the CSP, not run a classifier for it, so we dont worry about any overfit etc.\nT: csp.fit_transform(epochs_data, labels) will fit it and csp.plot_filters(epochs.info) and csp.plot_patterns(epochs.info) will plot filter and activation.\nQ: Let‚Äôs say you invite a subject again, but are allowed to only measure 5 channels not the 64. Which of the two plots would you choose to inform which channels to measure? In other words, does any of the plot tell you where the information to decode is strongest?\nT: Finally, let‚Äôs weight/transform the data to the fitted CSP components using csp_data = csp.transform(data). Note that because csp(...,transform_into=\"average_power\") is set by default, we will get data without a time-dimension, thus already aggregated. Plot the datapoints against each other, how easy is it now to define a classifier?\n\n\n\nLet‚Äôs use the LDA classifier (you could use any other if you fancy, up to you - LDA is a fast and simple one).\nYou need to: 1. Construct the classifier via\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\n\nReshape the csp_data to conform to labels x features (2D instead of 3D) e.g.¬†using flattenData = csp_data.reshape(csp_data.shape[0],-1)\nFit the LDA using lda.fit(data,labels)\nScore it using lda.score(data,labels)\n\nNote: we are doing a cardinal sin: Training and testing on the same data. How to fix that is next.\nT: How well can you classify?\n\n\n\nTo work against overfitting, we use crossvalidate and use some functions from sklearn for it. We will use a stratifiedShuffleSplit, which will split our data in training and test sets, but in a stratified way. Thus we dont have to worry about under/oversampling between our classes for now.\ncv = sklearn.model_selection.StratifiedShuffleSplit(10, test_size=0.2, random_state=42)\ncv_split = cv.split(epochs_train.get_data(),labels)\nNext we can walk though each test/train, fit CSP, fit LDA and evaluate. I will give you the skeleton and you only have to fill in the XXX to speed up programming :).\nscore_list = []\nfor train_idx, test_idx in cv_split:\n    y_train, y_test = labels[train_idx], labels[test_idx]\n\n    csp.fit_transform(epochs_train.get_data()[XXX_idx], y_XXX)\n    X_train = csp.transform(epochs_train.get_data()[XXX_idx])\n    X_test  = csp.transform(epochs_train.get_data()[XXX_idx])\n\n    lda.fit(X_XXX, y_XXX) \n    score = lda.score(X_XXX,y_XXX)\n    score_list.append(score)\n\n\n\nTypically you would like to use a pipeline-system to easily exchange components. We are using the scikit pipeline We need several incredients:\n\nTraining-Data & labels\nPipeline with feature selection (CSP) & Classifier (LDA)\nCross-Validation scheme\n\n\n\n\nCSP and LDA & pipeline\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport mne.decoding\nlda = LinearDiscriminantAnalysis()\ncsp = mne.decoding.CSP(n_components=2) # default is 4, typically youd like to do a nested cross-val hyperparam search. 2 is likely too low\n% and the pipeline simply:\npipe = sklearn.pipeline.Pipeline([('CSP', csp), ('LDA', lda)])\n\nThe sklearn Cross-validation pipeline is run like this\n\nimport sklearn.model_selection\ncv = sklearn.model_selection.StratifiedShuffleSplit(10, test_size=0.2, random_state=0)\nscores = sklearn.model_selection.cross_val_score(pipe, trainingData, labels, cv=cv, n_jobs=1)\nQ: What is the accuracy of the classifier? What is the chance level?\n\n\n\n\nThis formulations with the pipelines etc. is neat, because we can simply exchange parts of the pipeline and see what happens. Note though, that you are performing multiple testing and are very easily overfitting to your dataset.\nThus in principle, you would like to explore some options with one set of subjects, then test the pipeline on another set of subjects. But this is a bit too involved for the homework.\n\n\nInstead of applying a feature selection, maybe we can learn from the ‚Äúraw‚Äù data? T: Replace the ‚Äúcsd‚Äù in the pipeline with mne.decoding.Vectorizer(), this will ensure the reshaping of the features we performed earlier.\nQ: What is the accuracy now?\n\n\n\nNext we will fit a classifier for each timepoint and test it on that timepoint (you have to use the simple pipeline, see below) For this the convenience function timeDecode = mne.decoding.SlidingEstimator(pipe) exists. This function performs the pipeline on the last dimension (which is currently time).\nBecause we will get multiple scores per cross-val, we also have to switch our scorer to mne.decoding.cross_val_multiscore(timeDecode,...).\nT: Plot the performance against time **Bonus T:* You can also use mne.decoding.GeneralizingEstimator(...) to get the temporal decoding matrix (increased runtime warning)\nYou might be surprised - or not - by the performance you observed. Applying this to a dataset with actual evoked responses, will likely be much more satisfactory.\nThe reason seems clear, we would need CSP in between. But CSP needs multiple timepoints and doesn‚Äôt work with SlidingEstimator. We‚Äôd have to do it manually.\n\n\n\nIn a real decoding study, you‚Äôd run the decoder for each subject. This would give you a mean accuracy value per subject, which you would put into a significance test (best in a permutation test, because accuracies don‚Äôt follow a normal distribution)."
  },
  {
    "objectID": "exercises/ex10_decoding.html#extensions",
    "href": "exercises/ex10_decoding.html#extensions",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 10]",
    "section": "",
    "text": "This formulations with the pipelines etc. is neat, because we can simply exchange parts of the pipeline and see what happens. Note though, that you are performing multiple testing and are very easily overfitting to your dataset.\nThus in principle, you would like to explore some options with one set of subjects, then test the pipeline on another set of subjects. But this is a bit too involved for the homework.\n\n\nInstead of applying a feature selection, maybe we can learn from the ‚Äúraw‚Äù data? T: Replace the ‚Äúcsd‚Äù in the pipeline with mne.decoding.Vectorizer(), this will ensure the reshaping of the features we performed earlier.\nQ: What is the accuracy now?\n\n\n\nNext we will fit a classifier for each timepoint and test it on that timepoint (you have to use the simple pipeline, see below) For this the convenience function timeDecode = mne.decoding.SlidingEstimator(pipe) exists. This function performs the pipeline on the last dimension (which is currently time).\nBecause we will get multiple scores per cross-val, we also have to switch our scorer to mne.decoding.cross_val_multiscore(timeDecode,...).\nT: Plot the performance against time **Bonus T:* You can also use mne.decoding.GeneralizingEstimator(...) to get the temporal decoding matrix (increased runtime warning)\nYou might be surprised - or not - by the performance you observed. Applying this to a dataset with actual evoked responses, will likely be much more satisfactory.\nThe reason seems clear, we would need CSP in between. But CSP needs multiple timepoints and doesn‚Äôt work with SlidingEstimator. We‚Äôd have to do it manually.\n\n\n\nIn a real decoding study, you‚Äôd run the decoder for each subject. This would give you a mean accuracy value per subject, which you would put into a significance test (best in a permutation test, because accuracies don‚Äôt follow a normal distribution)."
  },
  {
    "objectID": "exercises/ex1_overview.html",
    "href": "exercises/ex1_overview.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "In this exercise we will install the mne-python toolbox, download a example dataset, make some basic visualizations and epoch, average and visualize the resulting ERPs.\nWhat we are doing is roughly outlined in the ‚ÄúWhat is EEG‚Äù video lecture.\n\n\n\nrun pip install mne or if you like conda: conda install -c conda-forge mne\nsimilarly, install pip install mne-bids, a tool to load the EEG data easily.\nTest the installation by import mne\n\n\n\nWe will be using a typical P3 oddball dataset. We expect a positive response over parietal/central electrodes (Cz/Pz) starting at 300-400ms, something like this.\nIf you want to read the details you can find it here. You can also investigate the sub-030_task-P3_eeg.json for a task description which is one of the files you will download next:\nPlease download the data from here\nNext, you need the ccs_eeg_utils.py file which you can add to your python code e.g.¬†via\nimport sys\nsys.path.insert(0,'.')\n\n\n\n# Load the data\nfrom mne_bids import (BIDSPath,read_raw_bids)\n\n# path where to save the datasets.\nbids_root = \"../local/bids\"\nsubject_id = '030' # recommend subject 30 for now\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\n\n# read the file\nraw = read_raw_bids(bids_path)\n# fix the annotations readin\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nTask: Extract a single channel and plot the whole timeseries.\n\n\n\n\n\n\nNote\n\n\n\nYou can directly interact with the raw object, e.g.¬†raw[1:10,1:5000], which extracts the first 10 channels and 5000 samples.\nYou can also use raw.get_data() to get the whole data as a numpy array.\n\n\n\nFor now we can use simple matplotlib to visualize the data, e.g.:\nfrom matplotlib import pyplot as plt\nplt.plot(raw[10,:][0].T)\n\nQuestion: What is the range of the data (in sense of min-y to max-y in ¬µ-volt)?\nTask: Have a look at raw.info and note down what the sampling frequency is (how many EEG-samples per second)\n\n\n\nTask: We will epoch the data now. Formost we will cut the raw data to one channel using raw.pick([\"Cz\"]) - note that this will permanently change the raw object and removes alle other channels from memory. If you want rather a copy you could use raw_subselect = raw.copy().pick([\"Cz\"])).\nTask Let‚Äôs investigate the annotation markers. Have a look at raw.annotations. These values reflect the values in the bids *_events.tsv file (have a look at this file via ../local/bids/sub-030/sub-030_task-P3_events.tsv). BIDS is a new standard to share neuroimaging and other physiological data. It is not really a fileformat, but more of a folder & filename structure with some additional json files. I highly recommend to put your data into bids-format as soon as possible. It helps you stay organized and on top of things!\nTask MNE-speciality: We have to convert annotations to events with evts,evts_dict = mne.events_from_annotations(raw). Have a look at evts - it shows you the sample, the duration and event-id (with the look-up table evts_dict). In this case we only want to look at stimulus evoked responses, so we subset the event table (note: this could be done after epoching too)\n# get all keys which contain \"stimulus\"\nwanted_keys = [e for e in evts_dict.keys() if \"stimulus\" in e]\n# subset the large event-dictionairy\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\nTask Epoch the data with epochs = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1)\nTask Now that we have the epochs we should plot them. Plot all trials ‚Äòmanually‚Äô, (without using mne‚Äôs functionality) (epochs.get_data()).\n\n\n\n\n\n\nNote\n\n\n\nYou should plot one line per trial.\n\n\nQuestion What is the scale-range of the epoched data now?\n\n\n\nTask But which epochs belong to targets and which to distractors? This is hidden in the event-description. Using the following lines you can find out which indices belong to which trial-types\ntarget = [\"stimulus:{}{}\".format(k,k) for k in [1,2,3,4,5]]\ndistractor = [\"stimulus:{}{}\".format(k,j) for k in [1,2,3,4,5] for j in [1,2,3,4,5] if k!=j]\nNow index the epochs evoked = epochs[index].average() and average them. You can then plot them either via evoked.plot() or with mne.viz.plot_compare_evokeds([evokedA,evokedB]).\nQuestion What is the unit/scale of the data now? Set it into context to the other two scales you reported before"
  },
  {
    "objectID": "exercises/ex1_overview.html#overview",
    "href": "exercises/ex1_overview.html#overview",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "In this exercise we will install the mne-python toolbox, download a example dataset, make some basic visualizations and epoch, average and visualize the resulting ERPs.\nWhat we are doing is roughly outlined in the ‚ÄúWhat is EEG‚Äù video lecture."
  },
  {
    "objectID": "exercises/ex1_overview.html#install-mne-toolbox",
    "href": "exercises/ex1_overview.html#install-mne-toolbox",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "run pip install mne or if you like conda: conda install -c conda-forge mne\nsimilarly, install pip install mne-bids, a tool to load the EEG data easily.\nTest the installation by import mne"
  },
  {
    "objectID": "exercises/ex1_overview.html#download-data",
    "href": "exercises/ex1_overview.html#download-data",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "We will be using a typical P3 oddball dataset. We expect a positive response over parietal/central electrodes (Cz/Pz) starting at 300-400ms, something like this.\nIf you want to read the details you can find it here. You can also investigate the sub-030_task-P3_eeg.json for a task description which is one of the files you will download next:\nPlease download the data from here\nNext, you need the ccs_eeg_utils.py file which you can add to your python code e.g.¬†via\nimport sys\nsys.path.insert(0,'.')"
  },
  {
    "objectID": "exercises/ex1_overview.html#load-data-plot-continuous-eeg",
    "href": "exercises/ex1_overview.html#load-data-plot-continuous-eeg",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "# Load the data\nfrom mne_bids import (BIDSPath,read_raw_bids)\n\n# path where to save the datasets.\nbids_root = \"../local/bids\"\nsubject_id = '030' # recommend subject 30 for now\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\n\n# read the file\nraw = read_raw_bids(bids_path)\n# fix the annotations readin\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nTask: Extract a single channel and plot the whole timeseries.\n\n\n\n\n\n\nNote\n\n\n\nYou can directly interact with the raw object, e.g.¬†raw[1:10,1:5000], which extracts the first 10 channels and 5000 samples.\nYou can also use raw.get_data() to get the whole data as a numpy array.\n\n\n\nFor now we can use simple matplotlib to visualize the data, e.g.:\nfrom matplotlib import pyplot as plt\nplt.plot(raw[10,:][0].T)\n\nQuestion: What is the range of the data (in sense of min-y to max-y in ¬µ-volt)?\nTask: Have a look at raw.info and note down what the sampling frequency is (how many EEG-samples per second)"
  },
  {
    "objectID": "exercises/ex1_overview.html#epoching",
    "href": "exercises/ex1_overview.html#epoching",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "Task: We will epoch the data now. Formost we will cut the raw data to one channel using raw.pick([\"Cz\"]) - note that this will permanently change the raw object and removes alle other channels from memory. If you want rather a copy you could use raw_subselect = raw.copy().pick([\"Cz\"])).\nTask Let‚Äôs investigate the annotation markers. Have a look at raw.annotations. These values reflect the values in the bids *_events.tsv file (have a look at this file via ../local/bids/sub-030/sub-030_task-P3_events.tsv). BIDS is a new standard to share neuroimaging and other physiological data. It is not really a fileformat, but more of a folder & filename structure with some additional json files. I highly recommend to put your data into bids-format as soon as possible. It helps you stay organized and on top of things!\nTask MNE-speciality: We have to convert annotations to events with evts,evts_dict = mne.events_from_annotations(raw). Have a look at evts - it shows you the sample, the duration and event-id (with the look-up table evts_dict). In this case we only want to look at stimulus evoked responses, so we subset the event table (note: this could be done after epoching too)\n# get all keys which contain \"stimulus\"\nwanted_keys = [e for e in evts_dict.keys() if \"stimulus\" in e]\n# subset the large event-dictionairy\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\nTask Epoch the data with epochs = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1)\nTask Now that we have the epochs we should plot them. Plot all trials ‚Äòmanually‚Äô, (without using mne‚Äôs functionality) (epochs.get_data()).\n\n\n\n\n\n\nNote\n\n\n\nYou should plot one line per trial.\n\n\nQuestion What is the scale-range of the epoched data now?"
  },
  {
    "objectID": "exercises/ex1_overview.html#my-first-erp",
    "href": "exercises/ex1_overview.html#my-first-erp",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 1]",
    "section": "",
    "text": "Task But which epochs belong to targets and which to distractors? This is hidden in the event-description. Using the following lines you can find out which indices belong to which trial-types\ntarget = [\"stimulus:{}{}\".format(k,k) for k in [1,2,3,4,5]]\ndistractor = [\"stimulus:{}{}\".format(k,j) for k in [1,2,3,4,5] for j in [1,2,3,4,5] if k!=j]\nNow index the epochs evoked = epochs[index].average() and average them. You can then plot them either via evoked.plot() or with mne.viz.plot_compare_evokeds([evokedA,evokedB]).\nQuestion What is the unit/scale of the data now? Set it into context to the other two scales you reported before"
  },
  {
    "objectID": "exercises/ex6_linearModels.html",
    "href": "exercises/ex6_linearModels.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "T Load the data into a pandas dataframe. A very useful concept here is the concept of a tidy dataframe. The idea is, that every observation is one row of a table, where columns are potential features / descriptors + the dependent variable (the average activity between 130 and 200ms at electrode PO8 here).\nimport pandas as pd\nd = pd.read_csv(\"ex6_N170.csv\",delimiter=\",\")\nT: Use a plottinglibrary of your choice to visualise the simple scatter plot between some/all variables. I recommend packages seaborn e.g.¬†pairplot) or plotnine for this. They make it easy to split up plots of continuous variables (e.g.¬†baseline vs.¬†PO8) by a categorical variable, e.g.¬†cond (scrambled/intact).\nQ: Can you already guess what the relationships between the variables are, solely based on the plots?\n\n\nWe have to generate a DesignMatrix (\\(X\\)) in order to fit our model. The designmatrix has to reflect our study design. The simplest designmatrix consists only of ‚Äú1‚Äù, one for each data-point, which would mean that we fit only one parameter summarizing the whole dataset.\nT: Generate this designmatrix and fit it using the pseudo-inverse: \\((X^TX)^{-1}X^TY\\). I recommend to generate a function similar to this:\ndef solve(*args):\n    # generate designmatrix by stacking\n    X = np.stack(args).T\n    return PSEUDOINVERSE\nBecause we will need it quite often.\nQ: What does the resulting value mean? Note: The result is often called a ‚Äúbeta‚Äù\nNote: While the pseudoinverse is great because it is easy to understand what is going on, typically we would not directly use it but rather go over a Cholesky-Decomposition first. This will greatly increase numeric stability. But right now we do not care much about that\nT: Now we add the condition cond to the designmarix (thus PO8 ~ intercept + cond) which differentiates between intact and scrambled images. Because this is a categorical variable we have to encode it first. For that, we need a ‚Äú1‚Äù when images are intact and a ‚Äú0‚Äù if they were scrambled. Fit the model\nQ: What do the outputs mean now?\nT: Next add a predictor for stimulus (PO8 ~ intercept + cond + stim) and fit it.\nQ: What changed to the previous modelfit - Bonus: An idea why the size of the change is the size it is? Hint: It has to do with correlations\nT: And an interaction (column of stimulus * column of cond)\nQ: What do the betas/results/parameters mean now? (You might want to do the plotting of the next step to help your interpretation)\nNow we have what we call a 2x2 design. Two categorical factors (often only ‚Äúfactor‚Äù) with two levels each + the interaction.\nT: It is about time to plot our results! Put cond on the x, and use color for stimulus. We will reconstruct our original data first:\nYou should reconstruct 4 values, one for intact faces, one for scrambled faces, one for intact cars and one for scrambled cars. It migt be helpful to explicitly write down the models (\\(y  = 1*\\beta_0 + ...\\) )"
  },
  {
    "objectID": "exercises/ex6_linearModels.html#statistical-analysis-of-n170-area-using-linear-regression",
    "href": "exercises/ex6_linearModels.html#statistical-analysis-of-n170-area-using-linear-regression",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "T Load the data into a pandas dataframe. A very useful concept here is the concept of a tidy dataframe. The idea is, that every observation is one row of a table, where columns are potential features / descriptors + the dependent variable (the average activity between 130 and 200ms at electrode PO8 here).\nimport pandas as pd\nd = pd.read_csv(\"ex6_N170.csv\",delimiter=\",\")\nT: Use a plottinglibrary of your choice to visualise the simple scatter plot between some/all variables. I recommend packages seaborn e.g.¬†pairplot) or plotnine for this. They make it easy to split up plots of continuous variables (e.g.¬†baseline vs.¬†PO8) by a categorical variable, e.g.¬†cond (scrambled/intact).\nQ: Can you already guess what the relationships between the variables are, solely based on the plots?\n\n\nWe have to generate a DesignMatrix (\\(X\\)) in order to fit our model. The designmatrix has to reflect our study design. The simplest designmatrix consists only of ‚Äú1‚Äù, one for each data-point, which would mean that we fit only one parameter summarizing the whole dataset.\nT: Generate this designmatrix and fit it using the pseudo-inverse: \\((X^TX)^{-1}X^TY\\). I recommend to generate a function similar to this:\ndef solve(*args):\n    # generate designmatrix by stacking\n    X = np.stack(args).T\n    return PSEUDOINVERSE\nBecause we will need it quite often.\nQ: What does the resulting value mean? Note: The result is often called a ‚Äúbeta‚Äù\nNote: While the pseudoinverse is great because it is easy to understand what is going on, typically we would not directly use it but rather go over a Cholesky-Decomposition first. This will greatly increase numeric stability. But right now we do not care much about that\nT: Now we add the condition cond to the designmarix (thus PO8 ~ intercept + cond) which differentiates between intact and scrambled images. Because this is a categorical variable we have to encode it first. For that, we need a ‚Äú1‚Äù when images are intact and a ‚Äú0‚Äù if they were scrambled. Fit the model\nQ: What do the outputs mean now?\nT: Next add a predictor for stimulus (PO8 ~ intercept + cond + stim) and fit it.\nQ: What changed to the previous modelfit - Bonus: An idea why the size of the change is the size it is? Hint: It has to do with correlations\nT: And an interaction (column of stimulus * column of cond)\nQ: What do the betas/results/parameters mean now? (You might want to do the plotting of the next step to help your interpretation)\nNow we have what we call a 2x2 design. Two categorical factors (often only ‚Äúfactor‚Äù) with two levels each + the interaction.\nT: It is about time to plot our results! Put cond on the x, and use color for stimulus. We will reconstruct our original data first:\nYou should reconstruct 4 values, one for intact faces, one for scrambled faces, one for intact cars and one for scrambled cars. It migt be helpful to explicitly write down the models (\\(y  = 1*\\beta_0 + ...\\) )"
  },
  {
    "objectID": "exercises/ex6_linearModels.html#changing-bases",
    "href": "exercises/ex6_linearModels.html#changing-bases",
    "title": "üß† EEG-2024",
    "section": "Changing Bases",
    "text": "Changing Bases\nWe discussed in the lecture that it is possible to change the bases. We will go back to the simple example of of Intercept + cond. But in addition of dummy coding (intact == 1, scrambled == 0), we will fit a second model with effect-coding (intact == 0.5, scrambled = -0.5).\nQ: How do the betas compare of the two models? How does the interpretation change?\nHint If you need more you can read two of my tutorial-blogposts on this topic here and here :)\nT: Now we run the full 2x2 model once with dummy and once with effect-coding. The interaction of an effect-coded model is still just the designmatrix columns multiplied with eachother\nQ: Can you put the results together, why do they have the results they have?\n\nContinuous Regressors\nNow it is time to involve our continuous regressor. In this case it is the baseline-value. We are following here the relatively new approach of Alday 2019 and instead of subtracting a baseline, we will regress it out. We did not talk about baseline correction in the lecture, this is something I will talk about at the end of the course.\nIn theory baseline corrections are not needed, the baseline (i.e.¬†what happens before stimulus onset, thus ‚Äúnegative‚Äù time in an ERP) should be flat / noise around 0, because stimulus order is random. But in practice, we only have limited number of trials and limited randomization. Thus it might be, that we have a bias with in one condition more residual drift / low-frequency activity than in the other. This will ‚Äúmove‚Äù the whole ERP curve up/down and bias results later in the epoch.\nClassically, baselines are simply subtracted. Thus every point of an ERP recieves the ‚Äúsame‚Äù baseline correction. This is equivalent to adding a known parameter to our model: y ~ b0 * constant + b1 * cond + 1*BSL. What we will do instead is the 2020-version, we regress the baseline. This allows us to remove less of the baseline activity (or more, but rarely happens).\nT: Plot the PO8 actiity against the baseline (you might have done this plot at the beginning of the exercise). Split it up by cond & stim\nT: Add the baseline as a predictor to your 2x2 model.\nQ: What is the resulting beta?\nTypically we do not only remove the overall baseline, but condition-indivudal baselines.\nT: Thus we have to generate interactions with all predictors & interaction of the interactions too\nQ: What is your interpretation of the resulting betas? For which conditions do we really need a baseline correction and how strong should it be?"
  },
  {
    "objectID": "exercises/ex6_linearModels.html#bonus-uncertainty-standard-errors",
    "href": "exercises/ex6_linearModels.html#bonus-uncertainty-standard-errors",
    "title": "üß† EEG-2024",
    "section": "Bonus: Uncertainty & Standard Errors",
    "text": "Bonus: Uncertainty & Standard Errors\nThis was not part of the lecture, but might be interesting nonetheless. In this exercise we talk about how we can generate errorbars to add to our 2x2 plot.\nWe can not only estimate the \\(\\hat{\\beta}\\), our estimated parameters, but also the variance of them, giving us a handle of uncertainty. We just need some additional assumptions (normal residuals). For a derivation see this.\nIt turns out, that \\(Var(\\beta) = \\sigma^2 (X^TX)^{-1}\\)\nT: Implement this formula. The sqrt of the diagonal elements are your Standard-Errors, which we can use as error-bars. The \\(\\sigma^2\\) is the variance of the residuals\nNow we have to apply it including our contrast vectors e.g.¬†c = [1 0 0 0*0], or c=[1 0 1 1*0]\n\\[se_c = \\sigma^2 c^T (X^TX)^{-1} c\\]"
  },
  {
    "objectID": "exercises/ex3_filter.html",
    "href": "exercises/ex3_filter.html",
    "title": "Exercise 3: Filtering",
    "section": "",
    "text": "We start out with a simple signal:\nfrom numpy import cos, sin, pi,  arange\n\nsample_rate = 100.0\nnsamples = 400 # fixed at 11.11.2020\nt = arange(nsamples) / sample_rate\nx = cos(2*pi*0.5*t) + 0.2*sin(2*pi*2.5*t+0.1) + \\\n        0.2*sin(2*pi*15.3*t) + 0.1*sin(2*pi*16.7*t + 0.1) + \\\n            0.1*sin(2*pi*23.45*t+.8)\nTask: Plot the signal against time."
  },
  {
    "objectID": "exercises/ex3_filter.html#simulate-a-simple-eeg-signal",
    "href": "exercises/ex3_filter.html#simulate-a-simple-eeg-signal",
    "title": "Exercise 3: Filtering",
    "section": "",
    "text": "We start out with a simple signal:\nfrom numpy import cos, sin, pi,  arange\n\nsample_rate = 100.0\nnsamples = 400 # fixed at 11.11.2020\nt = arange(nsamples) / sample_rate\nx = cos(2*pi*0.5*t) + 0.2*sin(2*pi*2.5*t+0.1) + \\\n        0.2*sin(2*pi*15.3*t) + 0.1*sin(2*pi*16.7*t + 0.1) + \\\n            0.1*sin(2*pi*23.45*t+.8)\nTask: Plot the signal against time."
  },
  {
    "objectID": "exercises/ex3_filter.html#transform-to-fourier-space",
    "href": "exercises/ex3_filter.html#transform-to-fourier-space",
    "title": "Exercise 3: Filtering",
    "section": "Transform to fourier space",
    "text": "Transform to fourier space\n\n\n\n\n\n\nTask\n\n\n\n\nrun the fourier analysis via fft.\nplot the full log10-magnitude\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you plot e.g.¬†the magnitude you will notice it is mirrorsymmetric around the middle! This is because FFT is actually defined for complex signals. Because we only have real signals, practically half of the FFT is redundant. But this also means, all of our manual-filters in the next steps need to take the mirror-symmetry into account. For this reason, typically a rfft function exists, returning only the real part."
  },
  {
    "objectID": "exercises/ex3_filter.html#a-simple-filter",
    "href": "exercises/ex3_filter.html#a-simple-filter",
    "title": "Exercise 3: Filtering",
    "section": "A simple filter",
    "text": "A simple filter\nA very simple frequency filter is to set the unwanted amplitude of the fourier components to something close to zero. We have to be a tad careful not to remove the phase information though.\n\n\n\n\n\n\nYour tasks are therefore\n\n\n\n\nsplit the complex fourier result into angle and magnitude\nset the respective magnitudes to zero (we start with a lowpass filter: magnitude[30:370] = 0)\ncombine angle and magnitude back to a complex fourier coefficient (\\(m*e^{1j*ang}\\))\napply the inverse FFT\nplot the signal with what you started out"
  },
  {
    "objectID": "exercises/ex3_filter.html#highpass-instead-of-lowpass",
    "href": "exercises/ex3_filter.html#highpass-instead-of-lowpass",
    "title": "Exercise 3: Filtering",
    "section": "Highpass instead of lowpass",
    "text": "Highpass instead of lowpass\n\n\n\n\n\n\nTask\n\n\n\nRepeat the steps from above, but this time, remove the low frequency components"
  },
  {
    "objectID": "exercises/ex3_filter.html#what-happens-to-the-frequency-and-time-response-if-we-add-artefacts",
    "href": "exercises/ex3_filter.html#what-happens-to-the-frequency-and-time-response-if-we-add-artefacts",
    "title": "Exercise 3: Filtering",
    "section": "What happens to the frequency and time response if we add ‚Äúartefacts‚Äù?",
    "text": "What happens to the frequency and time response if we add ‚Äúartefacts‚Äù?\nLet‚Äôs see how the FFT reacts to steps and spikes. A step-function could look like this:\n\n\n\n\n\n\n\nTask\n\n\n\nAdd a DC-offset (a step-function) starting from x[200:] and investigate the fourier space. Filter it again (low or high pass) and transfer it back to the time domain and investigate the signal around the step."
  },
  {
    "objectID": "exercises/ex3_filter.html#impulse-response-function",
    "href": "exercises/ex3_filter.html#impulse-response-function",
    "title": "Exercise 3: Filtering",
    "section": "Impulse Response Function",
    "text": "Impulse Response Function\nTo get a bit deeper understanding of what is going on, have a look at the fourier transform of a impulse signal (e.g.¬†1:400 =&gt; 0. and 200 =&gt; 1.).\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat do you observe?\nWhy did we see ringing in the previous filtered step-function, if when filtering we put coefficients to 0 - after all, we are not adding something?"
  },
  {
    "objectID": "exercises/ex3_filter.html#filtering-eeg-data",
    "href": "exercises/ex3_filter.html#filtering-eeg-data",
    "title": "Exercise 3: Filtering",
    "section": "Filtering EEG data",
    "text": "Filtering EEG data\nUsually we would not built our own filters - but understanding the properties is still very important! We will load our data from last time again:\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nimport importlib\nimport ccs_eeg_utils\n\nbids_root = \"../local/bids\"\nsubject_id = '030'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\n\n\n\n\n\n\nTask\n\n\n\n\nChoose the channel ‚ÄúPz‚Äù, plot the channel\nPlot the fourier space using raw.plot_psd()\n\n\n\nNow we filter using raw.filter(), specify a highpass of 0.5Hz and a lowpass of 50Hz.\n\n\n\n\n\n\nTask\n\n\n\n\nPlot the fourier spectrum after filtering again.\nPlot the channel again, did the filter work as indented?\n\n\n\n\n\n\n\n\n\nBonus-task\n\n\n\nBonus: comparing over-filtered ERPs\nIf you want, you can compare the ERP with and without filtering. You can also use ‚Äúinvalid‚Äù filter settings - HP up to 2-5Hz, lowpass until 10-20Hz. I say invalid here, because usually with such ranges, you would filter out results that you are actually interested in.\nBonus: Electrical Artefacts\nInstead of notch filtering 50/60Hz artefacts, one can also try to regress it out in smarter ways. A good tool for this is Zap_Line with a python implementation here: https://github.com/nbara/python-meegkit/. There are also several robust detrending methods, which could potentially replace highpass filters in the future. But more work needs to be invested to see how results compare. These methods are not (yet) common."
  },
  {
    "objectID": "exercises/ex5_ICA.html",
    "href": "exercises/ex5_ICA.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "We will write simple ICA algorithm to try to return the original unmixed signals based on a simulation example.\nTask: load the simulation dataset by x = ccs_eeg_utils.simulate_ICA(dims=2) and plot it.\nRemember that for ICA we have to undo a scaling and a rotation. The scaling we can do by whitening our signal x. The sphering (=whitening) matrix is the inverse of the matrix-squareroot of the covariance matrix of x.\nIn order to whiten (x_white), we have to remove the mean of x and then calculate the matrix product with the sphering matrix sph*x.\nTask: Whiten the data!\nNow we are ready to try a extermely simple, naive, restricted and inefficient ICA algorithm:\nWe will rotate the signal using a 2D rotation matrix and for each rotatio calculate the sum of the absolute valued kurtosis (use e.g.¬†scp.stats.kurtosis). Later we will try to find the maximum value, which will be the rotation for the most non-normal signal - hopefully the unmixed one\nturn =lambda k: np.reshape(np.array([np.cos(k), np.sin(k), -np.sin(k), np.cos(k)]),(2,2))\nTask: Implement the following pseudocode\nfor t in 0:pi\n    x_bar = turn(t) * x_white\n    abs(kurtosis(x_bar))\nThe solution to ICA would be then argmax(kurtosis_list).\nTask: Plot the solution!\nBonus If you want to be fancy, you can also generate an animation linking kurtosis + rotating the datapoints + marginal histograms together\n\n\n\nNext we will use the infomax algorithm implemented in mne (you could try implementing it yourself, but I found an effective adjustment of learning rate quite tricky). you can call the infomax algorithm using mne.preprocessing.infomax(x.T,verbose=True).\nTask: Run it on x = ccs_eeg_utils.simulate_ICA(dims=4) and plot the signals before and after.\n\n\n\nNow we are ready to run ICA on real EEG data.\nTask: Select the dataset from a previous exercise and start the ICA\n\nYou could downsample the data to speed up ICA calculation via raw.resample(100).\n\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nbids_root = \"../local/bids\"\nsubject_id = '030'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\n\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\nraw.filter(0.5,50, fir_design='firwin')\nraw.set_montage('standard_1020',match_case=False)\n\nGenerate an ICA object ica= mne.preprocessing.ICA(method=\"fastica\")\nfit the ICA on the data. ica.fit(raw,verbose=True)\nAfter the fit (can take some minutes) you can plot the component using ica.plot_components()\nIf you want to inspect a specific component, you can select it using ica.plot_properties(raw,picks=[3]).\nIn order to exclude components, write: ica.exclude = [3,4,16]\n\nYou can then compare the EEG before & after using:\n# ica.apply() changes the Raw object in-place, so let's make a copy first:\nreconst_raw = raw.copy()\nica.apply(reconst_raw)\n\nraw.plot()\nreconst_raw.plot()  \nOr use one of MNEs functionalities: `ica.plot_overlay(raw,exclude=[1,8,9])"
  },
  {
    "objectID": "exercises/ex5_ICA.html#overview",
    "href": "exercises/ex5_ICA.html#overview",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "We will write simple ICA algorithm to try to return the original unmixed signals based on a simulation example.\nTask: load the simulation dataset by x = ccs_eeg_utils.simulate_ICA(dims=2) and plot it.\nRemember that for ICA we have to undo a scaling and a rotation. The scaling we can do by whitening our signal x. The sphering (=whitening) matrix is the inverse of the matrix-squareroot of the covariance matrix of x.\nIn order to whiten (x_white), we have to remove the mean of x and then calculate the matrix product with the sphering matrix sph*x.\nTask: Whiten the data!\nNow we are ready to try a extermely simple, naive, restricted and inefficient ICA algorithm:\nWe will rotate the signal using a 2D rotation matrix and for each rotatio calculate the sum of the absolute valued kurtosis (use e.g.¬†scp.stats.kurtosis). Later we will try to find the maximum value, which will be the rotation for the most non-normal signal - hopefully the unmixed one\nturn =lambda k: np.reshape(np.array([np.cos(k), np.sin(k), -np.sin(k), np.cos(k)]),(2,2))\nTask: Implement the following pseudocode\nfor t in 0:pi\n    x_bar = turn(t) * x_white\n    abs(kurtosis(x_bar))\nThe solution to ICA would be then argmax(kurtosis_list).\nTask: Plot the solution!\nBonus If you want to be fancy, you can also generate an animation linking kurtosis + rotating the datapoints + marginal histograms together"
  },
  {
    "objectID": "exercises/ex5_ICA.html#infomax",
    "href": "exercises/ex5_ICA.html#infomax",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "Next we will use the infomax algorithm implemented in mne (you could try implementing it yourself, but I found an effective adjustment of learning rate quite tricky). you can call the infomax algorithm using mne.preprocessing.infomax(x.T,verbose=True).\nTask: Run it on x = ccs_eeg_utils.simulate_ICA(dims=4) and plot the signals before and after."
  },
  {
    "objectID": "exercises/ex5_ICA.html#ica-on-eeg-data",
    "href": "exercises/ex5_ICA.html#ica-on-eeg-data",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "Now we are ready to run ICA on real EEG data.\nTask: Select the dataset from a previous exercise and start the ICA\n\nYou could downsample the data to speed up ICA calculation via raw.resample(100).\n\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nbids_root = \"../local/bids\"\nsubject_id = '030'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\n\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\nraw.filter(0.5,50, fir_design='firwin')\nraw.set_montage('standard_1020',match_case=False)\n\nGenerate an ICA object ica= mne.preprocessing.ICA(method=\"fastica\")\nfit the ICA on the data. ica.fit(raw,verbose=True)\nAfter the fit (can take some minutes) you can plot the component using ica.plot_components()\nIf you want to inspect a specific component, you can select it using ica.plot_properties(raw,picks=[3]).\nIn order to exclude components, write: ica.exclude = [3,4,16]\n\nYou can then compare the EEG before & after using:\n# ica.apply() changes the Raw object in-place, so let's make a copy first:\nreconst_raw = raw.copy()\nica.apply(reconst_raw)\n\nraw.plot()\nreconst_raw.plot()  \nOr use one of MNEs functionalities: `ica.plot_overlay(raw,exclude=[1,8,9])"
  },
  {
    "objectID": "exercises/solutions/ex4_ICA.html",
    "href": "exercises/solutions/ex4_ICA.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "import sys\nsys.path.insert(0,\"..\")\nimport ccs_eeg_utils\nx = ccs_eeg_utils.simulate_ICA(dims=2)\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt.plot(x.T);\n\n\n\n\n\n\n\n\n\nimport scipy as scp\nimport scipy.stats\ndef whiten(x):\n    mM = np.mean(x,axis=1)\n    sph = np.linalg.inv(scp.linalg.sqrtm(np.cov(x)))\n    xM = np.subtract(x.T,mM)\n    whiteX = np.matmul(sph,xM.T)\n    return whiteX\nwhiteX = whiten(x)\n\n\nplt.plot(whiteX.T);\n\n\n\n\n\n\n\n\n\nplt.plot(whiteX[0,:],whiteX[1,:],'bo')\nplt.plot(x[0,:],x[1,:],'ro');\n\n\n\n\n\n\n\n\n\n# arbitrary 2d rotation matrix\nturn =lambda k: np.reshape(np.array([np.cos(k), np.sin(k), -np.sin(k), np.cos(k)]),(2,2))\nkurt = []\n# rotate from 0 to pi\nr = np.linspace(0,2*np.pi,1000)\nfor l in r:\n    # turn \n    x_bar = np.matmul(turn(l),whiteX)\n    # calc kurtosis\n    kurt.append(np.sum(np.abs(scp.stats.kurtosis(x_bar,axis=1))))\n\n\nplt.plot(r/np.pi*180,kurt)\nbest_turn = r[np.argmax(kurt)]\nprint(best_turn/np.pi*180)\n\n224.14414414414412\n\n\n\n\n\n\n\n\n\n\nx_unmixed = np.matmul(turn(best_turn),whiteX)\n\n\nx_unmixed = np.matmul(turn(45. /180 * np.pi),whiteX)\nplt.plot(x_unmixed.T)\nx_unmixed = np.matmul(turn(135. /180 * np.pi),whiteX)\nplt.plot(x_unmixed.T)\nx_unmixed = np.matmul(turn((180. + 45.) /180 * np.pi),whiteX)\nplt.plot(x_unmixed.T);\n\n\nplt.plot(x.T);\n\n\nplt.plot(x_unmixed.T);\n\n\n\n\n\n\n\n\n\nx = ccs_eeg_utils.simulate_ICA(dims=4)\nplt.plot(x.T);\n\n\n\n\n\n\n\n\n\nimport mne\nunmix = mne.preprocessing.infomax(x.T,verbose=True)\n\nComputing Extended Infomax ICA\nstep 1 - lrate 0.007213, wchange 439.64796958, angledelta  0.0 deg\nstep 2 - lrate 0.007213, wchange 147.64923695, angledelta  0.0 deg\nstep 3 - lrate 0.007213, wchange 9.17729304, angledelta 149.8 deg\nstep 4 - lrate 0.006492, wchange 3.00857525, angledelta 152.3 deg\nstep 5 - lrate 0.005843, wchange 0.28983694, angledelta 131.5 deg\nstep 6 - lrate 0.005259, wchange 2.55396760, angledelta 63.3 deg\nstep 7 - lrate 0.004733, wchange 1.25576871, angledelta 129.9 deg\nstep 8 - lrate 0.004259, wchange 3.60682907, angledelta 104.2 deg\nstep 9 - lrate 0.003834, wchange 0.78706908, angledelta 138.9 deg\nstep 10 - lrate 0.003450, wchange 0.15855737, angledelta 109.7 deg\nstep 11 - lrate 0.003105, wchange 0.29316184, angledelta 73.3 deg\nstep 12 - lrate 0.002795, wchange 0.66305510, angledelta 107.9 deg\nstep 13 - lrate 0.002515, wchange 0.76104828, angledelta 152.1 deg\nstep 14 - lrate 0.002264, wchange 1.39815136, angledelta 96.6 deg\nstep 15 - lrate 0.002037, wchange 0.16916776, angledelta 38.6 deg\nstep 16 - lrate 0.002037, wchange 1.17013022, angledelta 164.7 deg\nstep 17 - lrate 0.001834, wchange 0.95017242, angledelta 162.0 deg\nstep 18 - lrate 0.001650, wchange 0.58350322, angledelta 138.0 deg\nstep 19 - lrate 0.001485, wchange 0.34488643, angledelta 172.9 deg\nstep 20 - lrate 0.001337, wchange 0.27546380, angledelta 122.3 deg\nstep 21 - lrate 0.001203, wchange 0.06082744, angledelta 62.4 deg\nstep 22 - lrate 0.001083, wchange 0.05624716, angledelta 153.8 deg\nstep 23 - lrate 0.000974, wchange 0.02444557, angledelta  9.0 deg\nstep 24 - lrate 0.000974, wchange 0.01625955, angledelta 70.0 deg\nstep 25 - lrate 0.000877, wchange 0.06649628, angledelta 77.1 deg\nstep 26 - lrate 0.000789, wchange 0.03459649, angledelta 118.9 deg\nstep 27 - lrate 0.000710, wchange 0.14180648, angledelta 51.9 deg\nstep 28 - lrate 0.000710, wchange 0.10072199, angledelta 112.3 deg\nstep 29 - lrate 0.000639, wchange 0.02099114, angledelta 74.9 deg\nstep 30 - lrate 0.000575, wchange 0.06687893, angledelta 161.8 deg\nstep 31 - lrate 0.000518, wchange 0.05010068, angledelta 122.5 deg\nstep 32 - lrate 0.000466, wchange 0.01869956, angledelta 28.7 deg\nstep 33 - lrate 0.000466, wchange 0.00207146, angledelta 143.1 deg\nstep 34 - lrate 0.000419, wchange 0.00124883, angledelta 147.8 deg\nstep 35 - lrate 0.000378, wchange 0.00626026, angledelta 155.0 deg\nstep 36 - lrate 0.000340, wchange 0.00540405, angledelta 101.6 deg\nstep 37 - lrate 0.000306, wchange 0.01101259, angledelta 109.0 deg\nstep 38 - lrate 0.000275, wchange 0.00034382, angledelta 116.9 deg\nstep 39 - lrate 0.000248, wchange 0.00202240, angledelta 76.1 deg\nstep 40 - lrate 0.000223, wchange 0.00198221, angledelta 158.9 deg\nstep 41 - lrate 0.000201, wchange 0.00030651, angledelta 169.6 deg\nstep 42 - lrate 0.000181, wchange 0.00051933, angledelta 121.4 deg\nstep 43 - lrate 0.000163, wchange 0.00006583, angledelta 116.9 deg\nstep 44 - lrate 0.000146, wchange 0.00037886, angledelta 49.7 deg\nstep 45 - lrate 0.000146, wchange 0.00019064, angledelta 37.9 deg\nstep 46 - lrate 0.000146, wchange 0.00078735, angledelta 135.0 deg\nstep 47 - lrate 0.000132, wchange 0.00014887, angledelta 136.9 deg\nstep 48 - lrate 0.000118, wchange 0.00008778, angledelta 163.6 deg\nstep 49 - lrate 0.000107, wchange 0.00029554, angledelta 70.6 deg\nstep 50 - lrate 0.000096, wchange 0.00001016, angledelta 61.5 deg\nstep 51 - lrate 0.000086, wchange 0.00000880, angledelta 71.6 deg\nstep 52 - lrate 0.000078, wchange 0.00003686, angledelta 11.5 deg\nstep 53 - lrate 0.000078, wchange 0.00026652, angledelta 105.2 deg\nstep 54 - lrate 0.000070, wchange 0.00006390, angledelta 59.8 deg\nstep 55 - lrate 0.000070, wchange 0.00005258, angledelta 44.3 deg\nstep 56 - lrate 0.000070, wchange 0.00017507, angledelta 59.3 deg\nstep 57 - lrate 0.000070, wchange 0.00001763, angledelta 127.2 deg\nstep 58 - lrate 0.000063, wchange 0.00000926, angledelta 126.4 deg\nstep 59 - lrate 0.000057, wchange 0.00005094, angledelta 161.4 deg\nstep 60 - lrate 0.000051, wchange 0.00016853, angledelta 32.4 deg\nstep 61 - lrate 0.000051, wchange 0.00002469, angledelta 108.8 deg\nstep 62 - lrate 0.000046, wchange 0.00012241, angledelta 130.7 deg\nstep 63 - lrate 0.000041, wchange 0.00000320, angledelta 31.1 deg\nstep 64 - lrate 0.000041, wchange 0.00002304, angledelta 22.5 deg\nstep 65 - lrate 0.000041, wchange 0.00000213, angledelta 24.7 deg\nstep 66 - lrate 0.000041, wchange 0.00001618, angledelta 165.4 deg\nstep 67 - lrate 0.000037, wchange 0.00000193, angledelta 154.0 deg\nstep 68 - lrate 0.000033, wchange 0.00000200, angledelta 144.9 deg\nstep 69 - lrate 0.000030, wchange 0.00002110, angledelta 27.7 deg\nstep 70 - lrate 0.000030, wchange 0.00000228, angledelta 83.8 deg\nstep 71 - lrate 0.000027, wchange 0.00000426, angledelta 105.2 deg\nstep 72 - lrate 0.000024, wchange 0.00000085, angledelta 92.1 deg\nstep 73 - lrate 0.000022, wchange 0.00000317, angledelta 15.6 deg\nstep 74 - lrate 0.000022, wchange 0.00000609, angledelta 127.8 deg\nstep 75 - lrate 0.000020, wchange 0.00000126, angledelta 15.3 deg\nstep 76 - lrate 0.000020, wchange 0.00000007, angledelta 130.4 deg\nstep 77 - lrate 0.000018, wchange 0.00000946, angledelta 51.8 deg\nstep 78 - lrate 0.000018, wchange 0.00000530, angledelta 137.4 deg\nstep 79 - lrate 0.000016, wchange 0.00000548, angledelta 157.8 deg\nstep 80 - lrate 0.000014, wchange 0.00000268, angledelta 146.0 deg\nstep 81 - lrate 0.000013, wchange 0.00000071, angledelta 57.8 deg\nstep 82 - lrate 0.000013, wchange 0.00000841, angledelta 152.2 deg\nstep 83 - lrate 0.000012, wchange 0.00000146, angledelta 41.2 deg\nstep 84 - lrate 0.000012, wchange 0.00000180, angledelta 81.0 deg\nstep 85 - lrate 0.000010, wchange 0.00000109, angledelta 171.8 deg\nstep 86 - lrate 0.000009, wchange 0.00000265, angledelta 155.1 deg\nstep 87 - lrate 0.000009, wchange 0.00000040, angledelta 144.5 deg\nstep 88 - lrate 0.000008, wchange 0.00000075, angledelta 36.7 deg\nstep 89 - lrate 0.000008, wchange 0.00000530, angledelta 51.5 deg\nstep 90 - lrate 0.000008, wchange 0.00000024, angledelta 159.8 deg\nstep 91 - lrate 0.000007, wchange 0.00000045, angledelta 40.6 deg\nstep 92 - lrate 0.000007, wchange 0.00000053, angledelta 166.0 deg\nstep 93 - lrate 0.000006, wchange 0.00000037, angledelta 12.6 deg\nstep 94 - lrate 0.000006, wchange 0.00000027, angledelta 110.0 deg\nstep 95 - lrate 0.000006, wchange 0.00000048, angledelta 61.6 deg\nstep 96 - lrate 0.000005, wchange 0.00000005, angledelta 43.1 deg\nstep 97 - lrate 0.000005, wchange 0.00000013, angledelta 60.4 deg\nstep 98 - lrate 0.000005, wchange 0.00000003, angledelta 103.1 deg\nstep 99 - lrate 0.000004, wchange 0.00000017, angledelta 87.6 deg\nstep 100 - lrate 0.000004, wchange 0.00000041, angledelta 88.9 deg\nstep 101 - lrate 0.000003, wchange 0.00000010, angledelta 60.6 deg\nstep 102 - lrate 0.000003, wchange 0.00000004, angledelta 61.8 deg\nstep 103 - lrate 0.000003, wchange 0.00000020, angledelta 129.4 deg\nstep 104 - lrate 0.000002, wchange 0.00000007, angledelta 134.9 deg\nstep 105 - lrate 0.000002, wchange 0.00000022, angledelta 149.0 deg\nstep 106 - lrate 0.000002, wchange 0.00000001, angledelta 50.1 deg\nstep 107 - lrate 0.000002, wchange 0.00000007, angledelta 121.0 deg\nstep 108 - lrate 0.000002, wchange 0.00000000, angledelta 89.5 deg\nstep 109 - lrate 0.000002, wchange 0.00000002, angledelta 54.0 deg\nstep 110 - lrate 0.000002, wchange 0.00000002, angledelta 94.1 deg\nstep 111 - lrate 0.000001, wchange 0.00000001, angledelta 49.5 deg\nstep 112 - lrate 0.000001, wchange 0.00000003, angledelta 163.3 deg\nstep 113 - lrate 0.000001, wchange 0.00000004, angledelta 110.1 deg\nstep 114 - lrate 0.000001, wchange 0.00000003, angledelta 166.8 deg\nstep 115 - lrate 0.000001, wchange 0.00000007, angledelta 139.8 deg\nstep 116 - lrate 0.000001, wchange 0.00000001, angledelta 19.0 deg\nstep 117 - lrate 0.000001, wchange 0.00000000, angledelta 51.7 deg\nstep 118 - lrate 0.000001, wchange 0.00000001, angledelta 130.8 deg\nstep 119 - lrate 0.000001, wchange 0.00000000, angledelta 28.1 deg\nstep 120 - lrate 0.000001, wchange 0.00000001, angledelta 133.0 deg\nstep 121 - lrate 0.000001, wchange 0.00000000, angledelta  6.8 deg\nstep 122 - lrate 0.000001, wchange 0.00000000, angledelta 149.0 deg\nstep 123 - lrate 0.000001, wchange 0.00000001, angledelta 23.5 deg\nstep 124 - lrate 0.000001, wchange 0.00000000, angledelta 163.7 deg\nstep 125 - lrate 0.000001, wchange 0.00000001, angledelta 135.3 deg\nstep 126 - lrate 0.000001, wchange 0.00000001, angledelta 22.6 deg\nstep 127 - lrate 0.000001, wchange 0.00000000, angledelta 171.8 deg\nstep 128 - lrate 0.000000, wchange 0.00000001, angledelta 101.7 deg\nstep 129 - lrate 0.000000, wchange 0.00000000, angledelta 93.2 deg\nstep 130 - lrate 0.000000, wchange 0.00000000, angledelta 50.7 deg\nstep 131 - lrate 0.000000, wchange 0.00000000, angledelta 106.7 deg\nstep 132 - lrate 0.000000, wchange 0.00000000, angledelta 154.1 deg\nstep 133 - lrate 0.000000, wchange 0.00000000, angledelta 160.3 deg\nstep 134 - lrate 0.000000, wchange 0.00000001, angledelta 115.0 deg\nstep 135 - lrate 0.000000, wchange 0.00000000, angledelta 160.6 deg\nstep 136 - lrate 0.000000, wchange 0.00000000, angledelta 108.2 deg\nstep 137 - lrate 0.000000, wchange 0.00000000, angledelta 141.6 deg\nstep 138 - lrate 0.000000, wchange 0.00000000, angledelta 102.3 deg\nstep 139 - lrate 0.000000, wchange 0.00000000, angledelta  4.0 deg\nstep 140 - lrate 0.000000, wchange 0.00000000, angledelta 122.3 deg\nstep 141 - lrate 0.000000, wchange 0.00000000, angledelta 119.4 deg\nstep 142 - lrate 0.000000, wchange 0.00000000, angledelta 22.7 deg\nstep 143 - lrate 0.000000, wchange 0.00000000, angledelta  2.9 deg\nstep 144 - lrate 0.000000, wchange 0.00000000, angledelta 103.7 deg\nstep 145 - lrate 0.000000, wchange 0.00000000, angledelta 133.8 deg\nstep 146 - lrate 0.000000, wchange 0.00000000, angledelta 167.2 deg\nstep 147 - lrate 0.000000, wchange 0.00000000, angledelta 53.3 deg\nstep 148 - lrate 0.000000, wchange 0.00000000, angledelta 102.1 deg\nstep 149 - lrate 0.000000, wchange 0.00000000, angledelta 115.0 deg\nstep 150 - lrate 0.000000, wchange 0.00000000, angledelta 115.1 deg\nstep 151 - lrate 0.000000, wchange 0.00000000, angledelta 17.0 deg\nstep 152 - lrate 0.000000, wchange 0.00000000, angledelta 112.9 deg\nstep 153 - lrate 0.000000, wchange 0.00000000, angledelta 12.8 deg\nstep 154 - lrate 0.000000, wchange 0.00000000, angledelta 16.6 deg\nstep 155 - lrate 0.000000, wchange 0.00000000, angledelta 22.6 deg\nstep 156 - lrate 0.000000, wchange 0.00000000, angledelta 74.6 deg\nstep 157 - lrate 0.000000, wchange 0.00000000, angledelta 133.8 deg\nstep 158 - lrate 0.000000, wchange 0.00000000, angledelta 96.2 deg\n\n\n\nplt.plot(np.dot(unmix,x).T)\nfig, axs = plt.subplots(2, 2)\naxs[0, 0].plot(np.dot(unmix,x)[0,:])\naxs[0, 0].plot(np.dot(unmix,x)[0,:])\naxs[1, 0].plot(np.dot(unmix,x)[1,:])\naxs[0, 1].plot(np.dot(unmix,x)[2,:])\naxs[1, 1].plot(np.dot(unmix,x)[3,:]);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nbids_root = \"../local/bids/\"\nbids_root = \"/bigpool/export/users/ehinger/erp-core/bids\" # Bene's Server location\nbids_root = \"/store/data/erp-core/\"\nsubject_id = '003'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",\n                     datatype='eeg', suffix='eeg',session=\"P3\",\n                     root=bids_root)\nprint(bids_path)\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\nraw.filter(0.5,50, fir_design='firwin')\nraw.set_montage('standard_1020',match_case=False)\n\n\n/store/data/erp-core/sub-003/ses-P3/eeg/sub-003_ses-P3_task-P3_eeg.set\nReading /store/data/erp-core/sub-003/ses-P3/eeg/sub-003_ses-P3_task-P3_eeg.fdt\nReading events from /store/data/erp-core/sub-003/ses-P3/eeg/sub-003_ses-P3_task-P3_events.tsv.\nThe event \"response\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\nThe event \"stimulus\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/24\n\n\n/tmp/ipykernel_1691702/3619079600.py:13: RuntimeWarning: Data file name in EEG.data (sub-003_task-P3_eeg.fdt) is incorrect, the file name must have changed on disk, using the correct file name (sub-003_ses-P3_task-P3_eeg.fdt).\n  raw = read_raw_bids(bids_path)\n\n\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/14\nReading channel info from /store/data/erp-core/sub-003/ses-P3/eeg/sub-003_ses-P3_task-P3_channels.tsv.\nReading 0 ... 386047  =      0.000 ...   376.999 secs...\nFiltering raw data in 1 contiguous segment\nSetting up band-pass filter from 0.5 - 50 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 0.50\n- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n- Upper passband edge: 50.00 Hz\n- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n- Filter length: 6759 samples (6.601 sec)\n\n\n\n\n    \n        Measurement date\n        \n        Unknown\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n            \n            sub-003\n            \n        \n    \n    \n        Digitized points\n        \n        0 points\n        \n    \n    \n        Good channels\n        30 EEG, 3 EOG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        HEOG_left, HEOG_right, VEOG_lower\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        1024.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.50 Hz\n    \n    \n    \n    \n        Lowpass\n        50.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        sub-003_ses-P3_task-P3_eeg.fdt\n    \n    \n    \n        Duration\n        00:06:16 (HH:MM:SS)\n    \n\n\n\n\n    ica = mne.preprocessing.ICA(method=\"picard\") # picard is fast and supposedly good; infomax is slow but good\n    ica.fit(raw,verbose=True)\n    \n\nFitting ICA to data using 30 channels (please be patient, this may take a while)\nSelecting by non-zero PCA components: 30 components\nFitting ICA took 32.8s.\n\n\n\n\n\n\nMethod\npicard\n\n\nFit\n92 iterations on raw data (386048 samples)\n\n\nICA components\n30\n\n\nExplained variance\n100.0¬†%\n\n\nAvailable PCA components\n30\n\n\nChannel types\neeg\n\n\nICA components marked for exclusion\n‚Äî\n\n\n\n\n\n\n\nica.plot_components(range(20));\n\n\n\n\n\n\n\n\n\nicaact = ica.get_sources(raw)\nplt.plot(icaact[1,0:20000][0].T);\n#plt.plot(raw[0,0:20000][0].T)\n\n\n\n\n\n\n\n\n\nica.plot_properties(raw,picks=[0,1,7],psd_args={'fmax': 35.},reject=None);\n\n    Using multitaper spectrum estimation with 7 DPSS windows\nNot setting metadata\n188 matching events found\nNo baseline correction applied\n0 projection items activated\n0 bad epochs dropped\n\n\n\n\n\n\n\n\n\n\nica_infomax = mne.preprocessing.ICA(method=\"infomax\") # picard is fast and supposedly good; infomax is slow but good\nica_infomax.fit(raw,verbose=True)\n    \n\nFitting ICA to data using 30 channels (please be patient, this may take a while)\nSelecting by non-zero PCA components: 30 components\n \n\n\n/home/ehinger/miniconda3/envs/eegCourse/lib/python3.10/site-packages/mne/preprocessing/infomax_.py:192: RuntimeWarning: overflow encountered in exp\n  y = 1.0 / (1.0 + np.exp(-u))\n\n\nFitting ICA took 281.0s.\n\n\n\n\n\n\nMethod\ninfomax\n\n\nFit\n500 iterations on raw data (386048 samples)\n\n\nICA components\n30\n\n\nExplained variance\n100.0¬†%\n\n\nAvailable PCA components\n30\n\n\nChannel types\neeg\n\n\nICA components marked for exclusion\n‚Äî\n\n\n\n\n\n\n\nimport mne\nevts,evts_dict = mne.events_from_annotations(raw)\nwanted_keys = [e for e in evts_dict.keys() if \"response\" in e]\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\n\n\nreconst_raw = raw.copy()\n#ica.apply(reconst_raw,exclude=[1,8,9])\n\n#raw.plot()\n#reconst_raw.plot()  \nica.plot_overlay(raw,exclude=[1,8,9]);\n\nwhiteX = whiten(x)\nw = np.eye(x.shape[0])\n#f = lambda x: np.tanh(x) #f = lambda x: x-np.tanh(x) I = np.eye(x.shape[0]) #f =lambda x: 1./(1+np.exp(-x)) f = lambda x: (2 / (1 + np.exp(-x))) lr = 0.01\nbias1 = np.zeros((x.shape[0], 1)) for k in range(0,50000): u = np.matmul(w,whiteX)\n#A = np.matmul(f(u),u.T)\n#A = np.dot(f(u),u.T)\n#dW = lr * np.dot(( I - A ),w)\n\n\n\nunmixed = np.dot(w, whiteX) + bias1\nlogit = 1 - (2 / (1 + np.exp(-unmixed)))\nw = w + lr * np.dot(I + np.dot(logit, unmixed.T), w)\nbias1 = bias1 + lr * logit.sum(axis=1).reshape(bias1.shape)\n\nif np.mod(k,5000)==0:\n    print(A)\n\nif np.max(np.abs(w))&gt;100:\n    print(\"reducing learningrate \\n\")\n    lr = lr*0.2\n    # reset w\n    w = np.eye(x.shape[0]) \n    continue\nw = w+ dW\n\n#u2 = w2*x;\n#w2 = w2+0.0001*(I+ (1- 2*f2(u2))*u2')*w2;"
  },
  {
    "objectID": "exercises/solutions/ex2_filter.html",
    "href": "exercises/solutions/ex2_filter.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport scipy as scp\nimport scipy.fft\nimport numpy as np\nfrom numpy import cos, sin, pi,  arange\n\n#from scipy.signal import kaiserord, lfilter, firwin, freqz, convolve,filtfilt\n\nsample_rate = 100.0\nnsamples = 400\nt = arange(nsamples) / sample_rate\nx = cos(2*pi*0.5*t) + 0.2*sin(2*pi*2.5*t+0.1) + \\\n        0.2*sin(2*pi*15.3*t) + 0.1*sin(2*pi*16.7*t + 0.1) + \\\n            0.1*sin(2*pi*23.45*t+.8)\nplt.plot(t,x,)\nT = 4\n#dF = 1/T = 0.25Hz\nfs = 100\n#nyquist-frequency = fs/2\n\nf = np.linspace(0,fs-1/T,fs*T)\nplt.plot(f,np.log(np.abs(scp.fft.fft(x))))\n#plt.plot(f[0:len(f)//2+1],(np.abs(scp.fft.rfft(x))))\nplt.plot(np.log(np.abs(np.fft.fft(x))))\nplt.plot(np.angle(scp.fft.fft(x)))\nang = np.angle(scp.fft.fft(x))\nmag = np.abs(scp.fft.fft(x))\nplt.plot((mag))\nmag[30:370] = 0.\nplt.plot((mag))\nplt.plot(t,x)\nplt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang)).real)"
  },
  {
    "objectID": "exercises/solutions/ex2_filter.html#highpass-instead-of-lowpass",
    "href": "exercises/solutions/ex2_filter.html#highpass-instead-of-lowpass",
    "title": "üß† EEG-2024",
    "section": "Highpass instead of lowpass",
    "text": "Highpass instead of lowpass\nRepeat the steps from above, but this time, remove the low frequency components\n\nang = np.angle(scp.fft.fft(x))\nmag = np.abs(scp.fft.fft(x))\n\n\nmag[1:30] = 0.1\nmag[370:400] = 0.1\n\n\n\nplt.plot(t,x)\nplt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang)).real)\n\n#ang_random = np.random.rand(ang.shape[0])*2*np.pi - np.pi\n#plt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang_random)).real)"
  },
  {
    "objectID": "exercises/solutions/ex2_filter.html#what-happens-to-the-frequency-and-time-response-if-we-add-artefacts",
    "href": "exercises/solutions/ex2_filter.html#what-happens-to-the-frequency-and-time-response-if-we-add-artefacts",
    "title": "üß† EEG-2024",
    "section": "What happens to the frequency and time response if we add ‚Äúartefacts‚Äù?",
    "text": "What happens to the frequency and time response if we add ‚Äúartefacts‚Äù?\nAdd a DC-offset (a step-function) starting from x[200:] and investigate the fourier space. Filter it again (low or high pass) and transfer it back to the time domain and investigate the signal around the spike.\n\nxtmp = x.copy()\nxtmp[200:250] = xtmp[200:250]+10\n\nplt.plot(xtmp)\n\n\n\n\n\n\n\n\n\n\nang = np.angle(scp.fft.fft(xtmp))\nmag = np.abs(scp.fft.fft(xtmp))\nplt.plot(np.log10(mag))\nmag[30:370] = 0.2\nplt.plot(np.log10(mag))\nplt.show()\nplt.plot(t,xtmp)\nplt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang)).real)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxtmp = x.copy()\nxtmp[200:] = xtmp[200:]+20\nplt.plot(xtmp)\n\nplt.figure()\nang = np.angle(scp.fft.fft(xtmp))\nmag = np.abs(scp.fft.fft(xtmp))\nplt.plot(np.log10(mag))\nmag[1:30] = 0.2\nmag[370:400] = 0.2\nplt.plot(np.log10(mag))\nplt.show()\n\nplt.plot(t,x)\nplt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang)).real)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Impulse Response Function To get a bit deeper understanding of what is going on, have a look at the fourier transform of a new impulse signal (e.g.¬†1:400 =&gt; 0. and 200 =&gt; 1.). What do you observe? Why would we see ringing if we put most of the coefficients to 0?\n\n# Impulse Response Function\nx[:] =0\nx[200] = 1\nplt.plot(x)\n\n\n\nang = np.angle(scp.fft.fft(x))\nmag = np.abs(scp.fft.fft(x))\nplt.plot(np.log10(mag))\nmag[310:370] = 0.2\nmag[30:90] = 0.2\nplt.plot(np.log10(mag))\nplt.show()\n#plt.plot(t,x)\nplt.plot(t,scp.fft.ifft(mag*np.exp(1j*ang)).real)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Filtering EEG data\n\nimport sys\nsys.path.insert(0,\"..\")\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nimport importlib\nimport ccs_eeg_utils\n\nbids_root = \"../local/bids\"\nbids_root = \"/bigpool/export/users/ehinger/erp-core/bids\" # Bene's Server location\nbids_root = \"/store/data/erp-core/\"\n\nsubject_id = '030'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\n\nReading /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_eeg.fdt\n\n\n/tmp/ipykernel_2213339/3616083917.py:18: RuntimeWarning: Data file name in EEG.data (sub-030_task-P3_eeg.fdt) is incorrect, the file name must have changed on disk, using the correct file name (sub-030_ses-P3_task-P3_eeg.fdt).\n  raw = read_raw_bids(bids_path)\n\n\nReading events from /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_events.tsv.\nThe event \"response\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\nThe event \"stimulus\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\nReading channel info from /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_channels.tsv.\nReading 0 ... 393215  =      0.000 ...   383.999 secs...\n\n\n\n    \n        Measurement date\n        \n        Unknown\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n            \n            sub-030\n            \n        \n    \n    \n        Digitized points\n        \n        33 points\n        \n    \n    \n        Good channels\n        30 EEG, 3 EOG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        HEOG_left, HEOG_right, VEOG_lower\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        1024.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        512.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        sub-030_ses-P3_task-P3_eeg.fdt\n    \n    \n    \n        Duration\n        00:06:24 (HH:MM:SS)\n    \n\n\n\nT: Choose the channel ‚ÄúPz‚Äù, plot the channel (same as previous HW)\n\n#%matplotlib qt\nraw.pick([\"Pz\"])#[\"Pz\",\"Fz\",\"Cz\"])\nplt.plot(raw[:,:][0].T)\n\n\n\n\n\n\n\n\nT: Plot the fourier space using raw.plot_psd\n\n%matplotlib inline\n#raw.plot_psd(area_mode='range', tmax=10.0, average=False,xscale=\"linear\",);\n\nraw.compute_psd().plot()\n\nEffective window size : 2.000 (s)\nNeed more than one channel to make topography for eeg. Disabling interactivity.\n\n\n/tmp/ipykernel_2213339/794039717.py:4: RuntimeWarning: Channel locations not available. Disabling spatial colors.\n  raw.compute_psd().plot()\n/home/ehinger/micromamba/envs/course_EEG_ws2023/lib/python3.10/site-packages/mne/viz/utils.py:161: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  (fig or plt).show(**kwargs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT: Now we filter using raw.filter(), specify a highpass of 0.5Hz and a lowpass of 50Hz. Plot the fourier spectrum again.\nT: Plot the channel again, did the filter work as indented?\n\nraw_f = raw.copy().filter(0.5,40, fir_design='firwin')\nraw_f.plot_psd(area_mode='range', tmax=10.0, average=False);\n\nFiltering raw data in 1 contiguous segment\nSetting up band-pass filter from 0.5 - 40 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 0.50\n- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n- Upper passband edge: 40.00 Hz\n- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n- Filter length: 6759 samples (6.601 s)\n\nNOTE: plot_psd() is a legacy function. New code should use .compute_psd().plot().\nEffective window size : 2.000 (s)\nNeed more than one channel to make topography for eeg. Disabling interactivity.\n\n\n/tmp/ipykernel_2213339/192806494.py:2: RuntimeWarning: Channel locations not available. Disabling spatial colors.\n  raw_f.plot_psd(area_mode='range', tmax=10.0, average=False);\n/home/ehinger/micromamba/envs/course_EEG_ws2023/lib/python3.10/site-packages/mne/viz/utils.py:161: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  (fig or plt).show(**kwargs)\n\n\n\n\n\n\n\n\n\n\nplt.plot(raw[:,0:1000][0].T)\nplt.plot(raw_f[:,0:1000][0].T)\n\n\n\n\n\n\n\n\nDue to a huge DC-offset, the signals cannot be compared. We have to remove e.g.¬†the median from each signal\n\nplt.plot(raw[:,0:1000][0].T-np.median(raw[:,0:1000][0].T))\nplt.plot(raw_f[:,0:1000][0].T-np.median(raw_f[:,0:1000][0].T))\n#plt.plot(raw_f[:,0:1000][0].T-np.mean(raw_f[:,0:1000][0].T))"
  },
  {
    "objectID": "exercises/solutions/ex9_tf.html",
    "href": "exercises/solutions/ex9_tf.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "import numpy as np\nimport scipy\nimport sys\nsys.path.insert(0,\"..\")\nimport ccs_eeg_utils\nfrom matplotlib import pyplot as plt\n%load_ext autoreload\n%autoreload 2\nsig = ccs_eeg_utils.simulate_TF(signal=3)\n\nplt.plot(sig)\nsig.shape\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb Cell 2' in &lt;cell line: 9&gt;()\n      &lt;a href='vscode-notebook-cell:/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb#ch0000001?line=6'&gt;7&lt;/a&gt; get_ipython().run_line_magic('load_ext', 'autoreload')\n      &lt;a href='vscode-notebook-cell:/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb#ch0000001?line=7'&gt;8&lt;/a&gt; get_ipython().run_line_magic('autoreload', '2')\n----&gt; &lt;a href='vscode-notebook-cell:/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb#ch0000001?line=8'&gt;9&lt;/a&gt; sig = ccs_eeg_utils.simulate_TF(signal=3)\n     &lt;a href='vscode-notebook-cell:/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb#ch0000001?line=10'&gt;11&lt;/a&gt; plt.plot(sig)\n     &lt;a href='vscode-notebook-cell:/store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/ex9_tf.ipynb#ch0000001?line=11'&gt;12&lt;/a&gt; sig.shape\n\nFile /store/users/ehinger/courses/course_eeg_SS2022/exercises/solutions/../ccs_eeg_utils.py:144, in simulate_TF(signal, noise)\n    142 def simulate_TF(signal=1,noise=True):\n    143     # signal can be 1 (image), 2(chirp) or 3 (steps)\n--&gt; 144     import imageio\n    146     if signal == 2:\n    147         im = imageio.imread('ex9_tf.png')\n\nModuleNotFoundError: No module named 'imageio'\n\n\n\n\nimport mne\nnseglist = np.arange(32,256,64)\n\nfor (k,n) in enumerate(nseglist):\n    \n    (f,t,x) = scipy.signal.stft(sig,fs=500,nperseg=n,noverlap=n*0.3)\n\n    plt.subplot(len(nseglist),1,k+1).imshow(np.abs(x),origin='lower',interpolation='none',aspect='auto',extent=[min(t),max(t),min(f),max(f)])\n    \n\n\n\n\n\n\n\n\n\nimport mne\n#freqs = np.arange(10,250,1)\nfreqs = np.logspace(*np.log10([10, 250]), num=25)\n\ncyclist= [3,4,8,16]\nfor (k,cyc) in enumerate(cyclist):\n    power = mne.time_frequency.tfr_array_morlet(sig.reshape(1,1,-1), sfreq=500,freqs=freqs, n_cycles=cyc)\n    plt.subplot(len(cyclist),1,k+1).imshow(np.abs(power[0,0,:,:]),aspect=\"auto\",origin='lower', extent=[min(t),max(t),min(freqs),max(freqs)])\n\n\n\n\n\n\n\n\n\nTF of EEG Dataset\nGet a partially preprocessed P3 Dataset from the ERP-Core:\nepochs = ccs_eeg_utils.get_TF_dataset(  subject_id = '002',bids_root = \"../local/bids\")\nWe are looking only at epochs relative to the responses.\n\nepochs = ccs_eeg_utils.get_TF_dataset(  subject_id = '002',bids_root = \"../local/bids\")\n\nWe want to get an overview of the power. So for starters we choose bad frequency resolution but good time resolution with a wavelet transform.\nWe will evaluate the TF at freqs = np.logspace(*np.log10([5, 80]), num=25) with 1 cycle.\nRun:\npower_total = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=4,average=True) # ITC, inter-trial-coherence is quite similar to evoked power, it is a measure of phase consistency over trials but we havent discussed it in the lecture.\n\nfreqs = np.logspace(*np.log10([5, 80]), num=25)\nn_cycles = 1 \npower_total = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=4,average=True)\n\nLoading data for 202 events and 3073 original time points ...\n0 bad epochs dropped\n\n\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    3.8s\n[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    7.2s finished\n\n\nNext we will visualize all TFs at all electrodes and get an overview whats going on. If you use %matplotlib qt the plot will be interactive. Choose a baseline of -.5 to 0 using the command power.plot_topo()\n\n%matplotlib qt\npower_total.plot_topo(baseline=[-.5,0],mode=\"logratio\",vmin=-2,vmax=2);\n\nApplying baseline correction (mode: logratio)\n\n\n\n\n\n\n\n\n\nT: Plot the power at electrode Cz using power.plot():\n1. Without any baseline\n2. With a baseline of your choice\nT: Explain the general pattern you see. Can you spot differences betweem with and without baseline?\n\npower_total.plot(baseline=None,vmin=-2*10e-9,vmax=2*10e-9,picks='Cz');\npower_total.plot(baseline=[-.5,0],mode='percent',vmin=-4,vmax=4,picks='Cz');\n\nNo baseline correction applied\nApplying baseline correction (mode: percent)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT: Now lets improve upon our frequency resolution nz, increasing the number of cycles to 3. Plot channel Cz with BSL correction (-0.5,1).\nTipp: you can speed up the calculation by specifying picks=\"Cz\"\n\n\nn_cycles = 5\npower_total = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=4,average=True,picks='Cz')\n\n\nLoading data for 202 events and 3073 original time points ...\n\n\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    2.9s finished\n\n\n\npower_total.plot_topo(baseline=[-.5,0],mode='percent',vmin=-4,vmax=4);\n\nApplying baseline correction (mode: percent)\n\n\n\n\n\n\n\n\n\nT: We also want to calculate the induced and evoked TF. For this we first calculate the induced spectrum, then subtract the total from the induced.\n\nepochs.subtract_evoked() is a function that removes the ERP from each trial. It is a mne-consistent function that practically does: epochs_induced._data = epochs._data  - epochs.average().data. Run the tfr analysis again on the induced dataset, remember that if you dont make a copy of your epochs (via epochs.copy()) the dataset will be overwritten in memory\nIn order to get epochs_evoked, we have to subtract total and induced. We cand do this via\n\npower_evoked = mne.combine_evoked([power_total,power_induced],weights=[1,-1])\nNote: you could use this function also to combine/subtract condition effects!\nT: Visualize evoked, total & induced for electrode Cz\n\n\nepochs_induced = epochs.copy()\n#epochs_induced._data = epochs_induced._data  - epochs_induced.average().data # but we are using the offocial way here\nepochs_induced.subtract_evoked()\n\npower_induced = mne.time_frequency.tfr_morlet(epochs_induced, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=1,average=True,picks=\"Cz\")\n\nSubtracting Evoked from Epochs\n    The following channels are not included in the subtraction: HEOG_left, VEOG_lower, HEOG_right\n[done]\nLoading data for 202 events and 3073 original time points ...\n\n\n\npower_induced\n\n&lt;AverageTFR | time : [-1.000000, 2.000000], freq : [5.000000, 80.000000], nave : 202, channels : 1, ~620 kB&gt;\n\n\n\n%matplotlib qt\npower_evoked = mne.combine_evoked([power_total,power_induced],weights=[1,-1])\n\nmode = \"percent\"\nbsl = [-0.5,0]\ncmin = -3\ncmax = -cmin\npower_total.plot(baseline=bsl,mode=mode,picks='Cz',vmin=cmin,vmax=cmax)\npower_induced.plot(baseline=bsl,mode=mode,picks='Cz',vmin=cmin,vmax=cmax)\npower_evoked.plot(baseline=bsl,mode=mode,picks='Cz',vmin=cmin,vmax=cmax);\n\n\nApplying baseline correction (mode: percent)\nApplying baseline correction (mode: percent)\nApplying baseline correction (mode: percent)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npower_evoked.plot(baseline=bsl,mode=mode,picks='Cz',vmin=-20,vmax=20);\n\nApplying baseline correction (mode: percent)\n\n\n\n\n\n\n\n\n\n\npower_total.plot(baseline=bsl,mode=mode,picks='Cz',vmin=-1,vmax=1)\n\nApplying baseline correction (mode: percent)"
  },
  {
    "objectID": "exercises/solutions/ex6_linearModels.html",
    "href": "exercises/solutions/ex6_linearModels.html",
    "title": "Statistical Analysis of N170 area using Linear Regression",
    "section": "",
    "text": "T Load the data into a pandas dataframe. A very useful concept here is the concept of a tidy dataframe. The idea is, that every observation is one row of a table, where columns are potential features / descriptors + the independent variable (the average activity at PO8 here).\nimport pandas as pd\nimport seaborn as sns\nimport numpy as numpy\nd = pd.read_csv(\"../ex6_N170.csv\",delimiter=\",\")\nd\n\n\n\n\n\n\n\n\nepoch\nstim\ncond\nPO8\nbsl\n\n\n\n\n0\n1\ncar\nscrambled\n8.835035\n2.375931\n\n\n1\n4\ncar\nintact\n-9.949910\n-0.513365\n\n\n2\n6\ncar\nscrambled\n2.609454\n-4.410792\n\n\n3\n7\nface\nscrambled\n-4.120405\n-8.484117\n\n\n4\n9\ncar\nintact\n-1.127925\n3.305076\n\n\n...\n...\n...\n...\n...\n...\n\n\n240\n316\nface\nintact\n-2.478639\n3.748255\n\n\n241\n317\ncar\nintact\n-10.040087\n-1.260044\n\n\n242\n318\ncar\nscrambled\n-1.897411\n-4.049000\n\n\n243\n319\ncar\nintact\n-3.982105\n7.121623\n\n\n244\n320\ncar\nscrambled\n-4.024754\n-0.197297\n\n\n\n\n245 rows √ó 5 columns\nT: Use a plottinglibrary of your choice to visualise the simple scatter plot between some/all variables. I recommend packages seaborn e.g.¬†pairplot) or plotnine for this. They make it easy to split up plots of continuous variables (e.g.¬†baseline vs.¬†PO8) by a categorical variable, e.g.¬†cond (scrambled/intact).\nQ: Can you already guess what the relationships between the variables are, solely based on the plots?\nsns.pairplot(d,hue=\"stimcond\",)\nd[\"stimcond\"] = d.stim + d.cond"
  },
  {
    "objectID": "exercises/solutions/ex6_linearModels.html#changing-bases",
    "href": "exercises/solutions/ex6_linearModels.html#changing-bases",
    "title": "Statistical Analysis of N170 area using Linear Regression",
    "section": "Changing Bases",
    "text": "Changing Bases\nWe discussed in the lecture that it is possible to change the bases. We will go back to the simple example of of Intercept + cond. But in addition of dummy coding (intact == 1, scrambled == 0), we will fit a second model with effect-coding (intact == 0.5, scrambled = -0.5).\nQ: How do the betas compare of the two models? How does the interpretation change?\nHint If you need more you can read two of my tutorial-blogposts on this topic here and here :)\n\ncond\n\narray([0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n       1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n       0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n       1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n       0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n       1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n       1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n       0., 1., 1., 1., 0., 1., 0.])\n\n\n\ncond-0.5\n\n\n(np.mean(d.PO8[(d.cond==\"scrambled\")]) + np.mean(d.PO8[(d.cond==\"intact\")]))/2\n\n-3.8524028378068995\n\n\n\nnp.mean(d.PO8)\n\n-3.911091509146645\n\n\n\nprint(solve(const,cond))\nprint(solve(const,cond-0.5))\n\n[-0.97665794 -5.75148979]\n[-3.85240284 -5.75148979]\n\n\nT: Now we run the full 2x2 model once with dummy and once with effect-coding. The interaction of an effect-coded model is still just the designmatrix columns multiplied with eachother\nQ: Can you put the results together, why do they have the results they have?\n\nprint(solve(const,cond-0.5,stim-0.5,(cond-0.5)*(stim-0.5)))\nprint(solve(const,cond,stim,cond*stim)) # dummy cding / treatment coding\n\n[-3.73857326 -5.61334487 -1.40707547 -5.4995781 ]\n[-1.60325761 -2.86355582  1.34271358 -5.4995781 ]\n\n\n\nContinuous Regressors\nNow it is time to involve our continuous regressor. In this case it is the baseline-value. We are following here the relatively new approach of Alday 2019 and instead of subtracting a baseline, we will regress it out. We did not talk about baseline correction in the lecture, this is something I will talk about at the end of the course.\nIn theory baseline corrections are not needed, the baseline (i.e.¬†what happens before stimulus onset, thus ‚Äúnegative‚Äù time in an ERP) should be flat / noise around 0, because stimulus order is random. But in practice, we only have limited number of trials and limited randomization. Thus it might be, that we have a bias with in one condition more residual drift / low-frequency activity than in the other. This will ‚Äúmove‚Äù the whole ERP curve up/down and bias results later in the epoch.\nClassically, baselines are simply subtracted. Thus every point of an ERP recieves the ‚Äúsame‚Äù baseline correction. This is equivalent to adding a known parameter to our model: y ~ b0 * constant + b1 * cond + stim +1*BSL. What we will do instead is the 2020-version, we regress the baseline. This allows us to remove less of the baseline activity (or more, but rarely happens).\nT: Plot the PO8 actiity against the baseline (you might have done this plot at the beginning of the exercise). Split it up by cond & stim\n\nsns.scatterplot(x=d.bsl,y=d.PO8,hue=d.cond)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x=d.bsl,y=d.PO8,hue=d.stim)\n\n\n\n\n\n\n\n\nT: Add the baseline as a predictor to your 2x2 model.\nQ: What is the resulting beta?\n\nprint(solve(const,cond,stim,cond*stim,d.bsl))\nprint(solve(const,cond,stim,cond*stim))\n\n[-1.23950777 -3.80187398  1.27907238 -5.53859036  0.32611814]\n[-1.60325761 -2.86355582  1.34271358 -5.4995781 ]\n\n\nTypically we do not only remove the overall baseline, but condition-indivudal baselines.\nT: Thus we have to generate interactions with all predictors & interaction of the interactions too\nQ: What is your interpretation of the resulting betas? For which conditions do we really need a baseline correction and how strong should it be?\n\nprint(solve(const,cond,stim,cond*stim,d.bsl))\nprint(solve(const,cond,stim,cond*stim,d.bsl,d.bsl*cond,d.bsl*stim,d.bsl*cond*stim))\n\n[-1.23950777 -3.80187398  1.27907238 -5.53859036  0.32611814]\n[-1.59620965 -4.05240262  1.58967869 -5.42777959  0.00631882  0.66445615\n  0.26970879 -0.5248866 ]\n\n\n\\(y - BSL = \\beta_0 \\dots\\)\n\\(y = \\beta_0 \\dots + BSL\\)\n\\(y = \\beta_0 \\dots + \\gamma_0 * BSL\\)\n\nest = solve(const,cond,stim,cond*stim,d.bsl,d.bsl*cond,d.bsl*stim,d.bsl*cond*stim)\n#est = solve(const,cond,stim,cond*stim,d.bsl)\n\nres = pd.DataFrame({\"stim\":[\"car\",\"car\",\"face\",\"face\"],\n              \"cond\":[\"scrambled\",\"intact\",\"scrambled\",\"intact\"],\n              \"est\": [\n                    1*est[0]+0*est[1] + 0*est[2] + 0*0*est[3],\n                    1*est[0]+1*est[1] + 0*est[2] + 1*0*est[3],\n                    1*est[0]+0*est[1] + 1*est[2] + 0*1*est[3],\n                    1*est[0]+1*est[1] + 1*est[2] + 1*1*est[3]]})\nsns.lineplot(x=res.cond,y=res.est,hue=res.stim)\n\nest = solve(const,cond,stim,cond*stim)\nres = pd.DataFrame({\"stim\":[\"car\",\"car\",\"face\",\"face\"],\n              \"cond\":[\"scrambled\",\"intact\",\"scrambled\",\"intact\"],\n              \"est\": [\n                    1*est[0]+0*est[1] + 0*est[2] + 0*0*est[3],\n                    1*est[0]+1*est[1] + 0*est[2] + 1*0*est[3],\n                    1*est[0]+0*est[1] + 1*est[2] + 0*1*est[3],\n                    1*est[0]+1*est[1] + 1*est[2] + 1*1*est[3]]})\nsns.lineplot(x=res.cond,y=res.est,hue=res.stim)"
  },
  {
    "objectID": "exercises/solutions/ex6_linearModels.html#bonus-uncertainty-standard-errors",
    "href": "exercises/solutions/ex6_linearModels.html#bonus-uncertainty-standard-errors",
    "title": "Statistical Analysis of N170 area using Linear Regression",
    "section": "Bonus: Uncertainty & Standard Errors",
    "text": "Bonus: Uncertainty & Standard Errors\nThis was not part of the lecture, but might be interesting nonetheless. In this exercise we talk about how we can generate errorbars to add to our 2x2 plot.\nWe can not only estimate the \\(\\hat{\\beta}\\), our estimated parameters, but also the variance of them, giving us a handle of uncertainty. We just need some additional assumptions (normal residuals). For a derivation see this.\nIt turns out, that \\(Var(\\beta) = \\sigma^2 (X^TX)^{-1}\\)\nT: Implement this formula. The sqrt of the diagonal elements are your Standard-Errors, which we can use as error-bars. The \\(\\sigma^2\\) is the variance of the residuals\n\\(Y_i = X\\beta + e_i\\)\n\\(Y_i - X\\beta = e_i\\)\n\nimport numpy as  np\n\ndef se(*args):\n    X = np.stack(args).T\n    b = np.linalg.inv(X.T @ X)@X.T@d.PO8\n    e = d.PO8-X@b\n    s = np.var(e)\n    return np.sqrt(np.diag(s * np.linalg.inv(X.T @ X)))\n\nse = (se(const,cond,stim,cond*stim))    \nes = (solve(const,cond,stim,cond*stim))    \nprint(es)\nprint(se)\n\nprint(es/se)\n\n[-1.60325761 -2.86355582  1.34271358 -5.4995781 ]\n[0.73685984 1.07359417 1.07865236 1.51134258]\n[-2.17579724 -2.66726095  1.2448066  -3.63886929]\n\n\nNow we have to apply it including our contrast vectors e.g.¬†c = [1 0 0 0*0], or c=[1 0 1 1*0]\n\\[se_c = \\sigma^2 c^T (X^TX)^{-1} c\\]\n\ndef se_contrast(*args,c=[1,0,0,0]):\n    c = np.array(c)\n    X = np.stack(args).T\n    b = np.linalg.inv(X.T @ X)@X.T@d.PO8\n    e = d.PO8-X@b\n    s = np.var(e)\n    #print(np.sqrt(c.T @ np.linalg.inv(X.T @ X) @c))\n    return np.sqrt(s * c.T @ np.linalg.inv(X.T @ X) @c)\n\nse_A = se_contrast(const,cond,stim,cond*stim,c = [1,1,0,0])\nse_A\n\n0.7807957629392348\n\n\n\nres\n\n\n\n\n\n\n\n\nstim\ncond\nest\n\n\n\n\n0\ncar\nscrambled\n-1.603258\n\n\n1\ncar\nintact\n-4.466813\n\n\n2\nface\nscrambled\n-0.260544\n\n\n3\nface\nintact\n-8.623678\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nest = solve(const,cond,stim,cond*stim)\nres = pd.DataFrame({\"stim\":[\"car\",\"car\",\"face\",\"face\"],\n              \"cond\":[\"scrambled\",\"intact\",\"scrambled\",\"intact\"],\n              \"est\": [\n                    est@np.array([1,0 ,0,0*0]),\n                    est@np.array([1,1 ,0,1*0]),\n                    est@np.array([1,0 ,1,0*1]),\n                    est@np.array([1,1 ,1,1*1])],\n               \"se\": [se_contrast(const,cond,stim,cond*stim,c = [1,0,0,0]),\n                     se_contrast(const,cond,stim,cond*stim,c = [1,1,0,0]),\n                     se_contrast(const,cond,stim,cond*stim,c = [1,0,1,0]),\n                     se_contrast(const,cond,stim,cond*stim,c = [1,1,1,1])]})\nsns.lineplot(x=res.cond,y=res.est,hue=res.stim)\nplt.errorbar(x=res.cond,y=res.est,yerr=res.se,fmt=\"none\",c = 'k')\n\n\n\n\n\n\n\n\n\n\"y~1+cond+stim+cond:stim+bsl+bsl:cond+bsl:stim+bsl:cond:stim\"\nfit(LinearModel,\"y~1+cond*stim*bsl\",d)"
  },
  {
    "objectID": "exercises/solutions/ex8_clusterPerm.html",
    "href": "exercises/solutions/ex8_clusterPerm.html",
    "title": "Multiple Comparison Corrections",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport sys\nsys.path.insert(0,\"..\")\nimport ccs_eeg_utils\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport mne.stats\nimport scipy.stats\nUse data = ccs_eeg_utils.ex8_simulateData() to simulate a simple difference between two condition. Instead of electrodes x time, we now have a simple rectangular matrix (or \\(n_{subject}=15\\) of them), but the principles of multiple comparison corrections, can be applied to it as well.\nT: calculate the mean of the data over subjects and plot it as an image\ndata = ccs_eeg_utils.ex8_simulateData()\nplt.imshow(data.mean(axis=0))\nplt.colorbar()\nT: Plot some subjects individually. Can you infer anything from the ‚Äúsingle subject‚Äù displays?\nax = plt.subplots(3,5)\nfor k in range(15):\n    plt.subplot(3,5,k+1).imshow(data[k,:,:])"
  },
  {
    "objectID": "exercises/solutions/ex8_clusterPerm.html#false-discovery-rate",
    "href": "exercises/solutions/ex8_clusterPerm.html#false-discovery-rate",
    "title": "Multiple Comparison Corrections",
    "section": "False Discovery Rate",
    "text": "False Discovery Rate\nWe will investigate False Discovery Rate a bit. First let‚Äôs generate data without any effect and only with noise. Thus all possible findings with p&lt;0.05 have to be false positives\nT: run data_h0 = ccs_eeg_utils.ex8_simulateData(signal_mean=0,noise_between=0) and plot the t-values as an imshow (be sure to add a colorbar always)\nT: plot a histogram of all p-values (regardless of position)\nT: Also plot a histogram of the p-values of the data with the effect\n\ndata_h0 = ccs_eeg_utils.ex8_simulateData(signal_mean=0,noise_between=0,rng_seed=43)\nttest_h0 = scipy.stats.ttest_1samp(data_h0,popmean=0,axis=0)\nplt.imshow(ttest_h0.statistic,clim=[-3,3])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nplt.subplot(1,2,1).hist(ttest_h0.pvalue.reshape(np.prod(data_h0.shape[1:])),50),plt.ylim([0,200]);\nplt.subplot(1,2,2).hist(tstat.pvalue.reshape(np.prod(data_h0.shape[1:])),50),plt.ylim([0,200]);\n\n\n\n\n\n\n\n\n\n# and the t-values for fun\nplt.subplot(1,2,1).hist(ttest_h0.statistic.reshape(np.prod(data_h0.shape[1:])),50),plt.ylim([0,200]);\nplt.subplot(1,2,2).hist(tstat.statistic.reshape(np.prod(data_h0.shape[1:])),50),plt.ylim([0,200]);\n\n\n\n\n\n\n\n\nT: Count how many pvalues are below 0.05 each from the data with and without effect. For FDR we have to estimate how many ‚Äúsignificant‚Äù (=&gt; pvalue&lt;\\(\\alpha\\), with \\(alpha=0.05\\) typically) values we would get by chance (=false positives). Instead of estimating the number of p-values from one dataset (which is much more involved), we can also take our null-model pvalue-count as well. Calculate the ratio of H0/H1 significant-pvalues. This is your False-Discovery rate. Can you manually adjust alpha, so that the FDR is 0.05?\n\nalpha = 0.007\nnofdr  = np.sum(tstat.pvalue&lt;alpha)\nnofdr_h0 = np.sum(ttest_h0.pvalue&lt;alpha)\nyesfdr = np.sum(mne.stats.fdr_correction(tstat.pvalue)[1]&lt;0.05)\nprint(\"before fdr {}, h0 {:.3f}, ratio for alpha {}: {:.2f} FalsePositives, after fdr-mne: {}, fdr-manual: {}\".format(nofdr,nofdr_h0,alpha,nofdr_h0/nofdr,yesfdr,np.sum(tstat.pvalue&lt;0.007)))\n\nbefore fdr 131, h0 6.000, ratio for alpha 0.007: 0.05 FalsePositives, after fdr-mne: 88, fdr-manual: 131\n\n\nT: Use mne.stats.fdr_correction to calculate the proper fdr-correction. Use the plotting function from the beginning to directly compare the p-values with and without correction.\n\nfdr= (tstat.statistic,mne.stats.fdr_correction(tstat.pvalue)[1])\nplot_stats([tstat,fdr])"
  },
  {
    "objectID": "exercises/solutions/ex8_clusterPerm.html#bonus-f-max-permutation-test",
    "href": "exercises/solutions/ex8_clusterPerm.html#bonus-f-max-permutation-test",
    "title": "Multiple Comparison Corrections",
    "section": "Bonus: F-Max Permutation test",
    "text": "Bonus: F-Max Permutation test\nIn the lecture we discussed permutation tests and permutation cluster tests. What we didnt discuss is that you can easily adjust a permutation test to correct for multiple comparisons. We permute each 40x40 grid element concurrently, but instead of saving for each grid element the permuted statistics (e.g.¬†the t-value), we save the maximum of all grid elements. This biases our permutation distribution towards large t-values, and concurrently makes it harder for the observed value to ‚Äústand out‚Äù (=&gt; be unlikely) from that distribution. You can use mne.stats.permutation_t_test to calculate this.\n\ndata_flat = data.copy()\ndata_flat.shape = (data.shape[0], data.shape[1]*data.shape[2])\nperm= mne.stats.permutation_t_test(data_flat, verbose=False)\nperm[0].shape = data.shape[1:]\nperm[1].shape = data.shape[1:]\n\nplot_stats([tstat,fdr,perm])"
  },
  {
    "objectID": "exercises/solutions/ex8_clusterPerm.html#cluster-permutation-tests",
    "href": "exercises/solutions/ex8_clusterPerm.html#cluster-permutation-tests",
    "title": "Multiple Comparison Corrections",
    "section": "Cluster Permutation Tests",
    "text": "Cluster Permutation Tests\nWe will implement a simple cluster permutation test, before making use of the mne-implementation. For this we need the package scikit-image to be able to use skimage.measure.label to get the clusters.\nA cluster permutation test has the following structure\n\ncalculate t_obs, the t-values for your observed data (as before mne.stats.ttest_1samp_no_p)\nThreshold t_obs using scipy.stats.t.ppf(1-(2*alpha), n-1) as the threshold value. This converts a p-value back to the t-value. In principle you could also decide to use a t-theshold of e.g.¬†2. The threshold is arbitrarily set, but important.\nBecause our cluster are in image-space, neighbours can easily be calculated using skimage.measure.label\nFind the largest cluster and save it to c_obs\nPermutation, do 1000 times:\n\ngenerate a signFlip vector with length n (by default n=15 subjects) consisting of random ‚Äú1‚Äù and ‚Äú-1‚Äù, one for each subject. Assuming the \\(H_0\\) is true (which we do in this loop), the sign around ‚Äú0‚Äù is random for each subject, so no harm should be done in flipping it (it will change the resulting statistic obviously, but doing it 1000 time shoudnt introuce / hide an effect)\nMultiply the signFlip vector to the data\nrepeat step 1-4 of the observed data for the permuted data to get c_perm\nSave this largest clustersize\n\nAppend c_obs to your c_perm (the simplified reason is, that else you could get a p-value of 0 more details if of interest)\n1 - np.mean(c_obs&gt;=c_perms) gives you your p-value\n\nNote: The test could be improved by e.g.¬†summing the t-values of a cluster instead of merely counting the cluster-extend, but that leads us a bit astray from what we want to understand here.\n\nt_sum.shape\n\n(59,)\n\n\n\nlabel_img = label(np.abs(t_star)&gt;thr)\nplt.imshow(label_img)\ni,c = np.unique(label_img,return_counts=True)\nt_sum = np.zeros(len(i))\nfor j in i:\n   t_sum[j] = np.abs(np.sum(t_star[label_img == j]))\n#t_sum\nnp.argmax(t_sum[1:])\n\n   \n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n# calculate statistic\nt_obs = mne.stats.ttest_1samp_no_p(data)\n# calculate max clustersize\nc_obs = calc_max_cluster(t_obs)\n# permute 100 times\nn_perm = 1000\nc_perms = np.zeros(shape=n_perm)\nc_perms[len(c_perms)-1] = c_obs\n#for p in range(n_perm-1):\np = 1\nsignFlip = (np.random.rand(15)&gt;0.5)*2-1\nd_perm = np.einsum('t,tnh-&gt; tnh',signFlip,data)\nd_perm.shape\nthr = scipy.stats.t.ppf(1-(2*alpha), n-1)\n\nt_star = mne.stats.ttest_1samp_no_p(d_perm)\nplt.subplot(2,1,1).imshow(np.abs(t_star)&gt;thr)\nt_star = mne.stats.ttest_1samp_no_p(data)\nplt.subplot(2,1,2).imshow(np.abs(t_star)&gt;thr)\ni,c = np.unique(label(np.abs(t_star)&gt;thr),return_counts=True)\n\n\n\n\n\n\n\n\n\nfrom skimage.measure import label\n\ndef calc_max_cluster(t_star,alpha=0.05,n = 15,method=\"extend\"):\n    thr = scipy.stats.t.ppf(1-(2*alpha), n-1)\n    label_img = label(np.abs(t_star)&gt;thr)\n    i,c = np.unique(label_img,return_counts=True)\n    # remove the first because it is \"0\" =&gt; no effect\n    if method == \"extend\":\n        mx = np.max(c[1:])\n    elif method == \"tsum\":\n        t_sum = np.zeros(len(i))\n        for j in i:\n            t_sum[j] = np.abs(np.sum(t_star[label_img == j]))\n\n        mx = np.max(t_sum[1:])        \n        \n    return mx\n# calculate statistic\nt_obs = mne.stats.ttest_1samp_no_p(data)\n# calculate max clustersize\nc_obs = calc_max_cluster(t_obs,method=\"tsum\")\n# permute 100 times\nn_perm = 1000\nc_perms = np.zeros(shape=n_perm)\nc_perms[len(c_perms)-1] = c_obs\nfor p in range(n_perm-1):\n    if np.mod(p,100)==0:\n        print(p)\n    signFlip = (np.random.rand(15)&gt;0.5)*2-1\n    # python doesnt support broadcasting of vectors to 3d matrice. I.e. this should do\n    # signFlip * data, i.e. multiply/flip each subject by a random +1 / -1\n    d_perm = np.einsum('t,tnh-&gt; tnh',signFlip,data)\n    t_perm = mne.stats.ttest_1samp_no_p(d_perm)\n    c_perms[p] = calc_max_cluster(t_perm,method=\"tsum\")\n    \n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n\n\n\nplt.hist(c_perms,100);\n\n\n\n\n\n\n\n\n\nc_obs\n\n531.4821842222836\n\n\n\n\np_val = 1-np.mean(c_obs&gt;=c_perms)\np_val\n\n0.016000000000000014\n\n\nT: Running a simular permutation test in MNE is much easier:\nt_clust, clusters, p_values, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, threshold=threshold, adjacency=None,\n    n_permutations=1000, out_type='mask')\n\nThe threshold is the same threshold you used before\nTypically we would have to supply the adjacency manually, because the adjancy depends on which channels are next to eachother. But in this case we can put None and mne will assume it is a grid-structure\n\nIn order to fill the clusters with their respective p-values:\np_clust = np.ones(data.shape[1:])\nfor cl, p in zip(clusters, p_values):\n    p_clust[cl] = p\nThis step is controversial, because clusters do not have any real p-value see here, ‚Äúinterpretation of significant TFCE value‚Äù. But pragmatically, I think it is still useful to gauge the Signal-To-Noise ratio of the clusters. As long as you do not literally interpret the p-value as a probability, you should be fine.\nT: Add the cluster-permutation to your comparison plot\n\n mne.stats.permutation_cluster_1samp_test\n\n\nthreshold = scipy.stats.distributions.t.ppf(1 - 0.15, 15 - 1)\n\nt_clust, clusters, p_values, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, threshold=threshold, adjacency=None,\n    n_permutations=1000, out_type='mask')\n\n\nstat_fun(H1): min=-8.247474 max=5.110684\nRunning initial clustering\nFound 8 clusters\nPermuting 999 times...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  : 999/999 [00:06&lt;00:00,  162.55it/s]\nComputing cluster p-values\nDone.\n\n\n\n\np_clust = np.ones(data.shape[1:])\nfor cl, p in zip(clusters, p_values):\n    p_clust[cl] = p\n\ncluster = (t_clust,p_clust)\nplot_stats([tstat,fdr,perm,cluster])\n\n\n\n\n\n\n\n\n\n\np_clust = np.ones(data.shape[1:])\nfor cl, p in zip(clusters, p_values):\n    p_clust[cl] = p\n\ncluster = (t_clust,p_clust)\nplot_stats([tstat,fdr,perm,cluster])"
  },
  {
    "objectID": "exercises/solutions/ex8_clusterPerm.html#tfce",
    "href": "exercises/solutions/ex8_clusterPerm.html#tfce",
    "title": "Multiple Comparison Corrections",
    "section": "TFCE",
    "text": "TFCE\nlast but not least, we will get rid of this initial cluster-formung threshold. TFCE integrates over all possible thresholds. We will not implement TFCE here, but simply call the mne-python function.\nt_tfce, _, p_tfce, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, adjacency=None,threshold = dict(start=0, step=0.2),\n    n_permutations=1000, out_type='mask')\nT: Add this to your comparison plot. We are done! I hope you learned the differences and underlying algorithms of several multiple-comparison corrects!\n\nt_tfce, _, p_tfce, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, adjacency=None,threshold = dict(start=0, step=0.2),\n    n_permutations=1000, out_type='mask')\np_tfce.shape = t_tfce.shape\ntfce = (t_tfce,p_tfce)\nplot_stats([tstat,fdr,perm,cluster,tfce])\n\nstat_fun(H1): min=-8.247474 max=5.110684\nRunning initial clustering\nUsing 42 thresholds from 0.00 to 8.20 for TFCE computation (h_power=2.00, e_power=0.50)\nFound 1600 clusters\nPermuting 999 times...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  : 999/999 [00:52&lt;00:00,   18.86it/s]\nComputing cluster p-values\nDone."
  },
  {
    "objectID": "exercises/ex4_cleaning.html",
    "href": "exercises/ex4_cleaning.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 4]",
    "section": "",
    "text": "We will re-use the dataset we used in the first exercise.\nTask: Go through the dataset using the MNE explorer and clean it. You can use raw.plot() for this.\n\n\n\n\n\n\nTip\n\n\n\nIf you are working from a jupyter notebook, try to use %matplotlib qt for better support of the cleaning window. To get an understanding how the tool works, press help or type ? in the window. (Hint: You first have to add a new annotation by pressing a)\n\n\n\n\n\nTask: While going through the dataset, mark what you observe as bad electrodes. Those are saved in raw.info['bads'].\nThe channels can later be interpolated with raw.interpolate_bads() or epoch.interpolate_bads(). Compare the channel + neighbours before and after. Did the interpolation succeed?\nYou need channel locations to run the interpolation which you can get by using the default-standardized channel locations raw.set_montage('standard_1020',match_case=False)\n\n\n\n\n\n\nTip\n\n\n\nIf you are interested in the mathematical details of spline interpolation, checkout this link\n\n\nTask: Save the annotations to a file. Best practice is to use a csv file of sorts e.g.¬†using raw.annotations.save(filename). Try to subsetting for BAD_ first. Bonus: Save it in a BIDS derivate folder according to the BIDS guidelines.\n\n\n\n\n\n\nTip\n\n\n\nIn the semester project you will use mne-bids-pipeline, which will automatically save many results - you might nevertheless come to a point where saving your own logs might be a good idea\n\n\n\n\n\nIn MNE, if annotations with BAD_ exist, the epoching function automatically removes them. We are now ready to compare ERP results with and without removal of bad segments. Epoch the data, the respective bad segments will be removed automatically.\nTask: Compare the two ERPs for the channel Cz\n\n\n\nIn the epoching step, we can also specify rejection criterion for a peak-to-peak rejection method\nreject_criteria = dict(eeg=100e-6,       # 100 ¬µV\n                       eog=200e-6)       # 200 ¬µV\nepochs = mne.Epochs(raw, events, reject=reject_criteria,reject_by_annotation=False)\nTask: Compare these epochs with your manual rejection and with the ERPs without rejection. Plot a single channel Cz overlaying all three ‚Äúsolutions‚Äù.\nBonus: We are going to add to our comparison by using autoreject in MNE python. Have a look at https://autoreject.github.io/ for installation and usage examples. Hint: You need channel locations to run autoreject which you can get by using the default-standardized channel locations raw.set_montage('standard_1020',match_case=False)\nBonus: We can also try to circumvent cleaning by using trimmed / winsorized means. For this, the epochs.average(method=X) function can be exchanged with a averaging callback. Scipy has comparable functions that you can try out and compare."
  },
  {
    "objectID": "exercises/ex4_cleaning.html#cleaning-data",
    "href": "exercises/ex4_cleaning.html#cleaning-data",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 4]",
    "section": "",
    "text": "We will re-use the dataset we used in the first exercise.\nTask: Go through the dataset using the MNE explorer and clean it. You can use raw.plot() for this.\n\n\n\n\n\n\nTip\n\n\n\nIf you are working from a jupyter notebook, try to use %matplotlib qt for better support of the cleaning window. To get an understanding how the tool works, press help or type ? in the window. (Hint: You first have to add a new annotation by pressing a)"
  },
  {
    "objectID": "exercises/ex4_cleaning.html#marking-electrodes",
    "href": "exercises/ex4_cleaning.html#marking-electrodes",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 4]",
    "section": "",
    "text": "Task: While going through the dataset, mark what you observe as bad electrodes. Those are saved in raw.info['bads'].\nThe channels can later be interpolated with raw.interpolate_bads() or epoch.interpolate_bads(). Compare the channel + neighbours before and after. Did the interpolation succeed?\nYou need channel locations to run the interpolation which you can get by using the default-standardized channel locations raw.set_montage('standard_1020',match_case=False)\n\n\n\n\n\n\nTip\n\n\n\nIf you are interested in the mathematical details of spline interpolation, checkout this link\n\n\nTask: Save the annotations to a file. Best practice is to use a csv file of sorts e.g.¬†using raw.annotations.save(filename). Try to subsetting for BAD_ first. Bonus: Save it in a BIDS derivate folder according to the BIDS guidelines.\n\n\n\n\n\n\nTip\n\n\n\nIn the semester project you will use mne-bids-pipeline, which will automatically save many results - you might nevertheless come to a point where saving your own logs might be a good idea"
  },
  {
    "objectID": "exercises/ex4_cleaning.html#remove-bad-trials",
    "href": "exercises/ex4_cleaning.html#remove-bad-trials",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 4]",
    "section": "",
    "text": "In MNE, if annotations with BAD_ exist, the epoching function automatically removes them. We are now ready to compare ERP results with and without removal of bad segments. Epoch the data, the respective bad segments will be removed automatically.\nTask: Compare the two ERPs for the channel Cz"
  },
  {
    "objectID": "exercises/ex4_cleaning.html#automatic-rejection",
    "href": "exercises/ex4_cleaning.html#automatic-rejection",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 4]",
    "section": "",
    "text": "In the epoching step, we can also specify rejection criterion for a peak-to-peak rejection method\nreject_criteria = dict(eeg=100e-6,       # 100 ¬µV\n                       eog=200e-6)       # 200 ¬µV\nepochs = mne.Epochs(raw, events, reject=reject_criteria,reject_by_annotation=False)\nTask: Compare these epochs with your manual rejection and with the ERPs without rejection. Plot a single channel Cz overlaying all three ‚Äúsolutions‚Äù.\nBonus: We are going to add to our comparison by using autoreject in MNE python. Have a look at https://autoreject.github.io/ for installation and usage examples. Hint: You need channel locations to run autoreject which you can get by using the default-standardized channel locations raw.set_montage('standard_1020',match_case=False)\nBonus: We can also try to circumvent cleaning by using trimmed / winsorized means. For this, the epochs.average(method=X) function can be exchanged with a averaging callback. Scipy has comparable functions that you can try out and compare."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Signal processing and Analysis of human brain potentials (EEG)",
    "section": "",
    "text": "Overview\n\n\nüìÜ Schedule\nüî¨ Exercises\nüõ†Ô∏è Project information\nüìã Grading\n\n\n\nAuthors\nThis course was put together by Benedikt Ehinger with feedback from previous cohorts.\n\n\nLicenses\nWe are currently working to release all material under CC-by license (where possible) - but it is a long process. Thus, the materials here are not open-education materials yet."
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "You are here because you want to learn something. My job is to make that easier and provide opportunities. I will provide lectures whenever they are ready. Homeworks and solutions are already available at the start of the course. In principle, you can decide to not join most of the weekly ‚ÄúDiscussions‚Äù and teach yourself - but the discussions will be fun and helpful as you will see. Attending the milestones is mandatory though.\nAt the end of the course, you should be able to analyse a complete EEG dataset in different ways. This requires conceptual knowledge and implementation practice. Importantly, this course requires you to actually want to learn! I will give you lots of flexibility, but in the end, you must make the time and effort to learn the content.\nI will support, but I will not enforce.",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "course.html#course-philosophy",
    "href": "course.html#course-philosophy",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "You are here because you want to learn something. My job is to make that easier and provide opportunities. I will provide lectures whenever they are ready. Homeworks and solutions are already available at the start of the course. In principle, you can decide to not join most of the weekly ‚ÄúDiscussions‚Äù and teach yourself - but the discussions will be fun and helpful as you will see. Attending the milestones is mandatory though.\nAt the end of the course, you should be able to analyse a complete EEG dataset in different ways. This requires conceptual knowledge and implementation practice. Importantly, this course requires you to actually want to learn! I will give you lots of flexibility, but in the end, you must make the time and effort to learn the content.\nI will support, but I will not enforce.",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "course.html#formal-requirements-to-pass",
    "href": "course.html#formal-requirements-to-pass",
    "title": "üß† EEG-2024",
    "section": "Formal requirements to pass",
    "text": "Formal requirements to pass\nThe formal requirements to pass the course, is to get a grade &gt;=4.0 in the semester project and to present at each milestone (ungraded). There is no requirement to join the other seminar sessions, to do the homeworks or watch the lectures. All of these things will be extremely helpful though, and I recommend to watch them early in the semester (you have to watch them anyway!)",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "course.html#exercises",
    "href": "course.html#exercises",
    "title": "üß† EEG-2024",
    "section": "Exercises",
    "text": "Exercises\nThe homeworks are voluntarily, but HIGHLY recommended. The semester project is based on the exercises. If you went through them and understood the content, the semester project will be easier to tackle.\nA note to solutions: I provide solutions so that you can check yourself in case you get stuck. This is dangerous from a learning perspective because easy access to solutions also means you might not challenge yourself enough. I highly recommend trying out various ways on a problem first, and only to go for the solution if you do not succeed after 10-20min .",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "course.html#feedback",
    "href": "course.html#feedback",
    "title": "üß† EEG-2024",
    "section": "Feedback",
    "text": "Feedback\nIn order for you to learn, you need feedback. One possibility is the forum where you can also help each other (which I highly recommend!), another one is the ‚ÄúDiscussion‚Äù sessions, and lastly you can ask for my feedback for the exercises. The more specific your questions are, the better feedback I can provide! I aim to provide feedback within one week to the scheduled dates.",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "course.html#other-less-important-notes",
    "href": "course.html#other-less-important-notes",
    "title": "üß† EEG-2024",
    "section": "Other (less important) notes",
    "text": "Other (less important) notes\n\nThis is the fourth iteration of the course and the material improves every time. The major change this year is the new semester project\nThe other course (‚Äú√úbung‚Äù) will not be made available.\nThis Semesters ‚ÄúDiscussions‚Äù will be in Hybrid-Mode.",
    "crumbs": [
      "Course",
      "‚ÑπÔ∏è Organisation"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "not directly a software, but a specification: the BIDS specification\nIt might be helpful as a manual to get an overview what should be there and what shouldnt.",
    "crumbs": [
      "Resources",
      "üíª Software"
    ]
  },
  {
    "objectID": "software.html#bids",
    "href": "software.html#bids",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "not directly a software, but a specification: the BIDS specification\nIt might be helpful as a manual to get an overview what should be there and what shouldnt.",
    "crumbs": [
      "Resources",
      "üíª Software"
    ]
  },
  {
    "objectID": "software.html#mne-python",
    "href": "software.html#mne-python",
    "title": "üß† EEG-2024",
    "section": "mne-python",
    "text": "mne-python\nMNE Python is a general EEG processing, analysis and visualization toolbox. It will sufficice for 99% of your EEG analyses.",
    "crumbs": [
      "Resources",
      "üíª Software"
    ]
  },
  {
    "objectID": "software.html#mne-bids-pipeline",
    "href": "software.html#mne-bids-pipeline",
    "title": "üß† EEG-2024",
    "section": "mne-bids-pipeline",
    "text": "mne-bids-pipeline\nBIDS is a standardized format (mostly in terms of folder-structures), to save EEG (and other imaging) dataformats. On top of BIDS, one can relatively easily specify an MNE-pipeline using the MNE-BIDS-Pipeline package.\nget started here\nWe recommend strongly to try it out in a tutorial and use it for your semester project data analysis!",
    "crumbs": [
      "Resources",
      "üíª Software"
    ]
  },
  {
    "objectID": "software.html#iclabel",
    "href": "software.html#iclabel",
    "title": "üß† EEG-2024",
    "section": "IClabel",
    "text": "IClabel\na cool tool to automatically classify artefactual independent components is ICLabel. You can use it here: https://github.com/mne-tools/mne-icalabel.\n\n\n\n\n\n\nWarning\n\n\n\nThe combination iclabel + mne_bids_pipeline is relatively fresh. Discussion here: https://github.com/mne-tools/mne-bids-pipeline/pull/1018\nThis should install the correct version:\npip3 install git+https://github.com/jschepers/mne-bids-pipeline.git@merge_ic_label",
    "crumbs": [
      "Resources",
      "üíª Software"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule WS2024",
    "section": "",
    "text": "Note\n\n\n\nAn entry in the table means that your work is due to that weekly meeting (i.e.¬†Thursday, 9:45 am).\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\nDate\nSpecial-Session\nWatch this:\nSubmit this:\nDeliver this:\n\n\n\n\n01\n17.10.2024\nIntro, Organisation\nIntroduction to Neuroimaging\nE-1: My first ERP\n\n\n\n02\n24.10.2024\n\nERPs\nE-2: Tanaka ERP\nGroup-Names + Members\n\n\n03\n31.10.2024\nCancelled: Bene on conference\n\n.\n\n\n.\n\n\n.\n\n\n\n04\n07.11.2024\n\nFilter + Referencing\nE-3: Filtering\nMilestone 1: Paper+Dataset selected\n\n\n05\n14.11.2024\nCancelled: Bene on lab visit\n\n.\n\n\n.\n\n\n.\n\n\n\n06\n21.11.2024\n\nData Cleaning\nE-4: Cleaning\nMilestone 2: Processing Pipeline\n\n\n07\n28.11.2024\n\nBlind Source Separation\nE-5: ICA\n\n\n\n08\n05.12.2024\n\nStatistics + Multiple Linear Regression\nE-6: Linear Models\n\n\n\n09\n12.12.2024\n\nEncoding Models\nE-7: Encoding models\n\n\n\n10\n19.12.2024\n\nMultiple Comparisons\nE-8: Cluster Permutation Tests\nMilestone 3:First subject analyzed\n\n\n11\n26.12.2024\nHolidays\n\n.\n\n\n.\n\n\n.\n\n\n\n12\n02.01.2025\nHolidays\n\n.\n\n\n.\n\n\n.\n\n\n\n13\n09.01.2025\n\nTime Frequency\nE-9: Timefrequency\n\n\n\n14\n16.01.2025\n\nDecoding / RSA / BCI\nE-10: Decoding\n\n\n\n15\n23.01.2025\n\n\n\nMilestone 4 : All subjects\n\n\n16\n30.01.2025\n\nSource Localization\nE-11: Source Localization\n\n\n\n17\n06.02.2025\n\nOther Topics + Outlook\n\nMilestone 5: Propose further reaching analyses\n\n\n18\n13.02.2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe report is due: 31.03.2025",
    "crumbs": [
      "Course",
      "üìÜ Schedule"
    ]
  },
  {
    "objectID": "semesterproject.html",
    "href": "semesterproject.html",
    "title": "EEG Semester Project",
    "section": "",
    "text": "The goal of this project is to re-analyse and document a real EEG dataset using MNE-Python.\nHow I will grade the projects can be seen in grading. I value line of thinking and documented motivation much higher than actual results. I want to see that you understand what you are doing.\n\n\nIn previous iterations, we took a well understood and standardized EEG dataset, which made the task a bit unrealistic and, frankly, a bit boring. Thererfore, you will choose an EEG paper (with data) to reproduce yourself.\n\n\n\n\n\n\nImportant\n\n\n\nThis has the consequence that there will be many (individual) unforseen problems, inconsistencies and problems. This is to be expected and very typical for EEG analyses. See it as part of the challenge!\n\n\nWhat follows is a list of datasets with accompanying papers that I have screened. You can select one of these if you wish, but also feel free to look for your own datasets either on menar.org or on openneuro.org. Try to look for datasets below 20Gb of space, with a paper accompanied. Please send a potential candidate to me asap, so I can screen it.\n\n\n\nSubjects\ntask\nanalysis\nlink\n\n\n\n\n20\ngrating\ndecoding\ndataset\n\n\n19\nreaching\nERP\ndataset\n\n\n24\npattern symmetry\nERP\ndataset\n\n\n12\nreward\nERP\ndataset\n\n\n18\nwalking oddall\nERP\ndataset\n\n\n47\nsocial task\ntimefreq\ndataset\n\n\n17\ngame-playing\nERP\ndataset\n\n\n\n\n\n\nThe idea is to reproduce, but not with a direct reproduction. That is, we will not try to re-do their exact pipeline step-by-step, but rather we will try to obtain their result, with a different pipeline, checking for the robustness of the original analysis pipeline.\nA list of the typical steps you will perform: - Preprocessing: Filtering, re-referencening, ICA, Event-handling - (automatic) data cleaning: Time, channel and subjects\n\n\n\nGiven the new nature of the datasets and the unforeseeable complications, I propose student groups of 2-3 students.\n\n\n\nYou can find the link above\n\n\n\n\nPlease work modularly! Don‚Äôt put all your functions in a single jupyter-notebook. Please use notebooks for your analysis & your functions to a ./src/xyz.py files\nMake use of the BIDS structure and mne-bids-pipeline.\nThoroughly think about what you are doing and why. And note that down - most people can blindly apply a pipeline, I want to see your reasoning why you included / removed certain steps.\n\n\n\n\n\nWrite in the ILIAS Forum. Either others will help, or I myself will answer questions.\nI highly recommend watching the first talk in this session: https://www.crowdcast.io/e/live-meeg-2020/7 by Marijn van Vliet for an fitting introduction of analysing multiple subjects with MNE in a reproducible and documented way. After that, use mne-bids-pipeline as suggested before.\nThe MNE-documentation is quite extensive. It is worth looking into. You can also try stackexchange or neurostar for help.\n\n\n\n\n\n\n\n\n\n\nWhat is the scope of the project report?\n\n\n\nThe project should not be a dissertation. If you report goes beyond 40 pages (which depending on the amount of plots can easily happen), you should really think whether you need more pages to make your point. A project report with 20 pages or less is completely fine, as long as you discuss what you are seeing / plotting / doing.\n\n\n\n\n\n\n\n\nAm I allowed to use other packages\n\n\n\nYes! Everything goes this time around. I do recommend to stick mostly to mne-python though\n\n\n\n\n\n\n\n\nShould I use a single jupyternotebook?\n\n\n\nNo.¬†It is a good idea to use a notebook to display the data / report / results. But one large notebook quickly becomes messy. I‚Äôd recommend to encapsulate code in functions and put them in separate files that you can import and use in your notebook.\n\n\n\n\n\nYou can share a git of your project and a report, or the code and the report via Illias. The git can be on github or on the university gitlab. The report can be a jupyter notebook if you prefer to intermingle code and documentation. The report is there to document your decisions and thoughts along the pipeline. I am especially interested why you chose certain steps / parameters / analyses / visualizations. Make grading easy for me and show that you understood what you are doing and what your results are.\nYou make my life easier if you name the git / zip file with your last name, e.g.¬†Muller_SS2021_EEGSemesterProject.zip, and the folder containing in the zip as well.\n\n\n\nPlease hand in the documents until 31.03.2025",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#which-datasets",
    "href": "semesterproject.html#which-datasets",
    "title": "EEG Semester Project",
    "section": "",
    "text": "In previous iterations, we took a well understood and standardized EEG dataset, which made the task a bit unrealistic and, frankly, a bit boring. Thererfore, you will choose an EEG paper (with data) to reproduce yourself.\n\n\n\n\n\n\nImportant\n\n\n\nThis has the consequence that there will be many (individual) unforseen problems, inconsistencies and problems. This is to be expected and very typical for EEG analyses. See it as part of the challenge!\n\n\nWhat follows is a list of datasets with accompanying papers that I have screened. You can select one of these if you wish, but also feel free to look for your own datasets either on menar.org or on openneuro.org. Try to look for datasets below 20Gb of space, with a paper accompanied. Please send a potential candidate to me asap, so I can screen it.\n\n\n\nSubjects\ntask\nanalysis\nlink\n\n\n\n\n20\ngrating\ndecoding\ndataset\n\n\n19\nreaching\nERP\ndataset\n\n\n24\npattern symmetry\nERP\ndataset\n\n\n12\nreward\nERP\ndataset\n\n\n18\nwalking oddall\nERP\ndataset\n\n\n47\nsocial task\ntimefreq\ndataset\n\n\n17\ngame-playing\nERP\ndataset",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#what-should-the-project-contain",
    "href": "semesterproject.html#what-should-the-project-contain",
    "title": "EEG Semester Project",
    "section": "",
    "text": "The idea is to reproduce, but not with a direct reproduction. That is, we will not try to re-do their exact pipeline step-by-step, but rather we will try to obtain their result, with a different pipeline, checking for the robustness of the original analysis pipeline.\nA list of the typical steps you will perform: - Preprocessing: Filtering, re-referencening, ICA, Event-handling - (automatic) data cleaning: Time, channel and subjects",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#group-work",
    "href": "semesterproject.html#group-work",
    "title": "EEG Semester Project",
    "section": "",
    "text": "Given the new nature of the datasets and the unforeseeable complications, I propose student groups of 2-3 students.",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#where-do-i-get-the-data",
    "href": "semesterproject.html#where-do-i-get-the-data",
    "title": "EEG Semester Project",
    "section": "",
    "text": "You can find the link above",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#tipps",
    "href": "semesterproject.html#tipps",
    "title": "EEG Semester Project",
    "section": "",
    "text": "Please work modularly! Don‚Äôt put all your functions in a single jupyter-notebook. Please use notebooks for your analysis & your functions to a ./src/xyz.py files\nMake use of the BIDS structure and mne-bids-pipeline.\nThoroughly think about what you are doing and why. And note that down - most people can blindly apply a pipeline, I want to see your reasoning why you included / removed certain steps.",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#how-can-i-get-help-advice",
    "href": "semesterproject.html#how-can-i-get-help-advice",
    "title": "EEG Semester Project",
    "section": "",
    "text": "Write in the ILIAS Forum. Either others will help, or I myself will answer questions.\nI highly recommend watching the first talk in this session: https://www.crowdcast.io/e/live-meeg-2020/7 by Marijn van Vliet for an fitting introduction of analysing multiple subjects with MNE in a reproducible and documented way. After that, use mne-bids-pipeline as suggested before.\nThe MNE-documentation is quite extensive. It is worth looking into. You can also try stackexchange or neurostar for help.",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#faq-will-be-updated",
    "href": "semesterproject.html#faq-will-be-updated",
    "title": "EEG Semester Project",
    "section": "",
    "text": "What is the scope of the project report?\n\n\n\nThe project should not be a dissertation. If you report goes beyond 40 pages (which depending on the amount of plots can easily happen), you should really think whether you need more pages to make your point. A project report with 20 pages or less is completely fine, as long as you discuss what you are seeing / plotting / doing.\n\n\n\n\n\n\n\n\nAm I allowed to use other packages\n\n\n\nYes! Everything goes this time around. I do recommend to stick mostly to mne-python though\n\n\n\n\n\n\n\n\nShould I use a single jupyternotebook?\n\n\n\nNo.¬†It is a good idea to use a notebook to display the data / report / results. But one large notebook quickly becomes messy. I‚Äôd recommend to encapsulate code in functions and put them in separate files that you can import and use in your notebook.",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#format-of-report",
    "href": "semesterproject.html#format-of-report",
    "title": "EEG Semester Project",
    "section": "",
    "text": "You can share a git of your project and a report, or the code and the report via Illias. The git can be on github or on the university gitlab. The report can be a jupyter notebook if you prefer to intermingle code and documentation. The report is there to document your decisions and thoughts along the pipeline. I am especially interested why you chose certain steps / parameters / analyses / visualizations. Make grading easy for me and show that you understood what you are doing and what your results are.\nYou make my life easier if you name the git / zip file with your last name, e.g.¬†Muller_SS2021_EEGSemesterProject.zip, and the folder containing in the zip as well.",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "semesterproject.html#deadline",
    "href": "semesterproject.html#deadline",
    "title": "EEG Semester Project",
    "section": "",
    "text": "Please hand in the documents until 31.03.2025",
    "crumbs": [
      "Projects",
      "üìã Descriptions"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "You should be able to get these books via the library. If not, please contact me\n\n\nGood book with a general overview of many topics discussed in the course\n\n\n\nThe standard book for ERP analyses\n\n\n\nThe standard book for time-frequency analysis (a bit on the older side though, still excellent read)",
    "crumbs": [
      "Resources",
      "üìö Bibliography"
    ]
  },
  {
    "objectID": "bibliography.html#text-books",
    "href": "bibliography.html#text-books",
    "title": "Bibliography",
    "section": "",
    "text": "You should be able to get these books via the library. If not, please contact me\n\n\nGood book with a general overview of many topics discussed in the course\n\n\n\nThe standard book for ERP analyses\n\n\n\nThe standard book for time-frequency analysis (a bit on the older side though, still excellent read)",
    "crumbs": [
      "Resources",
      "üìö Bibliography"
    ]
  },
  {
    "objectID": "bibliography.html#websites",
    "href": "bibliography.html#websites",
    "title": "Bibliography",
    "section": "Websites",
    "text": "Websites\n\nLearning EEG\nThis is a really nice website with beautiful images. https://www.learningeeg.com/\n\n\nSteve Luck‚Äôs ERPLab tutorial\nThis website / book is very applied, and unfortunately uses ERPLab and Matlab throughout. Nevertheless, it is a very nice ressources accompanying the course. Applied Event-related Potential Data Analysis",
    "crumbs": [
      "Resources",
      "üìö Bibliography"
    ]
  },
  {
    "objectID": "exercises/ex11_sourceSpace.html",
    "href": "exercises/ex11_sourceSpace.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]\nIn this exercise we will learn how to move Sensor-Space data to Source-space.\n\n\nGeneral remarks\nSource localization is hard. There are numerous moving parts, many parameters and decisions, and a complex chain of tools. Especially the part named ‚Äústructural information‚Äù makes use of the Freesurfer tool, which is a tool that needs quite some time to understand the underlying functionalities, intrications and output. This box depicts how to get from an MRI to a 3D-surface/headmodel & how to align this headmodel to the coordinate system of your electrode sensors.\n\n\n\nMNE Flow\n\n\nWe will skip these steps completly and start with an already-segment ‚Äúdefault‚Äù MRI (called ‚Äòfsaverage‚Äô).\n\nImportant:\nSource-reconstructions with a default-MRI (opposite of a individual MRI) introduces even more noise (=uncertainty) than already existing in ‚Äúoptimal‚Äù source localization. Your source-localizations should therefore be interpreted even more carefully than with individual MRIs. Never let yourself be fooled by the apparent precision of source-localizations!!\n\n\n\nSetup\nWe first need to install the python packages ‚Äúpysurfer‚Äù and ‚Äúmayavi‚Äù - if mayavi doesnt work,you can also try pyvista & pyvistaqt\n\n!! 3D Frustration alert!!\n3D Plots are annoying. They crash your system, they are slow, they are unstable. The frustration is normal and unfortunately still to be expected.\n\n\nActual start of exercise\nNext we have to do some downloading, i.e.¬†download the average-brain MRI and the pre-computed headmodel (BEM-method)\n\nimport os.path as op\n\nimport mne\nfrom mne.datasets import eegbci\nfrom mne.datasets import fetch_fsaverage\n\n# Download fsaverage files\nfs_dir = fetch_fsaverage(verbose=True)\nsubjects_dir = op.dirname(fs_dir)\n\n# The files live in:\nsubject = 'fsaverage'\ntrans = 'fsaverage'  # MNE has a built-in fsaverage transformation\nsrc = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\nbem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\nNext we need a dataset that we want to source localize. We will use the LIMO dataset, which has lot‚Äôs of trials, but unfortauntely neither individual headmodel, nor individual electrode-positions.\nfrom mne.datasets.limo import load_data\nepochs = load_data(subject=3,path='../local/limo') #\nepochs.set_eeg_reference(projection=True)  # needed for inverse modeling\n\n\nAlignment of brains and sensors\nIt is of outmost importance, to align sensors and brains. This can be a very tricky task if you have individual MRIs & headmodels, because brains are mirror-symmetric and electrode locations as well. Thus R/L Flips are common.\nTypically we define 3 anatomical landmarks, the nasion (between your eyes) and the Auricular Points Left and Right, a point just infront of your ear-canals. These points can be defined in your electrode-locations (e.g.¬†using a digitizer), and are visible in the MRI.\nLuckily, for us everything is alligned already and we can plot it using:\n# Check that the locations of EEG electrodes is correct with respect to MRI\n%matplotlib qt\n#%gui qt\np = mne.viz.plot_alignment(\n    epochs.info, src=src, eeg=['original', 'projected'], trans=trans,\n    show_axes=True, mri_fiducials=True, dig='fiducials')\n\np\nHint: You can close the 3D plots using p.plotter.close() and later plots using b.close(). At least on my machine I had many crashes if I kept them floating around‚Ä¶\n\n\n\nThe forward model\nThe forward model translates source-activity to sensor-activity. We have to provide the sensor locations (epochs.info), the transformations of sensorlocations to BEM model (trans) and the actual physical spheres (bem). The default conductivities for the BEM model are already saved in the pre-computed standard BEM model.\nThe standard-forward model can be calculated using fwd = mne.make_forward_solution(epochs.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0)\nThe leadfield can be extracted by fwd[\"sol\"][\"data\"].\nT: What is the shape of the leadfield and what do the dimension refer to?\nT: select a random source-point and plot its respective sensor-topoplot (mne.viz.plot_topomap(vector,epochs.info))\nT: Next plot three following source-points. Be sure to start with 0,3,6,9‚Ä¶ (e.g.¬†multiply your random ‚Äústarting‚Äù-index by 3).What do you observe and what does it tell you about the structure of the forward model? (hint: you could also inspect fwd[\"source_nn\"]) - plot_topomap has an ‚Äúaxes‚Äù option to specify subplot-axes, be sure to put show=False if you want to use that.\nNow we will plot the reverse, choose one electrode and see which sources are most sensitive to that electrode. Note that you only need every third sample, i.e.¬†we are looking for the activity at the cortex in only one dimension. It is a bit more involved to collapse over all source-orientations.\nTo plot the vector on a 3D brain, you can use this piece of code:\n\nfrom mne.datasets import fetch_fsaverage\nfrom surfer import Brain\nfs_dir = fetch_fsaverage(verbose=True)\nsubjects_dir = op.dirname(fs_dir)\n\n# generate Brain Plot\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\n\n# get the surface model, get the left-hemisphere (`0`), geht the vertice-indices\nv = fwd[\"src\"][0]['vertno']\nnvert = len(v)\n\n# x is the leadfield slice spanning both hemisphere, we going to take only the first half of it.\nb.add_data(x[0:nvert],vertices=v,smoothing_steps='nearest')\n\n\nInverse Solutions\n\nMy first MNE\nWe learned in the lecture, that the Minimum Norm Solution, is simply the pseudo-inverse of the leadfield matrix.\nT: To get started, plot the evoked data once more to remember what the ERP looks like. We want to zoom in the component which has its strongest activity around 150ms.\nT: Use from scipy.linalg import pinv on one orientation of the Leadfield (fwd[\"sol\"][\"data\"][:,::3]) and @-multiply it on your epochs.copy().crop(tmin=0.15,tmax=0.15).average().interpolate_bads().data[:,0] - you need to interpolate the bad channels, else they will project noise to your source-space\nT: Plot a histogram of your source values. Do negative values make sense (solution below, don‚Äôt peak ;))?\nBonus: instead of cropping to around 150ms, you can also calculate all timepoints and later select a timepoint, but it is a bit more cumbersome to handle everything\nOnce we have the vector of source-activities, we want to plot it\n%gui qt\nfrom surfer import Brain\nimport numpy as np\n\n# generate brain\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\n\n# which vertices to plot?\nv = fwd[\"src\"][0]['vertno']\nnvert = len(v)\n\n# add the data to the plot - note we are using np.abs here!\nb.add_data(np.abs(s[0:nvert]),vertices=v,colormap=\"hot\")\n\n# use some kind of sensible colormap\nlim = np.percentile(np.abs(s),[30,80,98])\nb.scale_data_colormap(fmin=lim[0], fmid=lim[1], fmax=lim[2], transparent=True)\nNote the usage of np.abs around the source activity. It is generally not really possible to interpret positive / negative source activity. if you are on one side of a gyrus, the activity might be positive, on the other side it might be negative - simply because of dipole orientation. We therefore often just look at the absolute value\nBonus: If you wanted to plot all time-points, you can use b.set_time(200) to set time, or from surfer import TimeViewer and viewer = TimeViewer(b) to get a rudimentairy GUI\n\n\nFinishing words to the manual implementation\nThis toy-implementation has two serious limitations:\n- no control over how much regularisation\n- only one orientation of the Leadfield was used\nWe will fix those using the MNE functionalities next.\n\n\n\nCovariance Matrices\nIt turns out, calculating the MNE solution via pseudoinverse is not necessarily the most versatile approach. the MNE toolbox (this is b.t.w. where the name originally came from), decided that a different parameterization is useful.\nThey build everything around ‚Äúnoise-covariance‚Äù over channels. I.e. in a period of time where no systematic activity happens, how much do channels covary? Why is this useful? It is useful for regularisation. Naively we could regularize all activity, but it would be more effective, if we could regularize noise more than signal. Thus, if we know something about the structure of the noise (the channel noise-covariance), we can regularize more effectively.\nAs a sidenote: This is also convenient for the LCMV-beamformer: there we would need not the noise-covariance, but the data-covariances (how do channels covary if to-be-explained activity is present [per condition])\nAnother sidenote: In case we would want to recover the original MNE solution, we‚Äôd have to input a identity-matrix as the covariance matrix.\nT: Calculate the covariance matrix for noise and plot it:\nnoise_cov = mne.compute_covariance(epochs, tmax=0)\nnoise_cov.plot(epochs.info)\nQ: What does the 2d-image plot tell you?\n\nInverse Operator\nNext, we need to specify the inverse operator, this is where Leadfield (forward model) and noise-covariance meet. This is also where we could constrict the orientations of our dipoles to be orthogonal to the cortex & where we could use depth-correction of the leadfield.\nWe generate an inverse operator like this:\nfrom mne.minimum_norm import make_inverse_operator, apply_inverse\ninv_default = make_inverse_operator(epochs.info, fwd, noise_cov, loose=0.2, depth=0.8)\nT: Generate three more operator, one with loose=1, allowing for all dipole orientations, one with loose=0, enforcing strict orthogonal orientation (don‚Äôt do that) and one with loose=0.2, but depth=0. - deactivating depth weighting.\n\n\nApplying the inverse\nNext, we have to apply the inverse operator. For this, we still need to define how much regularisation we want to apply. If we expect noisy data, we should put lot‚Äôs of regularisation. If we expect high SNR, there should be less regularisation. Here the MNE implementation really shines: We can easily define a Signal-To-Noise ratio and translate it to a regularisation parameter that will fit.\nsnr = 10.0\nlambda2 = 1.0 / snr ** 2\nT: calculate the inverse solution and get the stc (source time course) for the inv_default - choose the MNE algorithm in the function mne.minimum_norm.apply_inverse T: Plot it! brain = stc.plot(time_viewer=True,hemi=\"both\")\nBonus: You can also plot the brain activity on a ‚Äúinflated‚Äù or ‚Äúflat‚Äù brain using surface = \"flat\" (or inflated respectively). This can be very helpful in comparing conditions etc. - but especially the flat map will be very hard to read for everyone who is not a brain-anatomy-expert\n\n\n\nComparing Inverse Solutions\nWe will do a set of source-solution comparisons. Note that this is just one example, and just a subset of all possible parameters. It might give you a small glimpse, and maybe even only a missleading intuition. But still, at least it will give you a starting point for experience in source localization.\n\nComparing our dipole-orientation & depth models\nI feel like this exercise is already pretty long, so I will give you larger chunks of code.\n# plotting parameters\nbrain_kwargs = dict(views=\"lateral\",hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n\n# getting larger figures\nplt.rcParams['figure.dpi'] = 100 \nplt.rcParams[\"figure.figsize\"] = (20,15)\n\n# a list of all our inverse operators\nsourceList = [inv_default,inv_loose0,inv_loose1,inv_depth0]\n\n#loop over inverse operators\nfor (ix,inv) in enumerate(sourceList):\n\n    # calculate the stc\n    s = apply_inverse(epochs.average(), inv, lambda2, 'MNE', verbose=True)#, pick_ori='vector')\n    \n    # plot it\n    h = s.plot(**brain_kwargs)\n\n    # grab a png from it\n    img = ccs_eeg_utils.stc_plot2img(h,closeAfterwards=False)\n\n    # add the png to a subplot\n    ax = plt.subplot(2,2,ix+1).imshow(img)\n    plt.axis('off')\nBonus-Q: Why did MNE suddenly decide to plot pos/neg for the loose=0 (fixed orientation) instead of abs?\n\n\nComparing algorithms\nNext we will compare the four different minimum-norm algorithms implemented in MNE: ['MNE','sLORETA','eLORETA','dSPM'] adapt the previous script to use inv_default for all of them and plot the solutions again.\n\n\nFinal exploration: Different regularisation\nAdapt the above script once more, and apply different SNRs (e.g.¬†[0.1,2,10,50]). What do you observe? Make note of the MNE text-output, it tells you how much % of your actual observed data is explained by the solution. Do they roughly match your expectations?\n\n\n\nSome further notes:\n\nThere are no ideal parameters or algorithms. Different combinations of parameters will introduce different assumptions will lead to potentially very different solutions. One conclusion from our exploration with a single subject might be: Activity comes from visual cortex, but maybe this is something we knew before.\nSo far, we were projecting ERPs conditionwise to the source space. But you can also plot subtractions of conditions, highlighting where the effects occured. Further, you could even directly project betas or difference-waves to source space.\nIf you have multiple subjects, and you want to perform statistics, you need to match the head-models of subjects. Given that we do not have individual headmodels, this is no problem for us. All vertices / dipoles are at exactly the same position for all subjects - convenient!\nPerforming statistics over subjects works quite similarly (check out this tutorial) to previous analysis. In case of cluster-permutation tests, adjacency/neighbourhood needs to be defined in 3D space on the surface. Computation time might be increased enourmously.\nAll of our source-space analysis have been performed on the surface of the cortex. This is where the gray matter is, and where we hypothesize that our EEG signal is produced. But we might be wrong, there might be deep-sources, e.g.¬†hippocampus or cerebellum also generating our potentials. Thus instead of restricting our source analysis to the surface, we could also make use of the whole 3D volume of the brain. This is less well documented in MNE, but also possible. In some sense it is a question of preference.\nAgain my warning: Do not overinterpret your source analysis results! You might be centimeters away from your real activity."
  },
  {
    "objectID": "exercises/solutions/ex11_sourceSpace.html",
    "href": "exercises/solutions/ex11_sourceSpace.html",
    "title": "General remarks",
    "section": "",
    "text": "# packages: ; pysurfer\n%load_ext autoreload\n%autoreload 2\nfrom matplotlib import pyplot as plt\n\nSource localization is hard. There are numerous moving parts, many parameters and decisions, and a complex chain of tools. Especially the part named ‚Äústructural information‚Äù makes use of the Freesurfer tool, which is a tool that needs quite some time to understand the underlying functionalities, intrications and output. This box depicts how to get from an MRI to a 3D-surface/headmodel & how to align this headmodel to the coordinate system of your electrode sensors.\n\n\n\nMNE Flow\n\n\nWe will skip these steps completly and start with an already-segment ‚Äúdefault‚Äù MRI (called ‚Äòfsaverage‚Äô).\n\nImportant:\nSource-reconstructions with a default-MRI (opposite of a individual MRI) introduces even more noise (=uncertainty) than already existing in ‚Äúoptimal‚Äù source localization. Your source-localizations should therefore be interpreted even more carefully than with individual MRIs. Never let yourself be fooled by the apparent precision of source-localizations!!\n\n\nSetup\nWe first need to install the python packages ‚Äúpyvista‚Äù & ‚Äúpyvistaqt‚Äù\n\n!! 3D Frustration alert!!\n3D Plots are annoying. They crash your system, they are slow, they are unstable. The frustration is normal and unfortunately still to be expected.\n\n\nActual start of exercise\nNext we have to do some downloading, i.e.¬†download the average-brain MRI and the pre-computed headmodel (BEM-method)\n\nimport os.path as op\n\nimport mne\nfrom mne.datasets import eegbci\nfrom mne.datasets import fetch_fsaverage\n\n# Download fsaverage files\nfs_dir = fetch_fsaverage(verbose=True)\nsubjects_dir = op.dirname(fs_dir)\n\n# The files live in:\nsubject = 'fsaverage'\ntrans = 'fsaverage'  # MNE has a built-in fsaverage transformation\nsrc = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\nbem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n\n\nimport os.path as op\n\nimport mne\nfrom mne.datasets import eegbci\nfrom mne.datasets import fetch_fsaverage\n\n# Download fsaverage files\nfs_dir = fetch_fsaverage(verbose=True)\nsubjects_dir = op.dirname(fs_dir)\n\n# The files live in:\nsubject = 'fsaverage'\ntrans = 'fsaverage'  # MNE has a built-in fsaverage transformation\nsrc = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\nbem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n\n0 files missing from root.txt in /home/ehinger/mne_data/MNE-fsaverage-data\n0 files missing from bem.txt in /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage\n\n\nNext we need a dataset that we want to source localize. We will use the LIMO dataset, which has lot‚Äôs of trials, but unfortauntely neither individual headmodel, nor individual electrode-positions.\nfrom mne.datasets.limo import load_data\nepochs = load_data(subject=3,path='../local/limo') #\nepochs.set_eeg_reference(projection=True)  # needed for inverse modeling\n\nfrom mne.datasets.limo import load_data\nepochs = load_data(subject=3,path='../local/limo') #\nepochs.set_eeg_reference(projection=True)  # needed for inverse modeling\n\nAdding metadata with 2 columns\n1072 matching events found\nNo baseline correction applied\n0 projection items activated\n0 bad epochs dropped\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n\n\n\n\n\n\n\n\n\n\nNumber of events\n1072\n\n\nEvents\nFace/A: 538\nFace/B: 534\n\n\nTime range\n-0.300 ‚Äì 0.500 sec\n\n\nBaseline\noff\n\n\n\n\n\n\n\n\nAlignment of brains and sensors\nIt is of outmost importance, to align sensors and brains. This can be a very tricky task if you have individual MRIs & headmodels, because brains are mirror-symmetric and electrode locations as well. Thus R/L Flips are common.\nTypically we define 3 anatomical landmarks, the nasion (between your eyes) and the Auricular Points Left and Right, a point just infront of your ear-canals. These points can be defined in your electrode-locations (e.g.¬†using a digitizer), and are visible in the MRI.\nLuckily, for us everything is alligned already and we can plot it using:\n# Check that the locations of EEG electrodes is correct with respect to MRI\n\np = mne.viz.plot_alignment(\n    epochs.info, src=src, eeg=['original', 'projected'], trans=trans,\n    show_axes=True, mri_fiducials=True, dig='fiducials')\n\np\nHint: You can close the 3D plots using p.plotter.close() and later plots using b.close(). At least on my machine I had many crashes if I kept them floating around‚Ä¶\n\n# Check that the locations of EEG electrodes is correct with respect to MRI\n\nmne.viz.set_3d_backend(\"notebook\")\n\np = mne.viz.plot_alignment(\n    epochs.info, src=src, eeg=['original', 'projected'], trans=\"fsaverage\", # try trans=None for fun\n    show_axes=True, mri_fiducials=True, dig='fiducials')\n\np\n\nReading /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif...\nUsing outer_skin.surf for head surface.\nChannel types:: eeg: 121\nProjecting sensors to the head surface\n\n\n\n\n\n\n\n\n&lt;mne.viz.backends._pyvista.PyVistaFigure at 0x7f42483d0910&gt;\n\n\n\n\n\nThe forward model\nThe forward model translates source-activity to sensor-activity. We have to provide the sensor locations (epochs.info), the transformations of sensorlocations to BEM model (trans) and the actual physical spheres (bem). The default conductivities for the BEM model are already saved in the pre-computed standard BEM model.\n\nfwd = mne.make_forward_solution(epochs.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0)\n\nSource space          : /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif\nMRI -&gt; head transform : /home/ehinger/miniconda3/envs/eegCourse/lib/python3.10/site-packages/mne/data/fsaverage/fsaverage-trans.fif\nMeasurement data      : instance of Info\nConductor model   : /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif\nAccurate field computations\nDo computations in head coordinates\nFree source orientations\n\nReading /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif...\nRead 2 source spaces a total of 20484 active source locations\n\nCoordinate transformation: MRI (surface RAS) -&gt; head\n     0.999994  0.003552  0.000202      -1.76 mm\n    -0.003558  0.998389  0.056626      31.09 mm\n    -0.000001 -0.056626  0.998395      39.60 mm\n     0.000000  0.000000  0.000000       1.00\n\nRead 128 EEG channels from info\nHead coordinate coil definitions created.\nSource spaces are now in head coordinates.\n\nSetting up the BEM model using /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif...\n\nLoading surfaces...\n\nLoading the solution matrix...\n\nThree-layer model surfaces loaded.\nLoaded linear_collocation BEM solution from /home/ehinger/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-5120-5120-5120-bem-sol.fif\nEmploying the head-&gt;MRI coordinate transform with the BEM model.\nBEM model fsaverage-5120-5120-5120-bem-sol.fif is now set up\n\nSource spaces are in head coordinates.\nChecking that the sources are inside the surface and at least    5.0 mm away (will take a few...)\nChecking surface interior status for 10242 points...\n    Found  2433/10242 points inside  an interior sphere of radius   47.7 mm\n    Found     0/10242 points outside an exterior sphere of radius   98.3 mm\n    Found     0/ 7809 points outside using surface Qhull\n    Found     0/ 7809 points outside using solid angles\n    Total 10242/10242 points inside the surface\nInterior check completed in 7386.9 ms\nChecking surface interior status for 10242 points...\n    Found  2241/10242 points inside  an interior sphere of radius   47.7 mm\n    Found     0/10242 points outside an exterior sphere of radius   98.3 mm\n    Found     0/ 8001 points outside using surface Qhull\n    Found     0/ 8001 points outside using solid angles\n    Total 10242/10242 points inside the surface\nInterior check completed in 7531.0 ms\n\nSetting up for EEG...\nComputing EEG at 20484 source locations (free orientations)...\n\nFinished.\n\n\n\nfwd[\"sol\"][\"data\"].shape\n\n(128, 61452)\n\n\n\n%matplotlib inline\n\nmne.viz.plot_topomap(fwd[\"sol\"][\"data\"][:,2],epochs.info)\nfrom matplotlib import pyplot as plt\nplt.figure()\nmne.viz.plot_topomap(fwd[\"sol\"][\"data\"][:,8],epochs.info)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(&lt;matplotlib.image.AxesImage at 0x7f4293e5b2e0&gt;,\n &lt;matplotlib.contour.QuadContourSet at 0x7f4293e5b5e0&gt;)\n\n\nThe leadfield can be extracted by fwd[\"sol\"][\"data\"].\nT: What is the shape of the leadfield and what do the dimension refer to?\nT: select a random source-point and plot its respective sensor-topoplot (mne.viz.plot_topomap(vector,epochs.info))\nT: Next plot three following source-points. Be sure to start with 0,3,6,9‚Ä¶ (e.g.¬†multiply your random ‚Äústarting‚Äù-index by 3).What do you observe and what does it tell you about the structure of the forward model? (hint: you could also inspect fwd[\"source_nn\"]) - plot_topomap has an ‚Äúaxes‚Äù option to specify subplot-axes, be sure to put show=False if you want to use that.\n\nimport mne.viz\nprint(fwd[\"sol\"][\"data\"].shape)\n#mne.viz.plot_topomap(fwd[\"sol\"][\"data\"][:,1],epochs.info);\nplt.figure()\nfig,ax = plt.subplots(1,3)\nfor ix in range(3):\n    mne.viz.plot_topomap(fwd[\"sol\"][\"data\"][:,6+ix],epochs.info,axes=ax[ix],show=False);\nplt.show();\n\n(128, 61452)\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNow we will plot the reverse, choose one electrode and see which sources are most sensitive to that electrode. Note that you only need every third sample, i.e.¬†we are looking for the activity at the cortex in only one dimension. It is a bit more involved to collapse over all source-orientations.\nTo plot the vector on a 3D brain, you can use this piece of code:\n\nfrom mne.datasets import fetch_fsaverage\nfrom surfer import Brain\nfs_dir = fetch_fsaverage(verbose=True)\nsubjects_dir = op.dirname(fs_dir)\n\n# generate Brain Plot\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\n\n# get the surface model, get the left-hemisphere (`0`), geht the vertice-indices\nv = fwd[\"src\"][0]['vertno']\nnvert = len(v)\n\n# x is the leadfield slice spanning both hemisphere, we going to take only the first half of it.\nb.add_data(x[0:nvert],vertices=v,smoothing_steps='nearest')\n¬¥¬¥¬¥\n\n::: {#cell-16 .cell execution_count=14}\n``` {.python .cell-code}\nfrom mne.viz import Brain\n\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\n\nx = fwd[\"sol\"][\"data\"][6,::3]\n\nnvert = len(fwd[\"src\"][0]['vertno'])\nv = fwd[\"src\"][0]['vertno']\n#v = fwd[\"src\"][0][\"use_tris\"]\nprint(len(v))\n#b.add_overlay()\nb.add_data(x[0:nvert],vertices=v,smoothing_steps='nearest')\n\n\n\n\n\n\n\n10242\n\n:::\n\nb.close()\n\n\n\nInverse Solutions\n\nMy first MNE\nWe learned in the lecture, that the Minimum Norm Solution, is simply the pseudo-inverse of the leadfield matrix.\nT: To get started, plot the evoked data once more to remember what the ERP looks like. We want to zoom in the component which has its strongest activity around 150ms.\nT: Use from scipy.linalg import pinv on one orientation of the Leadfield (fwd[\"sol\"][\"data\"][:,::3]) and @-multiply it on your epochs.copy().crop(tmin=0.15,tmax=0.15).average().interpolate_bads().data[:,0] - you need to interpolate the bad channels, else they will project noise to your source-space\nT: Plot a histogram of your source values. Do negative values make sense (solution below, don‚Äôt peak ;))?\nBonus: instead of cropping to around 150ms, you can also calculate all timepoints and later select a timepoint, but it is a bit more cumbersome to handle everything\n\nepochs.average().plot();\n\n\n\n\n\n\n\n\n\nL = fwd[\"sol\"][\"data\"][:,::3]\nL.shape\n\n(128, 20484)\n\n\n\nd = epochs.copy().crop(tmin=0.150,tmax=0.153).average().interpolate_bads().data\nd.shape\nfrom scipy.linalg import pinv\n\nMNE = pinv(L)\n\nMNE.shape\n\nInterpolating bad channels\n    Automatic origin fit: head of radius 95.0 mm\nComputing interpolation matrix from 121 sensor positions\nInterpolating 7 sensors\n\n\n(20484, 128)\n\n\n\nfrom scipy.linalg import pinv\n\nL = fwd[\"sol\"][\"data\"][:,::3]\n\nMNE = pinv(L)\n\nd = epochs.copy().crop(tmin=0.150,tmax=0.153).average().interpolate_bads().data\ns = MNE@d\n\nInterpolating bad channels\n    Automatic origin fit: head of radius 95.0 mm\nComputing interpolation matrix from 121 sensor positions\nInterpolating 7 sensors\n\n\n\nplt.hist(s,100);\n\n\n\n\n\n\n\n\nOnce we have the vector of source-activities, we want to plot it\n%gui qt\nfrom surfer import Brain\nimport numpy as np\n\n# generate brain\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\n\n# which vertices to plot?\nv = fwd[\"src\"][0]['vertno']\nnvert = len(v)\n\n# add the data to the plot - note we are using np.abs here!\nb.add_data(np.abs(s[0:nvert]),vertices=v,colormap=\"hot\")\n\n# use some kind of sensible colormap\nlim = np.percentile(np.abs(s),[30,80,98])\nb.scale_data_colormap(fmin=lim[0], fmid=lim[1], fmax=lim[2], transparent=True)\nNote the usage of np.abs around the source activity. It is generally not really possible to interpret positive / negative source activity. if you are on one side of a gyrus, the activity might be positive, on the other side it might be negative - simply because of dipole orientation. We therefore often just look at the absolute value\nBonus: If you wanted to plot all time-points, you can use b.set_time(200) to set time, or from surfer import TimeViewer and viewer = TimeViewer(b) to get a rudimentairy GUI\n\nv = fwd[\"src\"][0]['vertno']\nv\n\narray([    0,     1,     2, ..., 10239, 10240, 10241])\n\n\n\n%gui qt\nfrom surfer import Brain\nimport numpy as np\nb = Brain('fsaverage', \"lh\", \"white\", background='white',subjects_dir=subjects_dir)\nnvert = len(fwd[\"src\"][0]['vertno'])\nv = fwd[\"src\"][0]['vertno']\nprint(len(v))\nb.add_data(np.abs(s[0:nvert]),vertices=v,colormap=\"hot\")\n\nlim = np.percentile(np.abs(s),[30,80,98])\nb.scale_data_colormap(fmin=lim[0], fmid=lim[1], fmax=lim[2], transparent=True)\n\n10242\n\n\n\nb.close()\n\n\n\nFinishing words to the manual implementation\nThis toy-implementation has two serious limitations:\n- no control over how much regularisation\n- only one orientation of the Leadfield was used\nWe will fix those using the MNE functionalities next.\n\n\n\nCovariance Matrices\nIt turns out, calculating the MNE solution via pseudoinverse is not necessarily the most versatile approach. the MNE toolbox (this is b.t.w. where the name originally came from), decided that a different parameterization is useful.\nThey build everything around ‚Äúnoise-covariance‚Äù over channels. I.e. in a period of time where no systematic activity happens, how much do channels covary? Why is this useful? It is useful for regularisation. Naively we could regularize all activity, but it would be more effective, if we could regularize noise more than signal. Thus, if we know something about the structure of the noise (the channel noise-covariance), we can regularize more effectively.\nAs a sidenote: This is also convenient for the LCMV-beamformer: there we would need not the noise-covariance, but the data-covariances (how do channels covary if to-be-explained activity is present [per condition])\nAnother sidenote: In case we would want to recover the original MNE solution, we‚Äôd have to input a identity-matrix as the covariance matrix.\nT: Calculate the covariance matrix for noise and plot it:\nnoise_cov = mne.compute_covariance(epochs, tmax=0)\nnoise_cov.plot(epochs.info)\nQ: What does the 2d-image plot tell you?\n\nnoise_cov = mne.compute_covariance(epochs, tmax=0)\nnoise_cov.plot(epochs.info)\n\nComputing rank from data with rank=None\n\n\n/tmp/ipykernel_2058897/3142076790.py:1: RuntimeWarning: Epochs are not baseline corrected, covariance matrix may be inaccurate\n  noise_cov = mne.compute_covariance(epochs, tmax=0)\n\n\n    Using tolerance 1.5e-10 (2.2e-16 eps * 121 dim * 5.5e+03  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 1 projector\n    Created an SSP operator (subspace dimension = 1)\n    Setting small EEG eigenvalues to zero (without PCA)\nReducing data rank from 121 -&gt; 120\nEstimating covariance using EMPIRICAL\nDone.\nNumber of samples used : 81472\n[done]\nComputing rank from covariance with rank=None\n    Using tolerance 1e-13 (2.2e-16 eps * 121 dim * 3.7  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 0 projectors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInverse Operator\nNext, we need to specify the inverse operator, this is where Leadfield (forward model) and noise-covariance meet. This is also where we could constrict the orientations of our dipoles to be orthogonal to the cortex & where we could use depth-correction of the leadfield.\nWe generate an inverse operator like this:\nfrom mne.minimum_norm import make_inverse_operator, apply_inverse\ninv_default = make_inverse_operator(epochs.info, fwd, noise_cov, loose=0.2, depth=0.8)\nT: Generate three more operator, one with loose=1, allowing for all dipole orientations, one with loose=0, enforcing strict orthogonal orientation (don‚Äôt do that) and one with loose=0.2, but depth=0. - deactivating depth weighting.\n\nfrom mne.minimum_norm import make_inverse_operator, apply_inverse\n\ninv_default= make_inverse_operator(epochs.info, fwd, noise_cov, loose=0.2, depth=0.8)\ninv_loose0 = make_inverse_operator(epochs.info, fwd, noise_cov, loose=0,   depth=0.8)\ninv_loose1 = make_inverse_operator(epochs.info, fwd, noise_cov, loose=1,   depth=0.8)\ninv_depth0 = make_inverse_operator(epochs.info, fwd, noise_cov, loose=0.2, depth=0.)\n\nConverting forward solution to surface orientation\n    No patch info available. The standard source space normals will be employed in the rotation to the local surface coordinates....\n    Converting to surface-based source orientations...\n    [done]\ninfo[\"bads\"] and noise_cov[\"bads\"] do not match, excluding bad channels from both\nComputing inverse operator with 121 channels.\n    121 out of 128 channels remain after picking\nSelected 121 channels\nCreating the depth weighting matrix...\n    121 EEG channels\n    limit = 20485/20484 = 2.572931\n    scale = 174000 exp = 0.8\nApplying loose dipole orientations to surface source spaces: 0.2\nWhitening the forward solution.\n    Created an SSP operator (subspace dimension = 1)\nComputing rank from covariance with rank=None\n    Using tolerance 1e-13 (2.2e-16 eps * 121 dim * 3.7  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 1 projector\n    Setting small EEG eigenvalues to zero (without PCA)\nCreating the source covariance matrix\nAdjusting source covariance matrix.\nComputing SVD of whitened and weighted lead field matrix.\n    largest singular value = 10.8234\n    scaling factor to adjust the trace = 1.65211e+22 (nchan = 121 nzero = 1)\ninfo[\"bads\"] and noise_cov[\"bads\"] do not match, excluding bad channels from both\nComputing inverse operator with 121 channels.\n    121 out of 128 channels remain after picking\nSelected 121 channels\nCreating the depth weighting matrix...\n    121 EEG channels\n    limit = 20485/20484 = 2.572931\n    scale = 174000 exp = 0.8\n    Picked elements from a free-orientation depth-weighting prior into the fixed-orientation one\n    No patch info available. The standard source space normals will be employed in the rotation to the local surface coordinates....\n    Changing to fixed-orientation forward solution with surface-based source orientations...\n    [done]\nWhitening the forward solution.\n    Created an SSP operator (subspace dimension = 1)\nComputing rank from covariance with rank=None\n    Using tolerance 1e-13 (2.2e-16 eps * 121 dim * 3.7  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 1 projector\n    Setting small EEG eigenvalues to zero (without PCA)\nCreating the source covariance matrix\nAdjusting source covariance matrix.\nComputing SVD of whitened and weighted lead field matrix.\n    largest singular value = 10.824\n    scaling factor to adjust the trace = 1.16372e+22 (nchan = 121 nzero = 1)\ninfo[\"bads\"] and noise_cov[\"bads\"] do not match, excluding bad channels from both\nComputing inverse operator with 121 channels.\n    121 out of 128 channels remain after picking\nSelected 121 channels\nCreating the depth weighting matrix...\n    121 EEG channels\n    limit = 20485/20484 = 2.572931\n    scale = 174000 exp = 0.8\nWhitening the forward solution.\n    Created an SSP operator (subspace dimension = 1)\nComputing rank from covariance with rank=None\n    Using tolerance 1e-13 (2.2e-16 eps * 121 dim * 3.7  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 1 projector\n    Setting small EEG eigenvalues to zero (without PCA)\nCreating the source covariance matrix\nAdjusting source covariance matrix.\nComputing SVD of whitened and weighted lead field matrix.\n    largest singular value = 10.8232\n    scaling factor to adjust the trace = 3.60565e+22 (nchan = 121 nzero = 1)\nConverting forward solution to surface orientation\n    No patch info available. The standard source space normals will be employed in the rotation to the local surface coordinates....\n    Converting to surface-based source orientations...\n    [done]\ninfo[\"bads\"] and noise_cov[\"bads\"] do not match, excluding bad channels from both\nComputing inverse operator with 121 channels.\n    121 out of 128 channels remain after picking\nSelected 121 channels\nApplying loose dipole orientations to surface source spaces: 0.2\nWhitening the forward solution.\n    Created an SSP operator (subspace dimension = 1)\nComputing rank from covariance with rank=None\n    Using tolerance 1e-13 (2.2e-16 eps * 121 dim * 3.7  max singular value)\n    Estimated rank (eeg): 120\n    EEG: rank 120 computed from 121 data channels with 1 projector\n    Setting small EEG eigenvalues to zero (without PCA)\nCreating the source covariance matrix\nAdjusting source covariance matrix.\nComputing SVD of whitened and weighted lead field matrix.\n    largest singular value = 10.8192\n    scaling factor to adjust the trace = 4.07372e+22 (nchan = 121 nzero = 1)\n\n\n\n\nApplying the inverse\nNext, we have to apply the inverse operator. For this, we still need to define how much regularisation we want to apply. If we expect noisy data, we should put lot‚Äôs of regularisation. If we expect high SNR, there should be less regularisation. Here the MNE implementation really shines: We can easily define a Signal-To-Noise ratio and translate it to a regularisation parameter that will fit.\nsnr = 10.0\nlambda2 = 1.0 / snr ** 2\nT: calculate the inverse solution and get the stc (source time course) for the inv_default - choose the MNE algorithm in the function mne.minimum_norm.apply_inverse T: Plot it! brain = stc.plot(time_viewer=True,hemi=\"both\")\n\nsnr = 10.0\nlambda2 = 1.0 / snr ** 2\nstc = apply_inverse(epochs.average(), inv_default, lambda2, 'MNE')#, pick_ori='vector')\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n[done]\n\n\n\nbrain = stc.plot(time_viewer=True,hemi=\"both\")\n\nUsing control points [8.26314785e-12 9.82639882e-12 2.66900633e-11]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus: You can also plot the brain activity on a ‚Äúinflated‚Äù or ‚Äúflat‚Äù brain using surface = \"flat\" (or inflated respectively). This can be very helpful in comparing conditions etc. - but especially the flat map will be very hard to read for everyone who is not a brain-anatomy-expert\n\nstc.plot(surface=\"flat\",time_viewer=True)\n#stc.plot(surface=\"flat\",time_viewer=True)\n\nUsing control points [8.26314785e-12 9.82639882e-12 2.66900633e-11]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;mne.viz._brain._brain.Brain at 0x7f4292a452a0&gt;\n\n\n\n\n\nComparing Inverse Solutions\nWe will do a set of source-solution comparisons. Note that this is just one example, and just a subset of all possible parameters. It might give you a small glimpse, and maybe even only a missleading intuition. But still, at least it will give you a starting point for experience in source localization.\n\nComparing our dipole-orientation & depth models\nI feel like this exercise is already pretty long, so I will give you larger chunks of code.\n# plotting parameters\nbrain_kwargs = dict(views=\"lateral\",hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n\n# getting larger figures\nplt.rcParams['figure.dpi'] = 100 \nplt.rcParams[\"figure.figsize\"] = (20,15)\n\n# a list of all our inverse operators\nsourceList = [inv_default,inv_loose0,inv_loose1,inv_depth0]\n\n#loop over inverse operators\nfor (ix,inv) in enumerate(sourceList):\n\n    # calculate the stc\n    s = apply_inverse(epochs.average(), inv, lambda2, 'MNE', verbose=True)#, pick_ori='vector')\n    \n    # plot it\n    h = s.plot(**brain_kwargs)\n\n    # grab a png from it\n    img = ccs_eeg_utils.stc_plot2img(h,closeAfterwards=False)\n\n    # add the png to a subplot\n    ax = plt.subplot(2,2,ix+1).imshow(img)\n    plt.axis('off')\n\nimport sys\nsys.path.insert(0,\"..\")\n\nimport ccs_eeg_utils\nbrain_kwargs = dict(views='flat',surface=\"flat\", hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n# plotting parameters\nbrain_kwargs = dict(views=\"lateral\",hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n\n# getting larger figures\nplt.rcParams['figure.dpi'] = 100 \nplt.rcParams[\"figure.figsize\"] = (20,15)\n\n# a list of all our inverse operators\nsourceList = [inv_default,inv_loose0,inv_loose1,inv_depth0]\n\n#loop over inverse operators\nfor (ix,inv) in enumerate(sourceList):\n\n    # calculate the stc\n    s = apply_inverse(epochs.average(), inv, lambda2, 'MNE', verbose=True)#, pick_ori='vector')\n    \n    # plot it\n    h = s.plot(**brain_kwargs)\n\n    # grab a png from it\n    img = ccs_eeg_utils.stc_plot2img(h,closeAfterwards=False)\n\n    # add the png to a subplot\n    ax = plt.subplot(2,2,ix+1).imshow(img)\n    plt.axis('off')\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n[done]\nUsing control points [8.26314785e-12 9.82639882e-12 2.66900633e-11]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.1% variance\n[done]\nUsing control points [1.15861132e-11 1.38390192e-11 3.79632677e-11]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.6% variance\n    Combining the current components...\n[done]\nUsing control points [6.45174951e-12 7.36806058e-12 1.54805434e-11]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  47.9% variance\n    Combining the current components...\n[done]\nUsing control points [8.20140165e-12 1.00116981e-11 3.64245943e-11]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus-Q: Why did MNE suddenly decide to plot pos/neg for the loose=0 (fixed orientation) instead of abs?\n\nComparing algorithms\nNext we will compare the four different minimum-norm algorithms implemented in MNE: ['MNE','sLORETA','eLORETA','dSPM'] adapt the previous script to use inv_default for all of them and plot the solutions again.\n\n# plotting parameters\nbrain_kwargs = dict(views=\"lateral\",hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n\n# getting larger figures\nplt.rcParams['figure.dpi'] = 100 \nplt.rcParams[\"figure.figsize\"] = (20,15)\n\n# a list of all our inverse operators\n\n#loop over inverse operators\nfor (ix,method) in enumerate(['MNE','sLORETA','eLORETA','dSPM']):\n\n    # calculate the stc\n    s = apply_inverse(epochs.average(), inv_default, lambda2, method, verbose=True)#, pick_ori='vector')\n    \n    # plot it\n    h = s.plot(**brain_kwargs)\n\n    # grab a png from it\n    img = ccs_eeg_utils.stc_plot2img(h,closeAfterwards=False)\n\n    # add the png to a subplot\n    ax = plt.subplot(2,2,ix+1).imshow(img)\n    plt.axis('off')\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n[done]\nUsing control points [8.26314785e-12 9.82639882e-12 2.66900633e-11]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\n    Computing noise-normalization factors (sLORETA)...\n[done]\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n    sLORETA...\n[done]\nUsing control points [18.43577167 21.42066689 46.66754233]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\n    Computing optimized source covariance (eLORETA)...\n        Using independent orientation weights\n        Fitting up to 20 iterations (this make take a while)...\n        Converged on iteration 10 (4.8e-07 &lt; 1e-06)\n        Updating inverse with weighted eigen leads\n[done]\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads already weighted ... \n    Computing residual...\n    Explained  45.4% variance\n    Combining the current components...\n[done]\nUsing control points [7.39223674e-12 8.42494906e-12 1.71655980e-11]\n\n\n\n\n\n\n\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\n    Computing noise-normalization factors (dSPM)...\n[done]\nApplying inverse operator to \"0.50 √ó Face/A + 0.50 √ó Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n    dSPM...\n[done]\nUsing control points [34.68525843 40.1269434  92.54111535]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal exploration: Different regularisation\nAdapt the above script once more, and apply different SNRs (e.g.¬†[0.1,2,10,50]). What do you observe? Make note of the MNE text-output, it tells you how much % of your actual observed data is explained by the solution. Do they roughly match your expectations?\n\n# plotting parameters\nbrain_kwargs = dict(views=\"lateral\",hemi='split', size=(800, 400), initial_time=0.15,time_viewer=False,time_unit='s',show_traces=False)\n\n# getting larger figures\nplt.rcParams['figure.dpi'] = 100 \nplt.rcParams[\"figure.figsize\"] = (20,15)\n\n# a list of all our inverse operators\n\n#loop over inverse operators\nfor (ix,SNR) in enumerate([0.1,2,10,50]):\n    \n    lamb = 1.0 / SNR ** 2\n    # calculate the stc\n    s = apply_inverse(epochs.average(), inv_default, lamb, \"MNE\", verbose=True)#, pick_ori='vector')\n    \n    # plot it\n    h = s.plot(**brain_kwargs)\n\n    # grab a png from it\n    img = ccs_eeg_utils.stc_plot2img(h,closeAfterwards=False)\n\n    # add the png to a subplot\n    ax = plt.subplot(2,2,ix+1).imshow(img)\n    plt.axis('off')\n\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 * Face/A + 0.50 * Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained   0.3% variance\n    Combining the current components...\n[done]\nUsing control points [9.29470964e-15 1.14144336e-14 3.88214609e-14]\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 * Face/A + 0.50 * Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  10.8% variance\n    Combining the current components...\n[done]\nUsing control points [1.05253277e-12 1.22486818e-12 2.90830171e-12]\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 * Face/A + 0.50 * Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  46.4% variance\n    Combining the current components...\n[done]\nUsing control points [8.26314785e-12 9.82639882e-12 2.66900633e-11]\nPreparing the inverse operator for use...\n    Scaled noise and source covariance from nave = 1 to nave = 1072\n    Created the regularized inverter\n    Created an SSP operator (subspace dimension = 1)\n    Created the whitener using a noise covariance matrix with rank 120 (1 small eigenvalues omitted)\nApplying inverse operator to \"0.50 * Face/A + 0.50 * Face/B\"...\n    Picked 121 channels from the data\n    Computing inverse...\n    Eigenleads need to be weighted ...\n    Computing residual...\n    Explained  73.0% variance\n    Combining the current components...\n[done]\nUsing control points [3.06306472e-11 3.84585004e-11 1.42431687e-10]\n\n\n\n\n\n\n\n\n\n\n\n\nSome further notes:\n\nThere are no ideal parameters or algorithms. Different combinations of parameters will introduce different assumptions will lead to potentially very different solutions. One conclusion from our exploration with a single subject might be: Activity comes from visual cortex, but maybe this is something we knew before.\nSo far, we were projecting ERPs conditionwise to the source space. But you can also plot subtractions of conditions, highlighting where the effects occured. Further, you could even directly project betas or difference-waves to source space.\nIf you have multiple subjects, and you want to perform statistics, you need to match the head-models of subjects. Given that we do not have individual headmodels, this is no problem for us. All vertices / dipoles are at exactly the same position for all subjects - convenient!\nPerforming statistics over subjects works quite similarly (check out this tutorial) to previous analysis. In case of cluster-permutation tests, adjacency/neighbourhood needs to be defined in 3D space on the surface. Computation time might be increased enourmously.\nAll of our source-space analysis have been performed on the surface of the cortex. This is where the gray matter is, and where we hypothesize that our EEG signal is produced. But we might be wrong, there might be deep-sources, e.g.¬†hippocampus or cerebellum also generating our potentials. Thus instead of restricting our source analysis to the surface, we could also make use of the whole 3D volume of the brain. This is less well documented in MNE, but also possible. In some sense it is a question of preference.\nAgain my warning: Do not overinterpret your source analysis results! You might be centimeters away from your real activity."
  },
  {
    "objectID": "exercises/solutions/ex1_overview.html",
    "href": "exercises/solutions/ex1_overview.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "import sys\nsys.path.insert(0,'..')\nimport ccs_eeg_utils\n\n\nimport mne, mne_bids, ccs_eeg_utils\n\n#Load the data\nfrom mne_bids import (BIDSPath, read_raw_bids)\n\n#path where to save the datasets.\n#bids_root = \"../../local/bids\" # recommended data location\n#bids_root = \"/bigpool/export/users/ehinger/erp-core/bids\" # Bene's Server location\nbids_root = \"/store/data/erp-core/\"\nsubject_id = '030' # recommend subject 30 for now\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",session=\"P3\",\n                     datatype='eeg', suffix='eeg',\n                     root=bids_root)\n\n#read the file\nraw = read_raw_bids(bids_path)\n#fix the annotations readin\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\n\nReading /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_eeg.fdt\n\n\n/tmp/ipykernel_1145118/1487781420.py:18: RuntimeWarning: Data file name in EEG.data (sub-030_task-P3_eeg.fdt) is incorrect, the file name must have changed on disk, using the correct file name (sub-030_ses-P3_task-P3_eeg.fdt).\n  raw = read_raw_bids(bids_path)\n\n\nReading events from /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_events.tsv.\nThe event \"response\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/202\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\n    Renaming event: response -&gt; response/201\nThe event \"stimulus\" refers to multiple event values. Creating hierarchical event names.\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/31\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/34\n    Renaming event: stimulus -&gt; stimulus/33\n    Renaming event: stimulus -&gt; stimulus/32\n    Renaming event: stimulus -&gt; stimulus/35\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/24\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/21\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/25\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/22\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/23\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/15\n    Renaming event: stimulus -&gt; stimulus/11\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/14\n    Renaming event: stimulus -&gt; stimulus/13\n    Renaming event: stimulus -&gt; stimulus/12\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/55\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/52\n    Renaming event: stimulus -&gt; stimulus/54\n    Renaming event: stimulus -&gt; stimulus/51\n    Renaming event: stimulus -&gt; stimulus/53\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/45\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/41\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/44\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/43\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\n    Renaming event: stimulus -&gt; stimulus/42\nReading channel info from /store/data/erp-core/sub-030/ses-P3/eeg/sub-030_ses-P3_task-P3_channels.tsv.\nReading 0 ... 393215  =      0.000 ...   383.999 secs...\n\n\n\n    \n        Measurement date\n        \n        Unknown\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n            \n            sub-030\n            \n        \n    \n    \n        Digitized points\n        \n        33 points\n        \n    \n    \n        Good channels\n        30 EEG, 3 EOG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        HEOG_left, HEOG_right, VEOG_lower\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        1024.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        512.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        sub-030_ses-P3_task-P3_eeg.fdt\n    \n    \n    \n        Duration\n        00:06:24 (HH:MM:SS)\n    \n\n\n\nT: Extract a single channel and plot the whole timeseries. You can directly interact with the raw object, e.g.¬†raw[1:10,1:2000] extracts the first 10 channels and 2000 samples.\n\nfrom matplotlib import pyplot as plt\neeg, t = raw[10,:]\nplt.plot(eeg[0,0:1000])\n\n\nraw.plot(duration=5)\n\nUsing qt as 2D backend.\n\n\n&lt;mne_qt_browser._pg_figure.MNEQtBrowser at 0x7f6ccdfbc8b0&gt;\n\n\nChannels marked as bad:\nnone\n\n\nQ: What is the unit/scale of the data? 0.01V = 10000 ¬µV\nT: Have a look at raw.info and note down what the sampling frequency is (how many EEG-samples per second): 1024Hz\nT: We will epoch the data now. Formost we will cut the raw data to one channel using raw.pick([\"Cz\"]) - not that this will permanently change the ‚Äúraw‚Äù object and ‚Äúdeletes‚Äù alle other channels from memory. If you want rather a copy you could use raw_subselect = raw.copy().pick([\"Cz\"])).\n\nraw.info\n\n\nraw.pick([\"Oz\"])\nplt.plot(raw[:,:][0].T)\n\n\n\n\n\n\n\n\n\nevts,evts_dict = mne.events_from_annotations(raw)\nevts_dict\n\nUsed Annotations descriptions: ['response:201', 'response:202', 'stimulus:11', 'stimulus:12', 'stimulus:13', 'stimulus:14', 'stimulus:15', 'stimulus:21', 'stimulus:22', 'stimulus:23', 'stimulus:24', 'stimulus:25', 'stimulus:31', 'stimulus:32', 'stimulus:33', 'stimulus:34', 'stimulus:35', 'stimulus:41', 'stimulus:42', 'stimulus:43', 'stimulus:44', 'stimulus:45', 'stimulus:51', 'stimulus:52', 'stimulus:53', 'stimulus:54', 'stimulus:55']\n\n\n{'response:201': 1,\n 'response:202': 2,\n 'stimulus:11': 3,\n 'stimulus:12': 4,\n 'stimulus:13': 5,\n 'stimulus:14': 6,\n 'stimulus:15': 7,\n 'stimulus:21': 8,\n 'stimulus:22': 9,\n 'stimulus:23': 10,\n 'stimulus:24': 11,\n 'stimulus:25': 12,\n 'stimulus:31': 13,\n 'stimulus:32': 14,\n 'stimulus:33': 15,\n 'stimulus:34': 16,\n 'stimulus:35': 17,\n 'stimulus:41': 18,\n 'stimulus:42': 19,\n 'stimulus:43': 20,\n 'stimulus:44': 21,\n 'stimulus:45': 22,\n 'stimulus:51': 23,\n 'stimulus:52': 24,\n 'stimulus:53': 25,\n 'stimulus:54': 26,\n 'stimulus:55': 27}\n\n\n\nwanted_keys = [e for e in evts_dict.keys() if \"stimulus\" in e]\n\n\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\nevts_dict_stim\n\n{'stimulus:11': 3,\n 'stimulus:12': 4,\n 'stimulus:13': 5,\n 'stimulus:14': 6,\n 'stimulus:15': 7,\n 'stimulus:21': 8,\n 'stimulus:22': 9,\n 'stimulus:23': 10,\n 'stimulus:24': 11,\n 'stimulus:25': 12,\n 'stimulus:31': 13,\n 'stimulus:32': 14,\n 'stimulus:33': 15,\n 'stimulus:34': 16,\n 'stimulus:35': 17,\n 'stimulus:41': 18,\n 'stimulus:42': 19,\n 'stimulus:43': 20,\n 'stimulus:44': 21,\n 'stimulus:45': 22,\n 'stimulus:51': 23,\n 'stimulus:52': 24,\n 'stimulus:53': 25,\n 'stimulus:54': 26,\n 'stimulus:55': 27}\n\n\nT: MNE-speciality: We have to convert annotations to events\n\nimport mne\nevts,evts_dict = mne.events_from_annotations(raw)\nwanted_keys = [e for e in evts_dict.keys() if \"stimulus\" in e]\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\n\n\nepochs = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1)\n\nUsed Annotations descriptions: ['response:201', 'response:202', 'stimulus:11', 'stimulus:12', 'stimulus:13', 'stimulus:14', 'stimulus:15', 'stimulus:21', 'stimulus:22', 'stimulus:23', 'stimulus:24', 'stimulus:25', 'stimulus:31', 'stimulus:32', 'stimulus:33', 'stimulus:34', 'stimulus:35', 'stimulus:41', 'stimulus:42', 'stimulus:43', 'stimulus:44', 'stimulus:45', 'stimulus:51', 'stimulus:52', 'stimulus:53', 'stimulus:54', 'stimulus:55']\nNot setting metadata\n200 matching events found\nSetting baseline interval to [-0.099609375, 0.0] s\nApplying baseline correction (mode: mean)\n0 projection items activated\n\n\n\nepochs\n\n\n\n\n\n\n\n\n\nNumber of events\n200\n\n\nEvents\nstimulus:11: 8\nstimulus:12: 4\nstimulus:13: 10\nstimulus:14: 9\nstimulus:15: 9\nstimulus:21: 6\nstimulus:22: 8\nstimulus:23: 12\nstimulus:24: 3\nstimulus:25: 11\nstimulus:31: 8\nstimulus:32: 10\nstimulus:33: 8\nstimulus:34: 4\nstimulus:35: 10\nstimulus:41: 7\nstimulus:42: 12\nstimulus:43: 11\nstimulus:44: 8\nstimulus:45: 2\nstimulus:51: 8\nstimulus:52: 12\nstimulus:53: 4\nstimulus:54: 8\nstimulus:55: 8\n\n\nTime range\n-0.100 ‚Äì 1.000 s\n\n\nBaseline\n-0.100 ‚Äì 0.000 s\n\n\n\n\n\n\nT Now that we have the epochs we should plot them. Plot all trials ‚Äòmanually‚Äô, without using mne‚Äôs functionality (epochs.get_data()).\nQ What is the unit/scale of the data now?: 0.0001 V = 100¬µV\n\nimport numpy as np\n#plt.plot(np.squeeze(epochs.get_data()[:,0,:].T))\nplt.plot(np.squeeze(epochs.get_data()[:,0,:].T).mean(axis=1),color=\"black\")\n\nUsing data from preloaded Raw for 200 events and 1127 original time points ...\n\n\n\n\n\n\n\n\n\nNow index the epochs evoked = epochs[index].average() and average them. You can then plot them either via evoked.plot() or with mne.viz.plot_compare_evokeds([evokedA,evokedB]).\n\ntarget = epochs[[\"stimulus:{}{}\".format(k,k) for k in [1,2,3,4,5]]].average()\ndistractor = epochs[[\"stimulus:{}{}\".format(k,j) for k in [1,2,3,4,5] for j in [1,2,3,4,5] if k!=j]].average()\nmne.viz.plot_compare_evokeds([target,distractor]);\n\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n\n\n/tmp/ipykernel_1145118/86118832.py:3: RuntimeWarning: Cannot find channel coordinates in the supplied Evokeds. Not showing channel locations.\n  mne.viz.plot_compare_evokeds([target,distractor]);\n\n\n\n\n\n\n\n\n\n                                                                                            **Q** What is the unit/scale of the data now? Set it into context to the other two scales you reported (**Q**'s higher up). \n20¬µV. So we went from 20.000¬µV to 100¬µV to 20¬µV"
  },
  {
    "objectID": "exercises/solutions/ex3_cleaning.html",
    "href": "exercises/solutions/ex3_cleaning.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "import sys\nsys.path.insert(0, '..')\nfrom mne_bids import (BIDSPath,read_raw_bids)\nimport mne_bids\nimport importlib\nimport mne\nimport ccs_eeg_utils\n\n#ccs_eeg_utils.download_erpcore(task=\"P3\",subject=30,localpath=\"../local/bids/\")\n\nbids_root = \"../local/bids\"\nbids_root = \"/bigpool/export/users/ehinger/erp-core/bids\" # Bene's Server location\nbids_root = \"/store/data/erp-core/\"\nsubject_id = '030'\n\n\nbids_path = BIDSPath(subject=subject_id,task=\"P3\",\n                     datatype='eeg', suffix='eeg',session=\"P3\",\n                     root=bids_root)\n\nraw = read_raw_bids(bids_path)\nccs_eeg_utils.read_annotations_core(bids_path,raw)\nraw.load_data()\nraw.filter(0.5,50, fir_design='firwin')\nT: Go through the dataset using the MNE explorer and clean it. You can use raw.plot() for this. If you are working from a jupyter notebook, try to use %matplotlib qt for better support of the cleaning window. To get an understanding how the tool works, press help or type ? in the window. (Hint: You first have to add a new annotation by pressing a)\n%matplotlib qt\nraw.plot(n_channels=len(raw.ch_names))#,scalings =40e-6)\n# See below\nbad_ix = [i for i,a in enumerate(raw.annotations) if a['description']==\"BAD_\"]\n\n#raw.annotations[bad_ix].save(\"sub-{}_task-P3_badannotations.csv\".format(subject_id))\n\n\nannotations = mne.read_annotations(\"sub-{}_task-P3_badannotations.csv\".format(subject_id))\nraw.annotations.append(annotations.onset,annotations.duration,annotations.description)\nT: While going through the dataset, mark what you observe as bad electrodes. Those are saved in raw.info['bads']. The channels can be interpolated with raw.interpolate_bads() or epoch.interpolate_bads(). Compare the channel + neighbours before and after. Did the interpolation succeed? (If you are interested in the mathematical details of spline interpolation, checkout this https://mne.tools/dev/overview/implementation.html#id26) Hint: You need channel locations to run the interpolation which you can get by using the default-standardized channel locations raw.set_montage('standard_1020',match_case=False)\n# I thought there was not really a bad channel in this dataset, so I remove one at random.\nraw.info['bads'] = ['FP2']\n# for interpolation\nraw.set_montage('standard_1020',match_case=False)\nraw.interpolate_bads()\nT: In the epoching step, we can also specify rejection criterion for a peak-to-peak rejection method\n%matplotlib inline\nimport mne\nevts,evts_dict = mne.events_from_annotations(raw)\nwanted_keys = [e for e in evts_dict.keys() if \"stimulus\" in e]\nevts_dict_stim=dict((k, evts_dict[k]) for k in wanted_keys if k in evts_dict)\n\n# get epochs with and without rejection\nepochs        = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1,reject_by_annotation=False)\nepochs_manual = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1,reject_by_annotation=True)\nreject_criteria = dict(eeg=200e-6,       # 100 ¬µV # HAD TO INCREASE IT HERE, 100 was too harsh\n                       eog=200e-6)       # 200 ¬µV\nepochs_thresh = mne.Epochs(raw,evts,evts_dict_stim,tmin=-0.1,tmax=1,reject=reject_criteria,reject_by_annotation=False)\n\n#from matplotlib import pyplot as plt\n# compare\n#plt.plot([0,:])\nmne.viz.plot_compare_evokeds({'raw':epochs.average(),'clean':epochs_manual.average(),'thresh':epochs_thresh.average()},picks=\"Cz\")"
  },
  {
    "objectID": "exercises/solutions/ex3_cleaning.html#bonus-tasks",
    "href": "exercises/solutions/ex3_cleaning.html#bonus-tasks",
    "title": "üß† EEG-2024",
    "section": "Bonus Tasks!",
    "text": "Bonus Tasks!\n\nfrom autoreject import AutoReject\nar = AutoReject(verbose='tqdm')\nepochs.load_data()\nepochs_ar = ar.fit_transform(epochs)  \n\n\nr = ar.get_reject_log(epochs_ar)\n\n\nr.plot(orientation=\"horizontal\");\n\n\n\nmne.viz.plot_compare_evokeds({\n    'raw':epochs.average(),\n    'clean':epochs_manual.average(),\n    'ar':epochs_ar.average()\n    },picks=\"Cz\")\n\n\nfrom scipy.stats.mstats import winsorize\nimport numpy as np\ndef winsor(d):\n    return np.mean(winsorize(d,axis=0,limits=(0.2,0.2)),axis=0)\ndef median(d):\n    return np.median(d,axis=0)\n\nmne.viz.plot_compare_evokeds({\n    'clean':epochs_manual.average(),\n    'robust':epochs.load_data().average(method=winsor),\n    'median':epochs.load_data().average(method=median),\n    },picks=\"Cz\")\n\n\nmne.viz.plot_compare_evokeds({\n    'clean':epochs_manual.average(),\n    'robust':epochs.average(method=winsor),\n    'ar':epochs_ar.average()\n    },picks=\"Cz\")\n\n\n%matplotlib inline\nylim = dict(eeg=(-20, 20))\nepochs.average().plot(ylim=ylim, spatial_colors=True);\nepochs_ar.average().plot(ylim=ylim, spatial_colors=True);\nepochs_manual.average().plot(ylim=ylim, spatial_colors=True);"
  },
  {
    "objectID": "exercises/solutions/ex10_decoding.html",
    "href": "exercises/solutions/ex10_decoding.html",
    "title": "The EEG Motor Movement/Imagery Dataset",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport numpy as np\nimport scipy\n\nimport sys\nsys.path.insert(0,\"..\")\nimport matplotlib\nimport matplotlib.pylab as pl\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport sklearn.pipeline\nimport sklearn\nimport sklearn.model_selection\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport mne.decoding\n\nimport ccs_eeg_utils\nepochs = ccs_eeg_utils.get_classification_dataset(typeInt=4)\n\nloading subject 1 with runs [6, 10, 14]\nExtracting EDF parameters from /home/ehinger/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R06.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\nReading 0 ... 19999  =      0.000 ...   124.994 secs...\nExtracting EDF parameters from /home/ehinger/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R10.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\nReading 0 ... 19999  =      0.000 ...   124.994 secs...\nExtracting EDF parameters from /home/ehinger/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R14.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\nReading 0 ... 19999  =      0.000 ...   124.994 secs...\nFiltering raw data in 3 contiguous segments\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 265 samples (1.656 sec)\n\nUsed Annotations descriptions: ['T1', 'T2']\nNot setting metadata\n45 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 45 events and 801 original time points ...\n0 bad epochs dropped\nepochs\n\n\n\n\n\n\n\n\n\nNumber of events\n45\n\n\nEvents\nfeet: 24\nhands: 21\n\n\nTime range\n-1.000 ‚Äì 4.000 sec\n\n\nBaseline\noff\nInformation on the dataset at hand can be found here - a total of 109 subjects exist."
  },
  {
    "objectID": "exercises/solutions/ex10_decoding.html#extensions",
    "href": "exercises/solutions/ex10_decoding.html#extensions",
    "title": "The EEG Motor Movement/Imagery Dataset",
    "section": "Extensions",
    "text": "Extensions\nThis formulations with the pipelines etc. is neat, because we can simply exchange parts of the pipeline and see what happens. Note though, that you are performing multiple testing and are very easily overfitting to your dataset.\nThus in principle, you would like to explore some options with one set of subjects, then test the pipeline on another set of subjects. But this is a bit too involved for the homework.\n\nRemove CSP\nInstead of applying a feature selection, maybe we can learn from the ‚Äúraw‚Äù data? T: Replace the ‚Äúcsd‚Äù in the pipeline with mne.decoding.Vectorizer(), this will ensure the reshaping of the features we performed earlier.\nQ: What is the accuracy now?\n\npipe_simple = sklearn.pipeline.Pipeline([('vector',mne.decoding.Vectorizer()),('LDA', lda)])\ncv = sklearn.model_selection.StratifiedShuffleSplit(10, test_size=0.2, random_state=0)\nscores = sklearn.model_selection.cross_val_score(pipe_simple, epochs_train.get_data(), labels, cv=cv, n_jobs=1)\nprint(\"Accuracy: {:.1f}%, \".format(scores.mean()*100))\n\nAccuracy: 51.1%, \n\n\n\n\nClassify over time\nNext we will fit a classifier for each timepoint and test it on that timepoint (you have to use the simple pipeline, see below) For this the convenience function timeDecode = mne.decoding.SlidingEstimator(pipe) exists. This function performs the pipeline on the last dimension (which is currently time).\nBecause we will get multiple scores per cross-val, we also have to switch our scorer to mne.decoding.cross_val_multiscore(timeDecode,...).\nT: Plot the performance against time\nBonus T: You can also use mne.decoding.GeneralizingEstimator(...) to get the temporal decoding matrix (increased runtime warning)\n\ntimeDecode = mne.decoding.SlidingEstimator(pipe_simple)\nscores = mne.decoding.cross_val_multiscore(timeDecode, epochs.copy().resample(20).get_data(), labels, cv=cv,n_jobs=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot(epochs.copy().resample(20).times,scores.mean(axis=0))\nplt.hlines(0.5,-1,4,'k')\n\n\n\n\n\n\n\n\n\ntimeDecode = mne.decoding.GeneralizingEstimator(pipe_simple)\nscores = mne.decoding.cross_val_multiscore(timeDecode, epochs.copy().resample(20).get_data(), labels, cv=2,n_jobs=1) # resample speeds it up a lot! \n\n\n\n\n\n\n\n\n\n\n\n\n\nYou might be surprised - or not - by the performance you observed. Applying this to a dataset with actual evoked responses, will likely be much more satisfactory.\nThe reason seems clear, we would need CSP in between. But CSP needs multiple timepoints and doesntwork with SlidingEstimator. We have to do it manually.\n\nim = plt.imshow(scores.mean(axis=0), interpolation='lanczos', origin='lower', cmap='RdBu_r',vmin=0.2, vmax=0.8)\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nplt.plot(np.diag(scores.mean(axis=0)))\nplt.hlines(0.5,-1,100,'k')\n\n\n\n\n\n\n\n\n\nw_size = 0.7\ntimeVec = epochs.copy().resample(40).times\ntimeVec = timeVec[::10]\nt_scores = np.zeros(len(timeVec))\nfor t, w_time in enumerate(timeVec):\n        print(\"{}/{}\".format(t,len(timeVec)))\n        # Center the min and max of the window\n        w_tmin = w_time - w_size / 2.\n        w_tmax = w_time + w_size / 2.\n\n        # stop the program if the timewindow is outside of our epoch\n        if w_tmin &lt; timeVec[0]:\n            continue\n        if w_tmax &gt; timeVec[len(timeVec)-1]:\n            continue\n        # Crop data into time-window of interest\n        X = epochs.copy().resample(40).crop(w_tmin, w_tmax).get_data()\n\n        # Save mean scores over folds for each frequency and time window\n        t_scores[t] = np.mean( sklearn.model_selection.cross_val_score(estimator=pipe, X=X, y=labels,\n                                                     scoring='roc_auc', cv=2,\n                                                     n_jobs=1), axis=0)\n\n0/20\n1/20\n2/20\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n3/20\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n4/20\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.6e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n5/20\nComputing rank from data with rank=None\n    Using tolerance 2.9e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n6/20\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n7/20\nComputing rank from data with rank=None\n    Using tolerance 2.8e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 2.9e-05 (2.2e-16 eps * 64 dim * 2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n8/20\nComputing rank from data with rank=None\n    Using tolerance 2.8e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 2.9e-05 (2.2e-16 eps * 64 dim * 2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n9/20\nComputing rank from data with rank=None\n    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n10/20\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n11/20\nComputing rank from data with rank=None\n    Using tolerance 2.9e-05 (2.2e-16 eps * 64 dim * 2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 4.1e-05 (2.2e-16 eps * 64 dim * 2.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n12/20\nComputing rank from data with rank=None\n    Using tolerance 2.9e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 4e-05 (2.2e-16 eps * 64 dim * 2.8e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n13/20\nComputing rank from data with rank=None\n    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n14/20\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n15/20\nComputing rank from data with rank=None\n    Using tolerance 2.8e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n16/20\nComputing rank from data with rank=None\n    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n17/20\nComputing rank from data with rank=None\n    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\nComputing rank from data with rank=None\n    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n    Estimated rank (mag): 64\n    MAG: rank 64 computed from 64 data channels with 0 projectors\nReducing data rank from 64 -&gt; 64\nEstimating covariance using EMPIRICAL\nDone.\n18/20\n19/20\n\n\n\nplt.plot(timeVec,t_scores,'o-')\nplt.hlines(0.5,-1,4,'k')"
  },
  {
    "objectID": "exercises/solutions/ex7_encoding.html",
    "href": "exercises/solutions/ex7_encoding.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\nimport scipy\nfrom mne.datasets.limo import load_data\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pylab as pl\nimport sys\nsys.path.insert(0,\"..\")\nfrom ccs_eeg_utils import spline_matrix\n\n\n# choose any subject 1 - 18\nlimo_epochs = load_data(subject=3,path='../local/limo') #\n\nAdding metadata with 2 columns\nReplacing existing metadata with 2 columns\n1072 matching events found\nNo baseline correction applied\n0 projection items activated\n0 bad epochs dropped\nlimo_epochs.plot_sensors(show_names=True);\ny = limo_epochs.get_data(picks=\"B11\")\ny.shape\n\n(1072, 1, 201)\nX = np.ones([limo_epochs._data.shape[0],2])\nphase_coh = limo_epochs.metadata['phase-coherence']\n\nX[:,0] = 1\nX[:,1] =phase_coh\nprint(X.shape)\ny.shape\n\n(1072, 2)\n\n\n(1072, 1, 201)\nb,res,rnk,s = scipy.linalg.lstsq(X, y[:,0,:])\nb.shape\n\n(2, 201)\nplt.plot(limo_epochs.times,b.T)\nplt.legend([\"intercept\",\"phase-coherence\"])"
  },
  {
    "objectID": "exercises/solutions/ex7_encoding.html#compare-it-to-binned-data",
    "href": "exercises/solutions/ex7_encoding.html#compare-it-to-binned-data",
    "title": "üß† EEG-2024",
    "section": "Compare it to binned data",
    "text": "Compare it to binned data\n\nNext, generate one evoked.average() for each of the 18 stimulus coherence values.\nPlot them and compare them to your predictions. How well does a continuous predictor capture the data?\n\n\n#(partially taken from MNE tutorial: https://mne.tools/dev/auto_examples/datasets/plot_limo_data.html)\n# get levels of phase coherence\nlevels = sorted(phase_coh.unique())\n# create labels for levels of phase coherence (i.e., 0 - 85%)\nlabels = [\"{0:.2f}\".format(i) for i in np.arange(0., 0.90, 0.05)]\n\n# create dict of evokeds for each level of phase-coherence\nevokeds = {label: limo_epochs[phase_coh == level].average()\n           for level, label in zip(levels, labels)}\n\n\n# how many levels to evaluate\nn = 18\n# get unique phase-corr levels\nphase_corr_levels = np.sort(limo_epochs.metadata['phase-coherence'].unique())\n\n# build a designmatrix \n# we could put np.ones(n)*0 to remove the intercept\nXnew = np.vstack((np.ones(n),phase_corr_levels)).T\n\ncolors = plt.cm.magma(np.linspace(0,1,n))\n\n# This is the important bit!\ny_pred = Xnew@b\n\nfig,(ax1,ax2) = plt.subplots(1,2)\n# plot the predicted ERPs\nfor i in range(n):\n    ax1.plot(limo_epochs.times,y_pred[i,:] , color=colors[i])\n    plt.ylim(np.array([-1.5,1.2])*10**-5)\n\n# Plot the ERPs for each evoked object\n\nfor i,(k,v) in enumerate(evokeds.items()):\n    d = v.pick(['B11']).data # nt really sure why this is a function?!\n    ax2.plot(limo_epochs.times,d.T , color=colors[i],label=i)\n    plt.ylim(np.array([-1.5,1.2])*10**-5)"
  },
  {
    "objectID": "exercises/solutions/ex7_encoding.html#converting-a-continuous-predictor-to-a-spline-predictor",
    "href": "exercises/solutions/ex7_encoding.html#converting-a-continuous-predictor-to-a-spline-predictor",
    "title": "üß† EEG-2024",
    "section": "Converting a continuous predictor to a spline predictor",
    "text": "Converting a continuous predictor to a spline predictor\n\nFor this exercise we need from ccs_eeg_utils import spline_matrix function.\nThe function requires to specify where the spline bases should be evaluated x (the coherence value of each epoch) and where the knots should be. Think of the knots as the anchorpoints of the spline set. (Note: There is a huge literature on knot placement etc. we just use a linspace over the range of our continuous variable. If you want you can play around and e.g.¬†place more knots in the middle than the end). How many knots you ask? Choose a reasonable low number, you can change it later again. There are ways to decide this but they are outside the tutorial. Stay below 10.\nPlot the basisfunction as line-plots\nPlot the designmatrix as an 2D image\n\n\n\nfr = phase_corr_levels[0]\nto = phase_corr_levels[len(phase_corr_levels)-1]\n\nknotseq = np.linspace(fr,to,num=8) #np.linspace(0,0.90,num=8)\n\n# from where to where to evaluate the splines?\n# min-corr : max-corr\nx = np.linspace(fr,to,num=100)\n\n# run the ccs-provided function to generate splines\nXbs = spline_matrix(x,knots =knotseq)\nplt.plot(x,Xbs,'o-')\nplt.show()\nplt.imshow(Xbs,aspect=1./20,interpolation='none')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit the splines to your data (Note: If you wonder about the intercept, there are two ways here.Take the whole spline matrix and dont add an intercept {the intercept is implicit in the spline matrix}, or remove one spline and add an intercept {now all spline-predictors are relativ to the intercept}. Note that due to potential asymmetries in the spline basis set, the intercept is not necessarily what you would expect, see below)\n\n\nX_spl = spline_matrix(limo_epochs.metadata['phase-coherence'],knots =knotseq)\nb_spl,res,rnk,s = scipy.linalg.lstsq(X_spl, y[:,0,:])\n\n\nplt.plot(limo_epochs.metadata['phase-coherence'],X_spl,'o')\nplt.show()\nplt.imshow(X_spl,aspect=1./20,interpolation='none')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot the resulting ‚Äúraw‚Äù coefficients\n\n\nplt.plot(b_spl.T);\n\n\n\n\n\n\n\n\n\nThe ‚Äúraw‚Äù coefficients are hard to interpret. We should run conditional values again. Evaluate the fitted spline-betas at the respective 18 coherence levels and plot them.\n\n\n# get the designmatrix for only few corr-values\nXnew = spline_matrix(np.linspace(fr, to, num=18),knotseq)\n\n# get colors as well\ncolors = plt.cm.magma(np.linspace(0,1,18))\n\n# this is the \"magic\"\ny_pred_bs = Xnew@b_spl\n\n\n# Similar plot to before\nax = plt.subplots(1,3)[1]\n# plot the spline ERPs\nfor i in range(n):\n    ax[0].plot(limo_epochs.times,y_pred_bs[i,:] , color=colors[i])\n\n# plot the binned one\nfor i,(k,v) in enumerate(evokeds.items()):\n    d = v.pick(['B11']).data # nt really sure why this is a function?!\n    ax[1].plot(limo_epochs.times,d.T , color=colors[i])\n\n# plot the linear ERPs\nfor i in range(n):\n    ax[2].plot(limo_epochs.times,y_pred[i,:] , color=colors[i])\n\nplt.setp(ax,xlim=[-0.2,0.5],ylim=(np.array([-1.5,1.2])*10**-5));\n\n\n\n\n\n\n\n\n\nWe will add a categorical regressor to our spline designmatrix: The difference of FaceA vs.¬†FaceB. Note: If you havent remove a spline-column yet, please do so now and include an intercept.\nBecause it is a categorical predictor, we have to encode it using either dummy coding (e.g.¬†FaceA = 0, FaceB = 1) or effect coding (FaceA = -0.5, FaceB = 0.5 {you could also use -1 / 1})\nFit the model, and plot the faceA/faceB effect\n\n\n# Coh-Spline\nX_spl = spline_matrix(phase_coh,knots =knotseq)\n# intercept\nintercept = np.ones(X_spl.shape[0])\n# face-effect\nfaceB = (limo_epochs.metadata.face.values == \"B\")*2-1 # effect coded\n\n# get all of those\nX = np.vstack((intercept,faceB,X_spl[:,:-1].T))\n\n# fit it\nb,res,rnk,s = scipy.linalg.lstsq(X.T, y[:,0,:])\n\n\n# face-effect is the second beta\n\nplt.plot(limo_epochs.times,b[0:2,:].T)\nplt.legend([\"intercept\",\"face-diff\"])\n\n\n\n\n\n\n\n\n\nplt.plot(limo_epochs.times,b.T@np.matrix([[1, 1,0,0,0,0,0,0,0],\n                                          [1,-1,0,0,0,0,0,0,0]] ).T)\n\n\n\n\n\n\n\n\n\nIn order to facilitate calculating such effects, often a contrast-vector (or matrix) is used. We have a vector \\(b\\) that in our case represents \\([b_{intercept},b_{faceB},b_{spline1}, \\dots, b_{splineN}]\\). We typically want to use a sum of predictors, ie. \\(b_{intercept}\\) + \\(b_{faceB}\\) which can be represented as a matrix multiplication of \\(b*c\\) with \\(c = [1,1,0,\\dots,0]\\). If we chose effect coding before, we would need to put in the effect coded coefficients in our contrast-vector. I.e. \\(c = [1,-0.5,0,\\dots,0]\\) or \\(c = [1,-1,0,\\dots,0]\\). And finally, we could also combine multiple contrast vectors to one matrix to evaluate multiple effects of interest.\nUse a contrast matrix to plot the FaceA and FaceB ERP. Note that in difference to the example contrats-vector above, we now also want to add the correct spline-values. For this evaluate the spline-coefficients at noise-coherence of 0.9 and replace the 0‚Äôs of your \\(c_{spline1} \\dots c_{splineN}\\) contrast vector.\n\n\nknotseq\n\narray([-1.6209372 , -1.15515417, -0.68937115, -0.22358812,  0.24219491,\n        0.70797793,  1.17376096,  1.63954398])\n\n\n\n#  the calculation above slightly wrong, because I am putting all splines simply to 0. Because I removed the last spline, the implicit intercept relates to the lowest noiselevel. But due to the overlap of the spline-basis functions,  some spline-basis-betas should be non-zero. But which? Let's find out\nprint(spline_matrix([0],knots =knotseq))\n\n[[0.         0.         0.01757333 0.49740426 0.47119613 0.01382628\n  0.         0.        ]]\n\n\n\nprint(spline_matrix([1.6],knots =knotseq))\n\n[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n  2.54964056e-05 3.49333873e-03 1.18498707e-01 8.77982458e-01]]\n\n\n\n# I am making it explicit here which numbers I choose instead of concatenating\nplt.plot(limo_epochs.times,b.T@np.matrix([[1, 1, 0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815],\n                                          [1,-1,0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815]] ).T)\n\n\n\n\n\n\n\n\n\n# How much difference did it make? \nplt.plot(limo_epochs.times,b.T@np.matrix([[1,1, 0,0,0,0,0,0,0]] ).T)\nplt.plot(limo_epochs.times,b.T@np.matrix([[1,1, 0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815]] ).T)"
  },
  {
    "objectID": "exercises/solutions/ex7_encoding.html#bonus",
    "href": "exercises/solutions/ex7_encoding.html#bonus",
    "title": "üß† EEG-2024",
    "section": "Bonus:",
    "text": "Bonus:\n\nThere could be an interaction between the spline set and the categorical variable. For this, we have to calculate all pairwise multiplications of the faceB-column, with the spline columns.\nFit the resulting model and check whether you found an interaction effect\n\n\nX = np.vstack((intercept,faceB,X_spl[:,:-1].T))  # generate designmatrix \nX_interact = (X[1,:].reshape(X.shape[1],1) * X[2:,:].T).T # generate interaction matrix\n\nb_interact,res,rnk,s = scipy.linalg.lstsq(np.vstack((X,X_interact)).T, y[:,0,:]) # fit both combined\n\n\nplt.subplot(2,1,1).plot(limo_epochs.times,b.T@np.matrix([[1,1, 0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815],\n                                                         [1,-1,0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815]] ).T)\nplt.subplot(2,1,2).plot(b_interact.T@np.matrix([\n    [1,1, 0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815, 0,0,0,  0.16677456,  0.53777498,   0.28669231,  0.00875815],\n    [1,-1,0,0,0, 0.16677456, 0.53777498,  0.28669231, 0.00875815, 0,0,0, -0.16677456, -0.53777498,  -0.28669231, -0.00875815]] ).T) # note how we multiplied the second term with -1, identical to the c_FaceB term in the beginning of each contrast\n\n\n    # Also note: We are not doing any statistics here!"
  },
  {
    "objectID": "exercises/solutions/ex7_encoding.html#fitting-a-model-in-mne",
    "href": "exercises/solutions/ex7_encoding.html#fitting-a-model-in-mne",
    "title": "üß† EEG-2024",
    "section": "Fitting a model in MNE",
    "text": "Fitting a model in MNE\nTypically, one wouldnt fit the models with scipy manually, but one can use MNE to handle the fits (or even dedicated toolboxes like unfold* - but I am not aware of a documented toolbox in python). This also gives you much more support, i.e.¬†gives you standard errors(=SE),and through t = effect/SE, the t-values which can be easily looked up in the analytical H0 distribution and converted to single subject p-values.\n\nimport from mne.stats import linear_regression\nFit a model y~1 + faceB + coherence, thus a model with linear coherence effect: mne also requires to specify your own designmatrix, you can also give the columns names using res = linear_regression(...,names=['Intercept','effectB' ...]).\n\n\npredictor_vars = ['face B - face A', 'phase-coherence', 'intercept']\n\n# create design matrix data-frame\ndesign = limo_epochs.metadata[['phase-coherence', 'face']].copy()\ndesign['intercept'] = 1\ndesign['face B - face A'] = (limo_epochs.metadata['face'] == 'B')*2-1\n\ndesign = design[predictor_vars]\n\nfrom mne.stats import linear_regression\nreg = linear_regression(limo_epochs,\n                        design_matrix=design,\n                        names=predictor_vars)\n\ndesign\n\nFitting linear model to epochs, (26532 targets, 3 regressors)\n\n\n/tmp/ipykernel_317829/2779453045.py:11: RuntimeWarning: Fitting linear model to non-data or bad channels. Check picking\n  reg = linear_regression(limo_epochs,\n\n\nDone\n\n\n\n\n\n\n\n\n\nface B - face A\nphase-coherence\nintercept\n\n\n\n\n0\n-1\n1.255958\n1\n\n\n1\n-1\n0.296993\n1\n\n\n2\n-1\n-0.086593\n1\n\n\n3\n-1\n-1.429144\n1\n\n\n4\n-1\n-0.853765\n1\n\n\n...\n...\n...\n...\n\n\n1067\n1\n-0.086593\n1\n\n\n1068\n1\n0.105200\n1\n\n\n1069\n1\n-0.853765\n1\n\n\n1070\n1\n-0.470179\n1\n\n\n1071\n1\n0.105200\n1\n\n\n\n\n1072 rows √ó 3 columns\n\n\n\n\nYou can access the terms using res[‚ÄòIntercept‚Äô], and plot e.g.¬†the betas using res['Intecept'].beta.plot()\n\n\nreg['intercept']\n\nlm(beta=&lt;Evoked | '' (average, N=1), -0.29994 ‚Äì 0.49991 sec, baseline off, 132 ch, ~381 kB&gt;, stderr=&lt;Evoked | '' (average, N=1), -0.29994 ‚Äì 0.49991 sec, baseline off, 132 ch, ~381 kB&gt;, t_val=&lt;Evoked | '' (average, N=1), -0.29994 ‚Äì 0.49991 sec, baseline off, 132 ch, ~381 kB&gt;, p_val=&lt;Evoked | '' (average, N=1), -0.29994 ‚Äì 0.49991 sec, baseline off, 132 ch, ~381 kB&gt;, mlog10_p_val=&lt;Evoked | '' (average, N=1), -0.29994 ‚Äì 0.49991 sec, baseline off, 132 ch, ~381 kB&gt;)\n\n\n\nreg['intercept'].beta.plot();\nreg['phase-coherence'].beta.plot();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause we have all channels now, we can also plot topoplots. Think about how you could visualize the splines as topoplots (note: this is a hard question)\n\n\nreg['intercept'].beta.plot_topomap(times = np.arange(-0.3,0.5,0.1))\nreg['phase-coherence'].beta.plot_topomap(times = np.arange(-0.3,0.5,0.1));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also plot the p-values using res[‚Äòphase_coherence‚Äô].p_val.plot(). Do they follow your intuition?\n\n\nreg['phase-coherence'].p_val.plot();\n\n\n\n\n\n\n\n\n\nBonus: What does the distribution (histogram) of p-values of phase-coherence look like during the baseline? (note, bad channels are not automatically removed in MNE, thus you might want to increase your bin-width to spot them\n\n\npH0 = reg['phase-coherence'].p_val.copy().crop(tmax=0);\n\n\npH0.data.shape\n\n(132, 76)\n\n\n\nplData = pH0.data[np.where(np.sum(pH0.data &gt; 0.99,axis=1) &lt; 25),:]\nplt.hist(plData.reshape(np.prod(plData.shape)),bins=100);\n\n\n\n\n\n\n\n\n\nnp.mean(plData.reshape(np.prod(plData.shape)) &lt; 0.05)\n\n0.040736842105263155\n\n\n\n# the channels with the many 1's are the bad channels\nnp.where(np.sum(pH0.data &gt; 0.99,axis=1) &gt; 25)\n\n(array([ 12,  26,  40,  71,  80,  85, 116]),)"
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html",
    "href": "exercises/ex8_clusterPerm.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "Use data = ccs_eeg_utils.ex8_simulateData() to simulate a simple difference between two condition. Instead of electrodes x time, we now have a simple rectangular matrix (or \\(n_{subject}=15\\) of them), but the principles of multiple comparison corrections, can be applied to it as well.\nT: calculate the mean of the data over subjects and plot it as an image\nT: Plot some subjects individually. Can you infer anything from the ‚Äúsingle subject‚Äù displays?\n\n\nThe t-value is a weighting of your effect-strength (i.e.¬†the difference between the means of two conditions) and the standard deviation (=spread) of this effect. \\(t = \\frac{mean(x)}{\\frac{std(x)}{\\sqrt(n-1)}}\\). Use this formula to manually compare the t-values over the n=15 subjects. Compare it with scipy.stats.ttest_1samp(data,popmean=0,axis=0).\nT: The scipy function also returns p-values. We want to plot the t-values next to the p-values. Because we will iteratively add different multiple comparison corrections, it is best to generate a function that allows to subsequently increase the amount of plots one can plot. I recommend to plot the log10(p-values)\nBonus: You can also use m = np.ma.masked_where(pvalues&gt;0.05,tvalues) to get a nice masked view of the data.\n\n\n\n\nWe will investigate False Discovery Rate a bit. First let‚Äôs generate data without any effect and only with noise. Thus all possible findings with p&lt;0.05 have to be false positives\nT: run data_h0 = ccs_eeg_utils.ex8_simulateData(signal_mean=0,noise_between=0) and plot the t-values as an imshow (be sure to add a colorbar always)\nT: plot a histogram of all p-values (regardless of position)\nT: Also plot a histogram of the p-values of the data with the effect\nT: Count how many pvalues are below 0.05 each from the data with and without effect. For FDR we have to estimate how many ‚Äúsignificant‚Äù (=&gt; pvalue&lt;\\(\\alpha\\), with \\(alpha=0.05\\) typically) values we would get by chance (=false positives). Instead of estimating the number of p-values from one dataset (which is much more involved), we can also take our null-model pvalue-count as well. Calculate the ratio of H0/H1 significant-pvalues. This is your False-Discovery rate. Can you manually adjust alpha, so that the FDR is 0.05?\nT: Use mne.stats.fdr_correction to calculate the proper fdr-correction. Use the plotting function from the beginning to directly compare the p-values with and without correction.\n\n\n\nIn the lecture we discussed permutation tests and permutation cluster tests. What we didnt discuss is that you can easily adjust a permutation test to correct for multiple comparisons. We permute each 40x40 grid element concurrently, but instead of saving for each grid element the permuted statistics (e.g.¬†the t-value), we save the maximum of all grid elements. This biases our permutation distribution towards large t-values, and concurrently makes it harder for the observed value to ‚Äústand out‚Äù (=&gt; be unlikely) from that distribution. You can use mne.stats.permutation_t_test to calculate this.\n\n\n\nWe will implement a simple cluster permutation test, before making use of the mne-implementation. For this we need the package scikit-image to be able to use skimage.measure.label to get the clusters.\nA cluster permutation test has the following structure\n\ncalculate t_obs, the t-values for your observed data (as before mne.stats.ttest_1samp_no_p)\nThreshold t_obs using scipy.stats.t.ppf(1-(2*alpha), n-1) as the threshold value. This converts a p-value back to the t-value. In principle you could also decide to use a t-theshold of e.g.¬†2. The threshold is arbitrarily set, but important.\nBecause our cluster are in image-space, neighbours can easily be calculated using skimage.measure.label\nFind the largest cluster and save it to c_obs\nPermutation, do 1000 times:\n\ngenerate a signFlip vector with length n (by default n=15 subjects) consisting of random ‚Äú1‚Äù and ‚Äú-1‚Äù, one for each subject. Assuming the \\(H_0\\) is true (which we do in this loop), the sign around ‚Äú0‚Äù is random for each subject, so no harm should be done in flipping it (it will change the resulting statistic obviously, but doing it 1000 time shoudnt introuce / hide an effect)\nMultiply the signFlip vector to the data\nrepeat step 1-4 of the observed data for the permuted data to get c_perm\nSave this largest clustersize\n\nAppend c_obs to your c_perm (the simplified reason is, that else you could get a p-value of 0 more details if of interest)\n1 - np.mean(c_obs&gt;=c_perms) gives you your p-value\n\nNote: The test could be improved by e.g.¬†summing the t-values of a cluster instead of merely counting the cluster-extend, but that leads us a bit astray from what we want to understand here.\nT: Running a simular permutation test in MNE is much easier:\nt_clust, clusters, p_values, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, threshold=threshold, adjacency=None,\n    n_permutations=1000, out_type='mask')\n\nThe threshold is the same threshold you used before\nTypically we would have to supply the adjacency manually, because the adjancy depends on which channels are next to eachother. But in this case we can put None and mne will assume it is a grid-structure\n\nIn order to fill the clusters with their respective p-values:\np_clust = np.ones(data.shape[1:])\nfor cl, p in zip(clusters, p_values):\n    p_clust[cl] = p\nThis step is controversial, because clusters do not have any real p-value see here, ‚Äúinterpretation of significant TFCE value‚Äù. But pragmatically, I think it is still useful to gauge the Signal-To-Noise ratio of the clusters. As long as you do not literally interpret the p-value as a probability, you should be fine.\nT: Add the cluster-permutation to your comparison plot\n\n\n\nlast but not least, we will get rid of this initial cluster-formung threshold. TFCE integrates over all possible thresholds. We will not implement TFCE here, but simply call the mne-python function.\nt_tfce, _, p_tfce, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, adjacency=None,threshold = dict(start=0, step=0.2),\n    n_permutations=1000, out_type='mask')\nT: Add this to your comparison plot. We are done! I hope you learned the differences and underlying algorithms of several multiple-comparison corrects!"
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html#multiple-comparison-corrections",
    "href": "exercises/ex8_clusterPerm.html#multiple-comparison-corrections",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "Use data = ccs_eeg_utils.ex8_simulateData() to simulate a simple difference between two condition. Instead of electrodes x time, we now have a simple rectangular matrix (or \\(n_{subject}=15\\) of them), but the principles of multiple comparison corrections, can be applied to it as well.\nT: calculate the mean of the data over subjects and plot it as an image\nT: Plot some subjects individually. Can you infer anything from the ‚Äúsingle subject‚Äù displays?\n\n\nThe t-value is a weighting of your effect-strength (i.e.¬†the difference between the means of two conditions) and the standard deviation (=spread) of this effect. \\(t = \\frac{mean(x)}{\\frac{std(x)}{\\sqrt(n-1)}}\\). Use this formula to manually compare the t-values over the n=15 subjects. Compare it with scipy.stats.ttest_1samp(data,popmean=0,axis=0).\nT: The scipy function also returns p-values. We want to plot the t-values next to the p-values. Because we will iteratively add different multiple comparison corrections, it is best to generate a function that allows to subsequently increase the amount of plots one can plot. I recommend to plot the log10(p-values)\nBonus: You can also use m = np.ma.masked_where(pvalues&gt;0.05,tvalues) to get a nice masked view of the data."
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html#false-discovery-rate",
    "href": "exercises/ex8_clusterPerm.html#false-discovery-rate",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "We will investigate False Discovery Rate a bit. First let‚Äôs generate data without any effect and only with noise. Thus all possible findings with p&lt;0.05 have to be false positives\nT: run data_h0 = ccs_eeg_utils.ex8_simulateData(signal_mean=0,noise_between=0) and plot the t-values as an imshow (be sure to add a colorbar always)\nT: plot a histogram of all p-values (regardless of position)\nT: Also plot a histogram of the p-values of the data with the effect\nT: Count how many pvalues are below 0.05 each from the data with and without effect. For FDR we have to estimate how many ‚Äúsignificant‚Äù (=&gt; pvalue&lt;\\(\\alpha\\), with \\(alpha=0.05\\) typically) values we would get by chance (=false positives). Instead of estimating the number of p-values from one dataset (which is much more involved), we can also take our null-model pvalue-count as well. Calculate the ratio of H0/H1 significant-pvalues. This is your False-Discovery rate. Can you manually adjust alpha, so that the FDR is 0.05?\nT: Use mne.stats.fdr_correction to calculate the proper fdr-correction. Use the plotting function from the beginning to directly compare the p-values with and without correction."
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html#bonus-f-max-permutation-test",
    "href": "exercises/ex8_clusterPerm.html#bonus-f-max-permutation-test",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "In the lecture we discussed permutation tests and permutation cluster tests. What we didnt discuss is that you can easily adjust a permutation test to correct for multiple comparisons. We permute each 40x40 grid element concurrently, but instead of saving for each grid element the permuted statistics (e.g.¬†the t-value), we save the maximum of all grid elements. This biases our permutation distribution towards large t-values, and concurrently makes it harder for the observed value to ‚Äústand out‚Äù (=&gt; be unlikely) from that distribution. You can use mne.stats.permutation_t_test to calculate this."
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html#cluster-permutation-tests",
    "href": "exercises/ex8_clusterPerm.html#cluster-permutation-tests",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "We will implement a simple cluster permutation test, before making use of the mne-implementation. For this we need the package scikit-image to be able to use skimage.measure.label to get the clusters.\nA cluster permutation test has the following structure\n\ncalculate t_obs, the t-values for your observed data (as before mne.stats.ttest_1samp_no_p)\nThreshold t_obs using scipy.stats.t.ppf(1-(2*alpha), n-1) as the threshold value. This converts a p-value back to the t-value. In principle you could also decide to use a t-theshold of e.g.¬†2. The threshold is arbitrarily set, but important.\nBecause our cluster are in image-space, neighbours can easily be calculated using skimage.measure.label\nFind the largest cluster and save it to c_obs\nPermutation, do 1000 times:\n\ngenerate a signFlip vector with length n (by default n=15 subjects) consisting of random ‚Äú1‚Äù and ‚Äú-1‚Äù, one for each subject. Assuming the \\(H_0\\) is true (which we do in this loop), the sign around ‚Äú0‚Äù is random for each subject, so no harm should be done in flipping it (it will change the resulting statistic obviously, but doing it 1000 time shoudnt introuce / hide an effect)\nMultiply the signFlip vector to the data\nrepeat step 1-4 of the observed data for the permuted data to get c_perm\nSave this largest clustersize\n\nAppend c_obs to your c_perm (the simplified reason is, that else you could get a p-value of 0 more details if of interest)\n1 - np.mean(c_obs&gt;=c_perms) gives you your p-value\n\nNote: The test could be improved by e.g.¬†summing the t-values of a cluster instead of merely counting the cluster-extend, but that leads us a bit astray from what we want to understand here.\nT: Running a simular permutation test in MNE is much easier:\nt_clust, clusters, p_values, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, threshold=threshold, adjacency=None,\n    n_permutations=1000, out_type='mask')\n\nThe threshold is the same threshold you used before\nTypically we would have to supply the adjacency manually, because the adjancy depends on which channels are next to eachother. But in this case we can put None and mne will assume it is a grid-structure\n\nIn order to fill the clusters with their respective p-values:\np_clust = np.ones(data.shape[1:])\nfor cl, p in zip(clusters, p_values):\n    p_clust[cl] = p\nThis step is controversial, because clusters do not have any real p-value see here, ‚Äúinterpretation of significant TFCE value‚Äù. But pragmatically, I think it is still useful to gauge the Signal-To-Noise ratio of the clusters. As long as you do not literally interpret the p-value as a probability, you should be fine.\nT: Add the cluster-permutation to your comparison plot"
  },
  {
    "objectID": "exercises/ex8_clusterPerm.html#tfce",
    "href": "exercises/ex8_clusterPerm.html#tfce",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 8]",
    "section": "",
    "text": "last but not least, we will get rid of this initial cluster-formung threshold. TFCE integrates over all possible thresholds. We will not implement TFCE here, but simply call the mne-python function.\nt_tfce, _, p_tfce, H0 = mne.stats.permutation_cluster_1samp_test(\n    data, adjacency=None,threshold = dict(start=0, step=0.2),\n    n_permutations=1000, out_type='mask')\nT: Add this to your comparison plot. We are done! I hope you learned the differences and underlying algorithms of several multiple-comparison corrects!"
  },
  {
    "objectID": "exercises/ex9_tf.html",
    "href": "exercises/ex9_tf.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "In this exercise we will start with running STFT‚Äôs semi-manually using the scipy implementation. Then we will explore TimeFrequency utilities of the MNE toolbox\n\n\nGet the first signal using sig = ccs_eeg_utils.simulate_TF(signal=1). For now we will analyze a single trial, typically you‚Äôd run the STFT on each run and then average, as we will do later. The sampling rate is 500Hz\nT: Plot the signal, it is hard to recognize anything!\nT: Apply the stft using scipy.signal.stft . Systematically vary the nperseg from 32 to 256 in ~5 steps. Choose an noverlap of 0.9*nperseg.\nT: Plot the resulting STFTs as a colorcoded 2D plot. If you use plt.imshow, be sure to use interpolation='None'. Furthermore, aspect=\"auto\" may be helpful to fixate the sizes of the plot axes and origin='lower' will prevent the y-axis from being inverted.\nQ: What do you observe? Which nperseg is ideal for frequency-resolution, which for time-resolution? Do the oscillations overlap in time?\nT: Change the noverlap to 0.3*n. What changes?\n\n\n\nAs discussed in the lecture, sometimes you want high temporal resolution in high frequencies and low temporal resolution in low frequencies. Wavelet transform is one way to achieve that.\nT: Run mne.time_frequency.tfr_array_morlet(sig.reshape(1,1,-1), sfreq=500,freqs=freqs, n_cycles=cyc) with an appropriate set of freqs. Vary the amount of cycles (e.g [3,4,8,16]). Plot the results.\nQ: Do the expected results occur?\nBonus T: You can also run the same scripts on other signals, e.g.¬†sig = ccs_eeg_utils.simulate_TF(signal=2) or signal =3. Do your conclusions hold?\n\n\n\nGet a partially preprocessed P3 Dataset from the ERP-Core:\nepochs = ccs_eeg_utils.get_TF_dataset(subject_id = '002',bids_root = \"../local/bids\")\nWe are looking only at epochs relative to the responses, which have been already extracted by the helper function.\nWe want to get an overview of the power. So for starters we choose bad frequency resolution but good time resolution with a wavelet transform.\nWe will evaluate the TF at freqs = np.logspace(*np.log10([5, 80]), num=25) with 1 cycle.\nT:Run:\npower_total = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=4,average=True) # ITC, inter-trial-coherence is quite similar to evoked power, it is a measure of phase consistency over trials but we havent discussed it in the lecture.\nNext we will visualize all TFs at all electrodes and get an overview whats going on. If you use %matplotlib qt the plot will be interactive. Choose a baseline of -.5 to 0 using the command power.plot_topo()\nT: Plot the power at electrode Cz using power.plot(): 1. Without any baseline 2. With a baseline of your choice\nT: Explain the general pattern you see. Can you spot differences betweem with and withot baseline?\nT: Now lets improve upon our frequency resolution nz increasing the number of cycles to 3. Plot channel Cz with BSL correction (-0.5,0). Tipp: you can speed up the calculation by specifying picks=\"Cz\"\nT: We also want to calculate the induced and evoked TF. For this we first calculate the induced spectrum, then subtract the total from the induced.\n\nepochs.subtract_evoked() is a function that removes the ERP from each trial. It is a mne-consistent function that practically does: epochs_induced._data = epochs._data  - epochs.average().data. Run the tfr analysis again on the induced dataset, remember that if you dont make a copy of your epochs (via epochs.copy()) the dataset will be overwritten in memory\nIn order to get epochs_evoked, we have to subtract total and induced. We cand do this via\n\npower_evoked = mne.combine_evoked([power_total,power_induced],weights=[1,-1])\nNote: you could use this function also to combine/subtract condition effects!\nT: Visualize evoked, total & induced for electrode Cz"
  },
  {
    "objectID": "exercises/ex9_tf.html#time-frequency-analysis",
    "href": "exercises/ex9_tf.html#time-frequency-analysis",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "Get the first signal using sig = ccs_eeg_utils.simulate_TF(signal=1). For now we will analyze a single trial, typically you‚Äôd run the STFT on each run and then average, as we will do later. The sampling rate is 500Hz\nT: Plot the signal, it is hard to recognize anything!\nT: Apply the stft using scipy.signal.stft . Systematically vary the nperseg from 32 to 256 in ~5 steps. Choose an noverlap of 0.9*nperseg.\nT: Plot the resulting STFTs as a colorcoded 2D plot. If you use plt.imshow, be sure to use interpolation='None'. Furthermore, aspect=\"auto\" may be helpful to fixate the sizes of the plot axes and origin='lower' will prevent the y-axis from being inverted.\nQ: What do you observe? Which nperseg is ideal for frequency-resolution, which for time-resolution? Do the oscillations overlap in time?\nT: Change the noverlap to 0.3*n. What changes?"
  },
  {
    "objectID": "exercises/ex9_tf.html#morlet-wavelet-analysis",
    "href": "exercises/ex9_tf.html#morlet-wavelet-analysis",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "As discussed in the lecture, sometimes you want high temporal resolution in high frequencies and low temporal resolution in low frequencies. Wavelet transform is one way to achieve that.\nT: Run mne.time_frequency.tfr_array_morlet(sig.reshape(1,1,-1), sfreq=500,freqs=freqs, n_cycles=cyc) with an appropriate set of freqs. Vary the amount of cycles (e.g [3,4,8,16]). Plot the results.\nQ: Do the expected results occur?\nBonus T: You can also run the same scripts on other signals, e.g.¬†sig = ccs_eeg_utils.simulate_TF(signal=2) or signal =3. Do your conclusions hold?"
  },
  {
    "objectID": "exercises/ex9_tf.html#time-frequency-of-an-eeg-dataset",
    "href": "exercises/ex9_tf.html#time-frequency-of-an-eeg-dataset",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 9]",
    "section": "",
    "text": "Get a partially preprocessed P3 Dataset from the ERP-Core:\nepochs = ccs_eeg_utils.get_TF_dataset(subject_id = '002',bids_root = \"../local/bids\")\nWe are looking only at epochs relative to the responses, which have been already extracted by the helper function.\nWe want to get an overview of the power. So for starters we choose bad frequency resolution but good time resolution with a wavelet transform.\nWe will evaluate the TF at freqs = np.logspace(*np.log10([5, 80]), num=25) with 1 cycle.\nT:Run:\npower_total = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False,n_jobs=4,average=True) # ITC, inter-trial-coherence is quite similar to evoked power, it is a measure of phase consistency over trials but we havent discussed it in the lecture.\nNext we will visualize all TFs at all electrodes and get an overview whats going on. If you use %matplotlib qt the plot will be interactive. Choose a baseline of -.5 to 0 using the command power.plot_topo()\nT: Plot the power at electrode Cz using power.plot(): 1. Without any baseline 2. With a baseline of your choice\nT: Explain the general pattern you see. Can you spot differences betweem with and withot baseline?\nT: Now lets improve upon our frequency resolution nz increasing the number of cycles to 3. Plot channel Cz with BSL correction (-0.5,0). Tipp: you can speed up the calculation by specifying picks=\"Cz\"\nT: We also want to calculate the induced and evoked TF. For this we first calculate the induced spectrum, then subtract the total from the induced.\n\nepochs.subtract_evoked() is a function that removes the ERP from each trial. It is a mne-consistent function that practically does: epochs_induced._data = epochs._data  - epochs.average().data. Run the tfr analysis again on the induced dataset, remember that if you dont make a copy of your epochs (via epochs.copy()) the dataset will be overwritten in memory\nIn order to get epochs_evoked, we have to subtract total and induced. We cand do this via\n\npower_evoked = mne.combine_evoked([power_total,power_induced],weights=[1,-1])\nNote: you could use this function also to combine/subtract condition effects!\nT: Visualize evoked, total & induced for electrode Cz"
  },
  {
    "objectID": "exercises/ex7_encoding.html",
    "href": "exercises/ex7_encoding.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "More on the dataset or the original studies. In short: Faces were shown with varying noise added to them (phase-randomization in 2D-fourier space). An example: \nWe will build a designmatrix to manually apply a regression model to all timepoints. Then we will do the same thing within MNE!\nfrom mne.datasets.limo import load_data\n# choose any subject 1 - 18\nlimo_epochs = load_data(subject=4,path='../local/limo') #\n\nExtract data from electrode B11, this will be your ‚Äòy‚Äô\nThe linear covariate is saved in a dataframe called limo_epochs.metadata. Metadata is yet another way that you can save event-related information besides annotations & events\nGenerate a (design) matrix consisting of an intercet and a linear predictor of phase-coherence.\nUse scipy.linalg.lstsq or similar functions to solve the LeastSquares Problem at each time-point of the epoch\nPlot the result parameters and comment what the two parameters represent.\n\n\n\n\nWe now have values for an intercept and for a slope, but it is hard to gauge the true meaning of the slope. Therefore we going to generate predicted values at specific levels of the continuous variable. Note that in principle, you could also extrapolate (going outside the range of 0,1) or interpolate (values that were not run in the experiment), but it is not necessary here. These predictions can be seen as conditional ERPs, because you condition on the continuous variable to be a specific value.\n\nEvaluate the continuous regressor at the unique 18 coherence (noise) levels that were used in the experiment and plot them (hint: you could use \\(X_{new}b\\) but dont have to in this simple example)\nShould you add the Intercept to the resulting Plot?\n\n\n\n\n\nNext, generate one evoked.average() for each of the 18 stimulus coherence values.\nPlot them and compare them to your predictions. How well does a continuous predictor capture the data?\n\n\n\n\n\nFor this exercise we need from ccs_eeg_utils import spline_matrix function.\nThe function requires to specify where the spline bases should be evaluated x (the coherence value of each epoch) and where the knots should be. Think of the knots as the anchorpoints of the spline set. (Note: There is a huge literature on knot placement etc. we just use a linspace over the range of our continuous variable. If you want you can play around and e.g.¬†place more knots in the middle than the end). How many knots you ask? Choose a reasonable low number, you can change it later again. There are ways to decide this but they are outside the tutorial. Stay below 10.\nPlot the basisfunction as line-plots\nPlot the designmatrix as an 2D image\nFit the splines to your data (Note: If you wonder about the intercept, there are two ways here.Take the whole spline matrix and dont add an intercept {the intercept is implicit in the spline matrix}, or remove one spline and add an intercept {now all spline-predictors are relativ to the intercept}. Note that due to potential asymmetries in the spline basis set, the intercept is not necessarily what you would expect, see below)\nPlot the resulting ‚Äúraw‚Äù coefficients\nThe ‚Äúraw‚Äù coefficients are hard to interpret. We should run conditional values again. Evaluate the fitted spline-betas at the respective 18 coherence levels and plot them.\n\n\n\n\n\nWe will add a categorical regressor to our spline designmatrix: The difference of FaceA vs.¬†FaceB. Note: If you havent remove a spline-column yet, please do so now and include an intercept.\nBecause it is a categorical predictor, we have to encode it using either dummy coding (e.g.¬†FaceA = 0, FaceB = 1) or effect coding (FaceA = -0.5, FaceB = 0.5 {you could also use -1 / 1})\nFit the model, and plot the faceA/faceB effect\nIn order to facilitate calculating such effects, often a contrast-vector (or matrix) is used. We have a vector \\(b\\) that in our case represents \\([b_{intercept},b_{faceB},b_{spline1}, \\ddots, b_{splineN}]\\). We typically want to use a sum of predictors, ie. \\(b_{intercept}\\) + \\(b_{faceB}\\) which can be represented as a matrix multiplication of \\(b*c\\) with \\(c = [1,1,0,\\dots,0]\\). If we chose effect coding before, we would need to put in the effect coded coefficients in our contrast-vector. I.e. \\(c = [1,-0.5,0,\\dots,0]\\) or \\(c = [1,-1,0,\\dots,0]\\). And finally, we could also combine multiple contrast vectors to one matrix to evaluate multiple effects of interest.\nUse a contrast matrix to plot the FaceA and FaceB ERP. Note that in difference to the example contrats-vector above, we now also want to add the correct spline-values. For this evaluate the spline-coefficients at noise-coherence of 0.9 and replace the 0‚Äôs of your \\(c_{spline1} \\dots c_{splineN}\\) contrast vector.\n\n\n\n\n\nThere could be an interaction between the spline set and the categorical variable. For this, we have to calculate all pairwise multiplications of the faceB-column, with the spline columns.\nFit the resulting model and check whether you found an interaction effect\n\n\n\n\nTypically, one wouldnt fit the models with scipy manually, but one can use MNE to handle the fits (or even dedicated toolboxes like unfold* - but I am not aware of a documented toolbox in python). This also gives you much more support, i.e.¬†gives you standard errors(=SE),and through t = effect/SE, the t-values which can be easily looked up in the analytical H0 distribution and converted to single subject p-values.\n\nimport from mne.stats import linear_regression\nFit a model y~1 + faceB + coherence, thus a model with linear coherence effect: mne also requires to specify your own designmatrix, you can also give the columns names using res = linear_regression(...,names=['Intercept','effectB' ...]).\nYou can access the terms using res[‚ÄòIntercept‚Äô], and plot e.g.¬†the betas using res['Intecept'].beta.plot()\nBecause we have all channels now, we can also plot topoplots. Think about how you could visualize the splines as topoplots (note: this is a hard question)\nYou can also plot the p-values using res[‚Äòphase_coherence‚Äô].p_val.plot(). Do they follow your intuition?\nBonus: What does the distribution (histogram) of p-values of phase-coherence look like during the baseline? (note, bad channels are not automatically removed in MNE, thus you might want to increase your bin-width to spot them\nSchleichwerbung ;-)"
  },
  {
    "objectID": "exercises/ex7_encoding.html#download-dataset",
    "href": "exercises/ex7_encoding.html#download-dataset",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "More on the dataset or the original studies. In short: Faces were shown with varying noise added to them (phase-randomization in 2D-fourier space). An example: \nWe will build a designmatrix to manually apply a regression model to all timepoints. Then we will do the same thing within MNE!\nfrom mne.datasets.limo import load_data\n# choose any subject 1 - 18\nlimo_epochs = load_data(subject=4,path='../local/limo') #\n\nExtract data from electrode B11, this will be your ‚Äòy‚Äô\nThe linear covariate is saved in a dataframe called limo_epochs.metadata. Metadata is yet another way that you can save event-related information besides annotations & events\nGenerate a (design) matrix consisting of an intercet and a linear predictor of phase-coherence.\nUse scipy.linalg.lstsq or similar functions to solve the LeastSquares Problem at each time-point of the epoch\nPlot the result parameters and comment what the two parameters represent."
  },
  {
    "objectID": "exercises/ex7_encoding.html#conditional-erps",
    "href": "exercises/ex7_encoding.html#conditional-erps",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "We now have values for an intercept and for a slope, but it is hard to gauge the true meaning of the slope. Therefore we going to generate predicted values at specific levels of the continuous variable. Note that in principle, you could also extrapolate (going outside the range of 0,1) or interpolate (values that were not run in the experiment), but it is not necessary here. These predictions can be seen as conditional ERPs, because you condition on the continuous variable to be a specific value.\n\nEvaluate the continuous regressor at the unique 18 coherence (noise) levels that were used in the experiment and plot them (hint: you could use \\(X_{new}b\\) but dont have to in this simple example)\nShould you add the Intercept to the resulting Plot?"
  },
  {
    "objectID": "exercises/ex7_encoding.html#compare-it-to-binned-data",
    "href": "exercises/ex7_encoding.html#compare-it-to-binned-data",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "Next, generate one evoked.average() for each of the 18 stimulus coherence values.\nPlot them and compare them to your predictions. How well does a continuous predictor capture the data?"
  },
  {
    "objectID": "exercises/ex7_encoding.html#converting-a-continuous-predictor-to-a-spline-predictor",
    "href": "exercises/ex7_encoding.html#converting-a-continuous-predictor-to-a-spline-predictor",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "For this exercise we need from ccs_eeg_utils import spline_matrix function.\nThe function requires to specify where the spline bases should be evaluated x (the coherence value of each epoch) and where the knots should be. Think of the knots as the anchorpoints of the spline set. (Note: There is a huge literature on knot placement etc. we just use a linspace over the range of our continuous variable. If you want you can play around and e.g.¬†place more knots in the middle than the end). How many knots you ask? Choose a reasonable low number, you can change it later again. There are ways to decide this but they are outside the tutorial. Stay below 10.\nPlot the basisfunction as line-plots\nPlot the designmatrix as an 2D image\nFit the splines to your data (Note: If you wonder about the intercept, there are two ways here.Take the whole spline matrix and dont add an intercept {the intercept is implicit in the spline matrix}, or remove one spline and add an intercept {now all spline-predictors are relativ to the intercept}. Note that due to potential asymmetries in the spline basis set, the intercept is not necessarily what you would expect, see below)\nPlot the resulting ‚Äúraw‚Äù coefficients\nThe ‚Äúraw‚Äù coefficients are hard to interpret. We should run conditional values again. Evaluate the fitted spline-betas at the respective 18 coherence levels and plot them."
  },
  {
    "objectID": "exercises/ex7_encoding.html#add-another-predictor",
    "href": "exercises/ex7_encoding.html#add-another-predictor",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "We will add a categorical regressor to our spline designmatrix: The difference of FaceA vs.¬†FaceB. Note: If you havent remove a spline-column yet, please do so now and include an intercept.\nBecause it is a categorical predictor, we have to encode it using either dummy coding (e.g.¬†FaceA = 0, FaceB = 1) or effect coding (FaceA = -0.5, FaceB = 0.5 {you could also use -1 / 1})\nFit the model, and plot the faceA/faceB effect\nIn order to facilitate calculating such effects, often a contrast-vector (or matrix) is used. We have a vector \\(b\\) that in our case represents \\([b_{intercept},b_{faceB},b_{spline1}, \\ddots, b_{splineN}]\\). We typically want to use a sum of predictors, ie. \\(b_{intercept}\\) + \\(b_{faceB}\\) which can be represented as a matrix multiplication of \\(b*c\\) with \\(c = [1,1,0,\\dots,0]\\). If we chose effect coding before, we would need to put in the effect coded coefficients in our contrast-vector. I.e. \\(c = [1,-0.5,0,\\dots,0]\\) or \\(c = [1,-1,0,\\dots,0]\\). And finally, we could also combine multiple contrast vectors to one matrix to evaluate multiple effects of interest.\nUse a contrast matrix to plot the FaceA and FaceB ERP. Note that in difference to the example contrats-vector above, we now also want to add the correct spline-values. For this evaluate the spline-coefficients at noise-coherence of 0.9 and replace the 0‚Äôs of your \\(c_{spline1} \\dots c_{splineN}\\) contrast vector."
  },
  {
    "objectID": "exercises/ex7_encoding.html#bonus",
    "href": "exercises/ex7_encoding.html#bonus",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "There could be an interaction between the spline set and the categorical variable. For this, we have to calculate all pairwise multiplications of the faceB-column, with the spline columns.\nFit the resulting model and check whether you found an interaction effect"
  },
  {
    "objectID": "exercises/ex7_encoding.html#fitting-a-model-in-mne",
    "href": "exercises/ex7_encoding.html#fitting-a-model-in-mne",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 7]",
    "section": "",
    "text": "Typically, one wouldnt fit the models with scipy manually, but one can use MNE to handle the fits (or even dedicated toolboxes like unfold* - but I am not aware of a documented toolbox in python). This also gives you much more support, i.e.¬†gives you standard errors(=SE),and through t = effect/SE, the t-values which can be easily looked up in the analytical H0 distribution and converted to single subject p-values.\n\nimport from mne.stats import linear_regression\nFit a model y~1 + faceB + coherence, thus a model with linear coherence effect: mne also requires to specify your own designmatrix, you can also give the columns names using res = linear_regression(...,names=['Intercept','effectB' ...]).\nYou can access the terms using res[‚ÄòIntercept‚Äô], and plot e.g.¬†the betas using res['Intecept'].beta.plot()\nBecause we have all channels now, we can also plot topoplots. Think about how you could visualize the splines as topoplots (note: this is a hard question)\nYou can also plot the p-values using res[‚Äòphase_coherence‚Äô].p_val.plot(). Do they follow your intuition?\nBonus: What does the distribution (histogram) of p-values of phase-coherence look like during the baseline? (note, bad channels are not automatically removed in MNE, thus you might want to increase your bin-width to spot them\nSchleichwerbung ;-)"
  },
  {
    "objectID": "exercises/ex2_erpComponents.html",
    "href": "exercises/ex2_erpComponents.html",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "(This assignment is directly based on the ERP PURSUE Course)\nThe purpose of this assignment is to develop your ability to read and critically evaluate a journal article that uses ERP techniques to answer a research question. Specifically, you should be able to identify the research question and important elements of the research design, and explain how the ERP data help to answer this question. Don‚Äôt get bogged down with other details in the paper. Instead, use the assignment questions to guide your reading. Type your answers in the text field below each question. USE YOUR OWN WORDS to answer the questions.\n\n\nRead through the questions on this assignment, and then read the following article: Tanaka, J. W., & Curran, T. (2001). A neural basis for expert object recognition. Psychol Sci, 12(1), 43‚Äì47. Find the paper here (maybe behind paywall) or here\n\nIn your own words, what was the research question?\nUsing an if/then statement, what was the hypothesis? The ‚Äúif‚Äù part should relate to the theory, and the ‚Äúthen‚Äù part should relate to the expected ERP results.\nComplete the table below describing the order of trial events and their timing.\n\n\n\n\nEvent Number\nEvent Name\nTiming\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n\nPossible events: Picture, Category, Blank, Fixation, True/False Prompt possible timings: 255ms, 570ms, 735ms, 1000-1500ms, until response\n\nTo which event in the trial structure was the event-related potential synchronized?\nWhat were the dependent variable(s) in the reported ANOVAs?\nWhat were the channels of interest, and how did they decide which ones to analyze?\nDoes the graph in Figure 2 plot negative UP (the wrong way) or DOWN (the right way)?\nFigure 3 shows the scalp distribution of the mean voltage differences in the expertise effect. Do you think these results support a functional (qualitative) difference between experts and novices, or a quantitative difference, and why?\nBased on the results of this experiment, do YOU think an enhanced N170 component is domain-specific for faces or does it reflect expertise-related activity? Back up your answer with the statistical finding you found most relevant and convincing\nSuggest a follow-up ERP experiment and explain how it would extend these findings."
  },
  {
    "objectID": "exercises/ex2_erpComponents.html#assignment",
    "href": "exercises/ex2_erpComponents.html#assignment",
    "title": "Signal processing and analysis of human brain potentials (EEG) [Exercise 5]",
    "section": "",
    "text": "Read through the questions on this assignment, and then read the following article: Tanaka, J. W., & Curran, T. (2001). A neural basis for expert object recognition. Psychol Sci, 12(1), 43‚Äì47. Find the paper here (maybe behind paywall) or here\n\nIn your own words, what was the research question?\nUsing an if/then statement, what was the hypothesis? The ‚Äúif‚Äù part should relate to the theory, and the ‚Äúthen‚Äù part should relate to the expected ERP results.\nComplete the table below describing the order of trial events and their timing.\n\n\n\n\nEvent Number\nEvent Name\nTiming\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n\nPossible events: Picture, Category, Blank, Fixation, True/False Prompt possible timings: 255ms, 570ms, 735ms, 1000-1500ms, until response\n\nTo which event in the trial structure was the event-related potential synchronized?\nWhat were the dependent variable(s) in the reported ANOVAs?\nWhat were the channels of interest, and how did they decide which ones to analyze?\nDoes the graph in Figure 2 plot negative UP (the wrong way) or DOWN (the right way)?\nFigure 3 shows the scalp distribution of the mean voltage differences in the expertise effect. Do you think these results support a functional (qualitative) difference between experts and novices, or a quantitative difference, and why?\nBased on the results of this experiment, do YOU think an enhanced N170 component is domain-specific for faces or does it reflect expertise-related activity? Back up your answer with the statistical finding you found most relevant and convincing\nSuggest a follow-up ERP experiment and explain how it would extend these findings."
  },
  {
    "objectID": "exercises/exercises.html",
    "href": "exercises/exercises.html",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "These exercises are not intendend to be brain teasers (they can be nevertheless üôà) - this is by design. They are designed to help you think in different ways about the lecture material. The exercises are highly recommended but not mandatory. It is best if you generate a jupyter notebook for each exercise and upload it till the discussion session.\nYou are encouraged to work in groups of 2.\nExercises are only graded pass/failed and if requested via e-mail I will provide feedback. Again: Exercises are not mandatory to pass the course. The course-grade is solely based on the semesterproject.",
    "crumbs": [
      "Exercises",
      "üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise sheets"
    ]
  },
  {
    "objectID": "exercises/exercises.html#general-remarks-to-exercises",
    "href": "exercises/exercises.html#general-remarks-to-exercises",
    "title": "üß† EEG-2024",
    "section": "",
    "text": "These exercises are not intendend to be brain teasers (they can be nevertheless üôà) - this is by design. They are designed to help you think in different ways about the lecture material. The exercises are highly recommended but not mandatory. It is best if you generate a jupyter notebook for each exercise and upload it till the discussion session.\nYou are encouraged to work in groups of 2.\nExercises are only graded pass/failed and if requested via e-mail I will provide feedback. Again: Exercises are not mandatory to pass the course. The course-grade is solely based on the semesterproject.",
    "crumbs": [
      "Exercises",
      "üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise sheets"
    ]
  },
  {
    "objectID": "exercises/exercises.html#the-exercises",
    "href": "exercises/exercises.html#the-exercises",
    "title": "üß† EEG-2024",
    "section": "The exercises",
    "text": "The exercises\n\n\n\nOverview + first ERP\nExercise 1\n\n\nERPs\nExercise 2, Supplementary PDF file\n\n\nFilter\nExercise 3\n\n\nCleaning\nExercise 4\n\n\nICA\nExercise 5\n\n\nLinearModels\nExercise 6 Supplementary CSV file\n\n\nEncoding\nExercise 7\n\n\nClusterPermuatation\nExercise 8\n\n\nTimeFrequency\nExercise 9\n\n\nDecoding\nExercise 10\n\n\nSource Space\nExercise 11",
    "crumbs": [
      "Exercises",
      "üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise sheets"
    ]
  },
  {
    "objectID": "milestones.html",
    "href": "milestones.html",
    "title": "Project Milestones",
    "section": "",
    "text": "Note\n\n\n\nThe purpose of the milestones is to help you monitoring your progress throughout the semester.\nIn each milestone session, each project group briefly presents their current process and we can discuss open questions (approx. 1‚Äâmin presentation + 3‚Äâmin discussion per group).\nFor this purpose, each group is supposed to prepare 1 slide (singular!) for each of the milestones and upload them on Ilias until midnight before the respective session.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCheck out the schedule for the actual dates.\n\n\n\n\n\n\n\n\nMilestone 1: Paper+Dataset selected\n\n\n\n\n\nGoals to be reached:\n\nYou selected a paper and a dataset to replicate\nYou extracted the central hypotheses of the paper\n\nSlides: Prepare 1 Slide showing:\n\nTeamname + Members\nThe general idea of the experiment\nThe main research question\nThe main analysis you will perform (ERP, Time-Frequency, Source-space, others)\n\n\n\n\n\n\n\nTip\n\n\n\nResearch questions can often be answered by a ‚Äúyes‚Äù or ‚Äúno‚Äù.\n\n\n\n\n\n\n\n\n\n\n\nMilestone 2: Processing Pipeline\n\n\n\n\n\nGoals to be reached:\n\nMap out the required analysis pipeline of the authors\nCompare this to the analysis pipeline you want to do\nDownloaded + Loaded the data. Have a first impression\n\nSlides: Prepare 1 slide showing:\n\nThe central research question (one sentence, very brief)\nA table with the authors + your pipeline\n\n\n\n\n\n\n\nWarning\n\n\n\nNot all authors do a good job to clearly communicate how their pipeline looked like. Add notes when you are unsure!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is totally fine to adapt your processing pipeline as you go & experience problems / successes and get feedback.\n\n\n\n\n\n\n\n\n\n\n\nMilestone 3: First subject analyzed\n\n\n\n\n\nGoals to be reached:\n\nImplemented a preprocessing pipeline\nOne single subject analyzed\n\nSlides: Prepare 1 slide showing: - Your improved pipeline - First results of one subject (e.g.¬†an ERP or similar)\n\n\n\n\n\n\n\n\n\nMilestone 4: All subjects\n\n\n\n\n\nGoals to be reached:\n\nPipeline ran for all subjects\nFurther outliers / problems identified\n\nSlides: Provide 1-2 slides with results of your sanity checks.\nSanity checks could be (depending on project, come up with your own!)\n- ICA topography quality - ERP Signal-To-Noise\n\n\n\n\n\n\n\n\n\nMilestone 5: Outlook\n\n\n\n\n\nGoals to be reached:\n\nYou should by now be close to replicate (or not!) the results of the authors\nYou should think of an additional analysis (e.g.¬†ERP, decoding, linear model, time-frequency, source-space) to run on this data.\n\nSlides: Prepare 2 slides listing:\n\n(Slide 1) the current state of replication vs.¬†not\n(Slide 2) your ideas on what to look at next",
    "crumbs": [
      "Projects",
      "üéØ Milestones"
    ]
  }
]